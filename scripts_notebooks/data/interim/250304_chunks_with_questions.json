[
  {
    "text": "The work is made available under the Creative Commons CC0 public domain dedication.\n\nData Availability Statement: The data are available in the Supplemental Materials that are included with the manuscript.\n\nFunding: The work was completed while both authors were employees of the National Institutes of Health. The work was supported by the NIH intramural program.\n\nCompeting Interests: The authors have declared that no competing interests exist.\n\nRESEARCH ARTICLE\n\n# Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time\n\n### Robert M. Kaplan1 *, Veronica L. Irvin2\n\n1 Agency for Healthcare Research and Quality, U.S. Department of Health and Human Services, Rockville, Maryland, United States of America, 2 Oregon State University, Corvallis, Oregon, United States of America\n\n* Robert.Kaplan@ahrq.hhs.gov\n\n## Abstract\n\n### Background\n\nWe explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time.\n\n### Methods\n\nWe identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs >$500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death.",
    "is_useful": true,
    "question": "What is the status of data availability in research articles that promote open science?"
  },
  {
    "text": "RESEARCH ARTICLE\n\n# Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time\n\n### Robert M. Kaplan1 *, Veronica L. Irvin2\n\n1 Agency for Healthcare Research and Quality, U.S. Department of Health and Human Services, Rockville, Maryland, United States of America, 2 Oregon State University, Corvallis, Oregon, United States of America\n\n* Robert.Kaplan@ahrq.hhs.gov\n\n## Abstract\n\n### Background\n\nWe explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time.\n\n### Methods\n\nWe identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs >$500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality.",
    "is_useful": true,
    "question": "What trend has been observed in the outcomes of large clinical trials funded by the National Heart Lung, and Blood Institute over time?"
  },
  {
    "text": "### Methods\n\nWe identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs >$500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality.\n\n### Results\n\n17 of 30 studies (57%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8%) trials published after 2000 (\u03c72 =12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials. gov was strongly associated with the trend toward null findings.\n\n### Conclusions\n\nThe number NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.",
    "is_useful": true,
    "question": "What trend has been observed in the reporting of positive results in NHLBI supported RCTs after the year 2000?"
  },
  {
    "text": "### Results\n\n17 of 30 studies (57%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8%) trials published after 2000 (\u03c72 =12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials. gov was strongly associated with the trend toward null findings.\n\n### Conclusions\n\nThe number NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.\n\n### Introduction\n\nLarge randomized clinical trials (RCTs) provide the best evidence to justify new treatments or to identify treatments that do not improve patient outcomes. Gordon and colleagues reported that most large NHLBI-funded trials produce null results[1], but their analysis only considered papers published after 2000. Considering all large trials over the last 40 years, we explore whether there has been a trend toward null finding in recent years and consider potential explanations for trends in observing null outcomes.\n\n### Method\n\n### Sample of Studies\n\nWe identified all large RCTs that involved drugs or supplements funded between 1970\u20132012. To avoid non-publication bias, we focused on large trials where non-reporting of outcomes is rare[1].",
    "is_useful": true,
    "question": "How has the trend of reporting positive results in large randomized clinical trials changed after the year 2000?"
  },
  {
    "text": "Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.\n\n### Introduction\n\nLarge randomized clinical trials (RCTs) provide the best evidence to justify new treatments or to identify treatments that do not improve patient outcomes. Gordon and colleagues reported that most large NHLBI-funded trials produce null results[1], but their analysis only considered papers published after 2000. Considering all large trials over the last 40 years, we explore whether there has been a trend toward null finding in recent years and consider potential explanations for trends in observing null outcomes.\n\n### Method\n\n### Sample of Studies\n\nWe identified all large RCTs that involved drugs or supplements funded between 1970\u20132012. To avoid non-publication bias, we focused on large trials where non-reporting of outcomes is rare[1]. The search process is summarized in a PRISMA diagram (S1 Fig). Two independent searches were conducted to improve probability of accurately capturing all related trials\u2013one by the study authors and the second by NHLBI. We searched three different NIH grant databases (QVR, NIH REPORTER, and CRISP) for RCTs that were primarily funded or administered by NHLBI. QVR is an internal NIH data-base, but readers can replicate our search using NIH REPORTER and CRISP which are publically available resources listing all grants and associated publications.",
    "is_useful": true,
    "question": "What practices in randomized clinical trials may help ensure the reliability of study outcomes?"
  },
  {
    "text": "### Method\n\n### Sample of Studies\n\nWe identified all large RCTs that involved drugs or supplements funded between 1970\u20132012. To avoid non-publication bias, we focused on large trials where non-reporting of outcomes is rare[1]. The search process is summarized in a PRISMA diagram (S1 Fig). Two independent searches were conducted to improve probability of accurately capturing all related trials\u2013one by the study authors and the second by NHLBI. We searched three different NIH grant databases (QVR, NIH REPORTER, and CRISP) for RCTs that were primarily funded or administered by NHLBI. QVR is an internal NIH data-base, but readers can replicate our search using NIH REPORTER and CRISP which are publically available resources listing all grants and associated publications. Inclusion criteria were: RCT for studies funded from 1970\u20132012; grants or contracts; direct costs funded were large enough to require special authorization (>$500,000/ year); the word \"trial\" had to appear in the study objectives or abstract; and primary outcome was cardiovascular risk factor, event or death. Exclusion criteria included: project still active; no human subjects protocol required; pediatric studies; animal studies; non-RCTs (i.e. observational, cohort, case control, genetic or proteomics, measurement, basic clinical research); or interventions that did not involve a drug or supplement (i.e. behavior change, devices, surgeries). An expanded methods section is available in the Supplemental Materials.",
    "is_useful": true,
    "question": "What are the criteria for including studies in a systematic review of large randomized controlled trials funded by the NHLBI between 1970 and 2012?"
  },
  {
    "text": "QVR is an internal NIH data-base, but readers can replicate our search using NIH REPORTER and CRISP which are publically available resources listing all grants and associated publications. Inclusion criteria were: RCT for studies funded from 1970\u20132012; grants or contracts; direct costs funded were large enough to require special authorization (>$500,000/ year); the word \"trial\" had to appear in the study objectives or abstract; and primary outcome was cardiovascular risk factor, event or death. Exclusion criteria included: project still active; no human subjects protocol required; pediatric studies; animal studies; non-RCTs (i.e. observational, cohort, case control, genetic or proteomics, measurement, basic clinical research); or interventions that did not involve a drug or supplement (i.e. behavior change, devices, surgeries). An expanded methods section is available in the Supplemental Materials.\n\nWe coded the following variables: start year (earliest funding noted), publication year of main outcome study, funded through contract or cooperative agreement from NHLBI, type of comparator (placebo, active comparator, usual care), primary outcome specified or not, CON-SORT diagram included in publication, whether funding was exclusively from NIH versus joint industry/NIH funded (including industry contributed medications), and if they had listed any other significant results that were neither the primary outcomes or the side effects of the drug. In addition, we considered whether studies were registered in clinicaltrials.gov prior to publication.",
    "is_useful": true,
    "question": "What criteria were used to include and exclude studies in the analysis of cardiovascular risk factors funded by NIH?"
  },
  {
    "text": "In addition, we considered whether studies were registered in clinicaltrials.gov prior to publication.\n\nEach trial was categorized as showing significant benefit, null, or significant harm for the primary outcome and for total mortality (See Tables 1 and 2). Null was defined as a confidence interval for the RR that included 1.0.using a two-tailed test with alpha set at 0.05. The analysis was standardized by re-computing the relative risk (RR) with 95% confidence intervals (CI) for all trials.\n\n### Results\n\nAmong 4,089 individual years of grant funding, almost half were excluded as multiple years of the same grant and over 20% were excluded because they were single sites in multi-site trials, coordinating centers, or ancillary studies of the same trial. An additional 1,176 grant abstracts did not match our criteria and were excluded (see S1 Table for detailed reasons). Main outcome papers were searched for 84 trials; 10 were not published and 25 did not match search criteria and were excluded (See S1 Fig for the PRISMA diagram and S1 Table for the number of studies Table 1. Study characteristics and overall effect for main outcome and total mortality for studies not registered in ClinicalTrials.gov prior to publication.\n\n| Study | Acronym | Start | Pub | Primary Outcome (PO) | Primary | Total |\n| --- | --- | --- | --- | --- | --- | --- |\n|  |  | Year | Year |  | Outcome | Mortality |\n| Asymptomatic Carotid Artery Progression |",
    "is_useful": true,
    "question": "What considerations are made regarding the registration of clinical trials prior to publication in the context of evaluating their outcomes and overall effect?"
  },
  {
    "text": "Prior to 2000 when trials were not registered in clinical trials.gov, there was substantial variability in outcome. Following the imposition of the requirement that trials preregister in clinical trials.gov the relative risk on primary outcomes showed considerably less variability around 1.0.\n\ndoi:10.1371/journal.pone.0132382.g001\n\nexcluded by reason.) Following exclusions, we identified a total of 49 funded grants. Four of these grants resulted in multiple unique trials (ACCORD Blood Pressure, Diabetes, and Lipid; ALLHAT-BP, DOX, LLT; WHI Estrogen and Estrogen-Progestin, and WHS aspirin and vitamin E). A total of 55 trials were analyzed\u2013 30 were published prior to 2000 and 25 were\n\n![](_page_5_Figure_1.jpeg)\n\n| Acronym | RR (95% CI) | Weight |  | RR (95% CI) | 0% Weight |\n| --- | --- | --- | --- | --- | --- |\n|  |  |  | Acronym |  |  |\n| Clinical or angiographic outcome |  |  |  |  |  |\n| ACAPS | 0.36 (0.13, 0.98) | 0.81 | ACCORD-BP | 0.88 (0.74, 1.05) | 3.35 |\n| AMIS | 1.09 (0.91, 1.31) | 5.",
    "is_useful": true,
    "question": "How has preregistration of clinical trials affected the variability of primary outcomes in research?"
  },
  {
    "text": "SANDS study authors concluded that there may not be favorable long-term outcomes for participants randomized to treatment.\n\nd. Chi-square test uses the Yates correction for continuity.\n\ndoi:10.1371/journal.pone.0132382.t003\n\nNHLBI trials published prior to 2000 used a placebo as the comparator in contrast to 64% trials published after 2000 (see S3 Table). Placebos were used as the comparator at about the same rate prior to and after the year 2000 (p = .979).\n\nTo investigate the effect of industry co-sponsorship, we tabulated sponsorship for all reports. Unfortunately, industry co-sponsorship was not always reported prior to the year 2000 and journals did not uniformly require disclosure. After the year 2000, when the International Committee of Medical Journal Editors (ICMJE) asked for disclosure, it became apparent that industry co-sponsorship is very common. In our sample, 23 of 25 (92%) of the NHLBI trials published after 2000 had partial industry sponsorship or contribution of medications. All but two of these trials obtained null results. We also looked at previous financial relationships between investigators and industry. Prior to 2000, these relationships were reported in only 1 of the 30 trials (3%). Even after 2000, 28% of the studies did not include a disclosure section. But among articles that included disclosures, there was a financial consulting relationship between at least one author and industry in all (100%) of the cases.",
    "is_useful": true,
    "question": "What trends have been observed in the reporting of industry co-sponsorship in clinical trials before and after the year 2000?"
  },
  {
    "text": "Unfortunately, industry co-sponsorship was not always reported prior to the year 2000 and journals did not uniformly require disclosure. After the year 2000, when the International Committee of Medical Journal Editors (ICMJE) asked for disclosure, it became apparent that industry co-sponsorship is very common. In our sample, 23 of 25 (92%) of the NHLBI trials published after 2000 had partial industry sponsorship or contribution of medications. All but two of these trials obtained null results. We also looked at previous financial relationships between investigators and industry. Prior to 2000, these relationships were reported in only 1 of the 30 trials (3%). Even after 2000, 28% of the studies did not include a disclosure section. But among articles that included disclosures, there was a financial consulting relationship between at least one author and industry in all (100%) of the cases. Industry influence would produce a bias in favor of positive results, so connections between investigators and industry is not a likely explanation for the trend toward null results in recent years.\n\nWe considered a variety of aspects of transparent reporting. Prior to 2000, 5 of the 30 published trials (17%) included a diagram that clearly accounted for the number of participants at each phase of the project. Following 2000, publications were significantly more likely to account for patients throughout the study: 14 of the 25 trials (56%) included such a flow diagram (\u03c72 = 9.22, p = 0.002).",
    "is_useful": true,
    "question": "What has been observed regarding the reporting of industry sponsorship in medical research publications before and after the year 2000?"
  },
  {
    "text": "Even after 2000, 28% of the studies did not include a disclosure section. But among articles that included disclosures, there was a financial consulting relationship between at least one author and industry in all (100%) of the cases. Industry influence would produce a bias in favor of positive results, so connections between investigators and industry is not a likely explanation for the trend toward null results in recent years.\n\nWe considered a variety of aspects of transparent reporting. Prior to 2000, 5 of the 30 published trials (17%) included a diagram that clearly accounted for the number of participants at each phase of the project. Following 2000, publications were significantly more likely to account for patients throughout the study: 14 of the 25 trials (56%) included such a flow diagram (\u03c72 = 9.22, p = 0.002). After the year 2000 all of the published papers clearly identified primary outcome variable, while the primary outcome variable was not specified in 23% of the publications prior to 2000 (\u03c72 = 4.75, p = 0.03).\n\nA final explanation for the trend toward null reports is that current authors face greater constraints in reporting the results of their studies. In our review, the year 2000 marks the beginning of a natural experiment. After the year 2000, all (100%) of large NHLBI were registered prospectively in ClinicalTrials.Gov prior to publication. Prior to 2000 none of the trials (0%) were prospectively registered.",
    "is_useful": true,
    "question": "How has the reporting of trial outcomes and transparency changed in publications since the year 2000?"
  },
  {
    "text": "Following 2000, publications were significantly more likely to account for patients throughout the study: 14 of the 25 trials (56%) included such a flow diagram (\u03c72 = 9.22, p = 0.002). After the year 2000 all of the published papers clearly identified primary outcome variable, while the primary outcome variable was not specified in 23% of the publications prior to 2000 (\u03c72 = 4.75, p = 0.03).\n\nA final explanation for the trend toward null reports is that current authors face greater constraints in reporting the results of their studies. In our review, the year 2000 marks the beginning of a natural experiment. After the year 2000, all (100%) of large NHLBI were registered prospectively in ClinicalTrials.Gov prior to publication. Prior to 2000 none of the trials (0%) were prospectively registered. Although many of the earlier studies are in the ClinicalTrials. Gov database, they were registered after the results had been published. Following the implementation of ClinicalTrials.gov, investigators were required to prospectively declare their primary and secondary outcome variables. Prior to 2000, investigators had a greater opportunity to measure a range of variables and to select the most successful outcomes when reporting their results. For trials published before the year 2000, we found that 17 out of 30 (57%) reported significant benefit for their primary outcome.",
    "is_useful": true,
    "question": "How did the practices of clinical trial reporting change after the year 2000?"
  },
  {
    "text": "In our review, the year 2000 marks the beginning of a natural experiment. After the year 2000, all (100%) of large NHLBI were registered prospectively in ClinicalTrials.Gov prior to publication. Prior to 2000 none of the trials (0%) were prospectively registered. Although many of the earlier studies are in the ClinicalTrials. Gov database, they were registered after the results had been published. Following the implementation of ClinicalTrials.gov, investigators were required to prospectively declare their primary and secondary outcome variables. Prior to 2000, investigators had a greater opportunity to measure a range of variables and to select the most successful outcomes when reporting their results. For trials published before the year 2000, we found that 17 out of 30 (57%) reported significant benefit for their primary outcome. In the new era where primary outcomes are prospectively declared (published post 2000), only 2 of 25 trials (8%) reported a significant benefit (\u03c72 = 12.2, p = 0.0005).\n\nProspective declaration of the primary outcome variable is important because it eliminates the possibility of selecting for reporting an outcome among many different measures included in the study. In order to investigate this issue, we looked at the statistical significance of other variables not declared as the primary outcomes for preregistered studies. Among the 25 preregistered trials published in 2000 or later, 12 reported significant, positive effects for cardiovascular-related variables other than the primary outcome.",
    "is_useful": true,
    "question": "What impact did the prospective registration of trials on ClinicalTrials.gov after the year 2000 have on the reporting of significant benefits in clinical trials?"
  },
  {
    "text": "For trials published before the year 2000, we found that 17 out of 30 (57%) reported significant benefit for their primary outcome. In the new era where primary outcomes are prospectively declared (published post 2000), only 2 of 25 trials (8%) reported a significant benefit (\u03c72 = 12.2, p = 0.0005).\n\nProspective declaration of the primary outcome variable is important because it eliminates the possibility of selecting for reporting an outcome among many different measures included in the study. In order to investigate this issue, we looked at the statistical significance of other variables not declared as the primary outcomes for preregistered studies. Among the 25 preregistered trials published in 2000 or later, 12 reported significant, positive effects for cardiovascular-related variables other than the primary outcome. Importantly, almost half of the trials might have been able to report a positive result if they had not declared a primary outcome in advance. Had the prospective declaration of a primary outcome not have been required, it is possible that the number of positive studies post-2000 would have looked very similar to the pre-2000 period.\n\n### Discussion\n\nBeginning in approximately 2000, the likelihood of showing a significant benefit in large NHLBI funded studies declined. Among the explanations we evaluated, the requirement of prospective registration in Clinicaltrials.gov is most strongly associated with the observed trend toward null clinical trials. The decline is not easily explained by the increased use of active comparators or a decline in industry sponsorship.",
    "is_useful": true,
    "question": "How has the requirement for prospective declaration of primary outcomes influenced the reporting of significant benefits in clinical trials over time?"
  },
  {
    "text": "Among the 25 preregistered trials published in 2000 or later, 12 reported significant, positive effects for cardiovascular-related variables other than the primary outcome. Importantly, almost half of the trials might have been able to report a positive result if they had not declared a primary outcome in advance. Had the prospective declaration of a primary outcome not have been required, it is possible that the number of positive studies post-2000 would have looked very similar to the pre-2000 period.\n\n### Discussion\n\nBeginning in approximately 2000, the likelihood of showing a significant benefit in large NHLBI funded studies declined. Among the explanations we evaluated, the requirement of prospective registration in Clinicaltrials.gov is most strongly associated with the observed trend toward null clinical trials. The decline is not easily explained by the increased use of active comparators or a decline in industry sponsorship. In addition to the explanations at we evaluated using reported characteristics of the trials, we considered several other suggestions.\n\nOne explanation is that newer clinical trial management methodologies remove error variance and provide more precise estimates of treatment effects. If this were the explanation, refined methodologies and greater precision should have resulted in reductions in error variance, ultimately increasing the likelihood of finding treatments effects. But the probability of finding a treatment benefit decreased rather than increased as studies became more precise. As shown in Fig 1, variability in trial results declined systematically around the year 2000. As a result, we do not find better trial management to be a compelling explanation for the trend toward null results.\n\nIt is widely noted that journals favor publication of statistically significant findings[2].",
    "is_useful": true,
    "question": "What impact did the requirement of prospective registration of primary outcomes have on the reporting of clinical trial results post-2000?"
  },
  {
    "text": "As a result, we do not find better trial management to be a compelling explanation for the trend toward null results.\n\nIt is widely noted that journals favor publication of statistically significant findings[2]. Bias in favor of publishing positive outcomes is not a likely explanation for our results. We focused on large trials because previous analyses by NHLBI reported that 97% of trials with annual budgets over $500,000/year were published [3], thus removing publication bias as a rival explanation. In our analysis, 88% of the trials were published, although there may be a slight delay in the date of publication for null trials6 . If positive trials are more likely to be published than null trials, we would have expected more positive published reports following 2000. A \"file drawer\" problem of suppressing null trial findings would result in over reporting positive results. Our observation of a trend toward null results goes in the opposite direction. If there is a bias, it is possible that stricter reporting standards and greater rigor in reporting requirements are suppressing the declaration of positive outcomes.\n\nIt has been argued that there have been few efficacious drugs in the pipeline[4,5]. Since about 1998, there has been a systematic decline in the number approvals for new cardiovascular drugs[6]. Thus, we would expect more null trials because the rate of developing effective new principals has declined. We believe this explanation unlikely because nearly all of the trials evaluated treatments that had been previously studied. For example, all of the treatments had been approved by the US FDA and these approvals require early phase trial evidence of safety and efficacy.",
    "is_useful": true,
    "question": "What factors may contribute to the trend toward null results in clinical trials, according to recent analyses?"
  },
  {
    "text": "A \"file drawer\" problem of suppressing null trial findings would result in over reporting positive results. Our observation of a trend toward null results goes in the opposite direction. If there is a bias, it is possible that stricter reporting standards and greater rigor in reporting requirements are suppressing the declaration of positive outcomes.\n\nIt has been argued that there have been few efficacious drugs in the pipeline[4,5]. Since about 1998, there has been a systematic decline in the number approvals for new cardiovascular drugs[6]. Thus, we would expect more null trials because the rate of developing effective new principals has declined. We believe this explanation unlikely because nearly all of the trials evaluated treatments that had been previously studied. For example, all of the treatments had been approved by the US FDA and these approvals require early phase trial evidence of safety and efficacy.\n\nAnother explanation for the increase in null trials is the possibility that medical care and supportive therapy have improved since 2000. As a result it has become difficult to demonstrate treatment effects because new approaches must compete with higher quality medical care. In support of this argument is the observation that outcomes in cardiovascular diseases continue to improve despite wide variation in the specific care that patients receive. On the other hand, outcomes of studies that compared treatment to an active standard of care comparison group achieved results quite similar to studies that compared treatment to placebo. However, we do recognize that the quality of background cardiovascular care continues to improve, making it increasingly difficult to demonstrate the incremental value of new treatments. The improvement in usual cardiovascular care could serve as alternative explanation for the trend toward null results in recent years.",
    "is_useful": true,
    "question": "What are some potential explanations for the increase in null trial results in cardiovascular drug development?"
  },
  {
    "text": "For example, all of the treatments had been approved by the US FDA and these approvals require early phase trial evidence of safety and efficacy.\n\nAnother explanation for the increase in null trials is the possibility that medical care and supportive therapy have improved since 2000. As a result it has become difficult to demonstrate treatment effects because new approaches must compete with higher quality medical care. In support of this argument is the observation that outcomes in cardiovascular diseases continue to improve despite wide variation in the specific care that patients receive. On the other hand, outcomes of studies that compared treatment to an active standard of care comparison group achieved results quite similar to studies that compared treatment to placebo. However, we do recognize that the quality of background cardiovascular care continues to improve, making it increasingly difficult to demonstrate the incremental value of new treatments. The improvement in usual cardiovascular care could serve as alternative explanation for the trend toward null results in recent years.\n\nOur results may also reflect greater involvement by NHLBI in trial design and execution. Prior to 2000, most large NHLBI clinical trials were investigator initiatLaed while nearly 80% of the trials published after 2000 had direct involvement of NHLBI through cooperative agreements. We recognize that industry sponsored trials may have a higher success rate. It is possible that industry conducts trials designed to demonstrate effectiveness while NHLBI uses its resources when there is true equipoise.\n\nAll post 2000 trials reported total mortality while total mortality was only reported in about 80% of the pre-2000 trials and many of the early trials were not powered to detect changes in mortality.",
    "is_useful": true,
    "question": "What factors could contribute to an increase in null trial results in clinical research?"
  },
  {
    "text": "It is possible that industry conducts trials designed to demonstrate effectiveness while NHLBI uses its resources when there is true equipoise.\n\nAll post 2000 trials reported total mortality while total mortality was only reported in about 80% of the pre-2000 trials and many of the early trials were not powered to detect changes in mortality. The effects on total mortality were null for both pooled analyses of trials that were registered or not registered prior to publication (see data in online supplement) In addition, prior to 2000 and the implementation of Clinicaltrials.gov, investigators had the opportunity to change the p level or the directionality of their hypothesis post hoc. Further, they could create composite variables by adding variables together in a way that favored their hypothesis. Preregistration in ClinicalTrials.gov essentially eliminated this possibility.\n\n### Limitations\n\nOur analysis is limited to large NHLBI-funded trials and to studies on cardiovascular outcomes in adults. We focused on NHLBI because the Institute has championed transparency and allowed us full access to all trials. We emphasized large trials because we had access to outcomes of nearly all studies, thus reducing the risk of publication bias. Although we focused on cardiovascular trials, null results are common in other areas of medicine. For example, among\n\n221 agents with the potential to modify outcomes for Alzheimer's disease, all placebo controlled trials registered in clinical trials.gov have failed to identify positive benefits on the declared primary outcome[7].\n\nOur analysis underscores the importance of NHLBI involvement in trials. A greater number of recent trials used direct NHLBI, oversight.",
    "is_useful": true,
    "question": "What is the significance of NHLBI involvement in clinical trials, particularly regarding transparency and the accuracy of reported outcomes?"
  },
  {
    "text": "Further, they could create composite variables by adding variables together in a way that favored their hypothesis. Preregistration in ClinicalTrials.gov essentially eliminated this possibility.\n\n### Limitations\n\nOur analysis is limited to large NHLBI-funded trials and to studies on cardiovascular outcomes in adults. We focused on NHLBI because the Institute has championed transparency and allowed us full access to all trials. We emphasized large trials because we had access to outcomes of nearly all studies, thus reducing the risk of publication bias. Although we focused on cardiovascular trials, null results are common in other areas of medicine. For example, among\n\n221 agents with the potential to modify outcomes for Alzheimer's disease, all placebo controlled trials registered in clinical trials.gov have failed to identify positive benefits on the declared primary outcome[7].\n\nOur analysis underscores the importance of NHLBI involvement in trials. A greater number of recent trials used direct NHLBI, oversight. The institute is fully vetted for conflict of interest and applies high quality control standards including full transparency, open data access, and registration in ClincalTrials.gov. Our conclusions may not generalize to trials sponsored by industry or to other funding agencies.\n\nWe cannot say that trend toward null trials to preregistration in ClinicalTrials.gov is causal. Our analysis included only a small number of trials and the design of the study does not allow causal inferences. Most importantly, many variables may have changed around the year 2000. It is likely that other variables that are unknown or unmeasured also correspond to the decline in reports of significant therapeutic treatment effects.",
    "is_useful": true,
    "question": "What role does preregistration in ClinicalTrials.gov play in enhancing transparency and reducing publication bias in clinical trials?"
  },
  {
    "text": "Our analysis underscores the importance of NHLBI involvement in trials. A greater number of recent trials used direct NHLBI, oversight. The institute is fully vetted for conflict of interest and applies high quality control standards including full transparency, open data access, and registration in ClincalTrials.gov. Our conclusions may not generalize to trials sponsored by industry or to other funding agencies.\n\nWe cannot say that trend toward null trials to preregistration in ClinicalTrials.gov is causal. Our analysis included only a small number of trials and the design of the study does not allow causal inferences. Most importantly, many variables may have changed around the year 2000. It is likely that other variables that are unknown or unmeasured also correspond to the decline in reports of significant therapeutic treatment effects.\n\n### Implications\n\nThe transparency of RCTs is likely to have improved following the FDA Modernization Act of 1997, which created the ClinicalTrials.gov registry[8], a service that required registration of studies that test drugs, biologics, or devices for the treatment of serious or life threatening diseases[9\u201311]. Registered studies must provide: the study's purpose, recruitment status, design, eligibility criteria, locations and pre-specified primary and secondary outcomes[11]. The Consolidated Standards of Reporting Trials (CONSORT) were introduced in 1996 but expanded in 2001 to require greater transparency in the reporting of Randomized Clinical Trials (RCTs) [12].",
    "is_useful": true,
    "question": "What role does transparency and registration play in the conduct of clinical trials, particularly in relation to regulatory requirements and standards?"
  },
  {
    "text": "Most importantly, many variables may have changed around the year 2000. It is likely that other variables that are unknown or unmeasured also correspond to the decline in reports of significant therapeutic treatment effects.\n\n### Implications\n\nThe transparency of RCTs is likely to have improved following the FDA Modernization Act of 1997, which created the ClinicalTrials.gov registry[8], a service that required registration of studies that test drugs, biologics, or devices for the treatment of serious or life threatening diseases[9\u201311]. Registered studies must provide: the study's purpose, recruitment status, design, eligibility criteria, locations and pre-specified primary and secondary outcomes[11]. The Consolidated Standards of Reporting Trials (CONSORT) were introduced in 1996 but expanded in 2001 to require greater transparency in the reporting of Randomized Clinical Trials (RCTs) [12]. Shortly after 2001, many major journals began requiring prospective registration of clinical trials as a condition for publication and the International Committee of Medical Journal Editors started requiring CONSORT reporting in all major journals beginning in 2004 (icjme. org). NHLBI was an early adopter of trial registration. All of their large trials published after 2000 were preregistered and transparently reported. Although we cannot say that stricter reporting requirements caused the trend toward more null reports from NHLBI trials, we do find the association worthy of more investigation.\n\nIn conclusion, null findings in large RCTs may be disappointing to investigators, but they are not negative for science.",
    "is_useful": true,
    "question": "How have reporting requirements impacted the transparency and outcomes of randomized clinical trials (RCTs)?"
  },
  {
    "text": "The Consolidated Standards of Reporting Trials (CONSORT) were introduced in 1996 but expanded in 2001 to require greater transparency in the reporting of Randomized Clinical Trials (RCTs) [12]. Shortly after 2001, many major journals began requiring prospective registration of clinical trials as a condition for publication and the International Committee of Medical Journal Editors started requiring CONSORT reporting in all major journals beginning in 2004 (icjme. org). NHLBI was an early adopter of trial registration. All of their large trials published after 2000 were preregistered and transparently reported. Although we cannot say that stricter reporting requirements caused the trend toward more null reports from NHLBI trials, we do find the association worthy of more investigation.\n\nIn conclusion, null findings in large RCTs may be disappointing to investigators, but they are not negative for science. Properly powered trials might identify treatments that will improve public health. A growing collection of trials suggests that promising treatments do not match their potential when systematically tested and transparently reported. Publication of these trials may lead to the protection of patients from treatments that use resources while not enhancing patient outcomes. For example, a recent economic analysis of the Women's Health Initiative clinical trial suggested that the publication of the study may have resulted in 126,000 fewer breast cancer deaths, and 76,000 deaths from heart disease between 2003 and 2012. The economic analysis estimated that there was about $140 returned for each dollar invested in the study[13].",
    "is_useful": true,
    "question": "How do proper reporting and transparency in clinical trials contribute to public health outcomes?"
  },
  {
    "text": "Although we cannot say that stricter reporting requirements caused the trend toward more null reports from NHLBI trials, we do find the association worthy of more investigation.\n\nIn conclusion, null findings in large RCTs may be disappointing to investigators, but they are not negative for science. Properly powered trials might identify treatments that will improve public health. A growing collection of trials suggests that promising treatments do not match their potential when systematically tested and transparently reported. Publication of these trials may lead to the protection of patients from treatments that use resources while not enhancing patient outcomes. For example, a recent economic analysis of the Women's Health Initiative clinical trial suggested that the publication of the study may have resulted in 126,000 fewer breast cancer deaths, and 76,000 deaths from heart disease between 2003 and 2012. The economic analysis estimated that there was about $140 returned for each dollar invested in the study[13]. Transparent and impartial reporting of clinical trial results will ultimately identify the treatments most likely to maximize benefit and reduce harm.\n\n### Supporting Information\n\nS1 Fig. PRISMA Diagram. (TIFF) S2 Fig. Summary of results on all cause mortality. (TIF)\n\nS1 Table. Total number of returned searches in NIH grant databases, type and number of exclusions, number of grants identified, and total number of trials analyzed. (PDF)\n\nS2 Table. Sample sizes, number of primary outcome events, number of deaths from all causes for trials published before the year 2000. (PDF)\n\nS3 Table.",
    "is_useful": true,
    "question": "What are the implications of null findings in randomized controlled trials (RCTs) for public health and resource allocation?"
  },
  {
    "text": "![](_page_0_Picture_2.jpeg)\n\n## **Use of the Journal Impact Factor in academic review, promotion, and tenure evaluations**\n\nErin C. McKiernan1,*, Lesley A. Schimanski2 , Carol Mu\u00f1oz Nieves2 , Lisa Matthias3 , Meredith T. Niles4 , and Juan Pablo Alperin2,5**\n\n1Departamento de F\u00edsica, Facultad de Ciencias, Universidad Nacional Aut\u00f3noma de M\u00e9xico 2Scholarly Communications Lab, Simon Fraser University 3 John F. Kennedy Institute, Freie Universit\u00e4t Berlin 4Department of Nutrition and Food Sciences, Food Systems Program, University of Vermont 5School of Publishing, Simon Fraser University *Corresponding author: emckiernan@ciencias.unam.mx **Corresponding author: juan@alperin.ca\n\n#### 1 **Abstract**\n\n The Journal Impact Factor (JIF) was originally designed to aid libraries in deciding which journals to index and purchase for their collections. Over the past few decades, however, it has become a relied upon metric used to evaluate research articles based on journal rank. Surveyed faculty often report feeling pressure to publish in journals with high JIFs and mention reliance on the JIF as one problem with current academic evaluation systems. While faculty reports are useful, information is lacking on how often and in what ways the JIF is currently used for review, promotion, and tenure (RPT).",
    "is_useful": true,
    "question": "What metric is commonly used in academia to evaluate research articles based on journal rank and may influence publication decisions?"
  },
  {
    "text": "Over the past few decades, however, it has become a relied upon metric used to evaluate research articles based on journal rank. Surveyed faculty often report feeling pressure to publish in journals with high JIFs and mention reliance on the JIF as one problem with current academic evaluation systems. While faculty reports are useful, information is lacking on how often and in what ways the JIF is currently used for review, promotion, and tenure (RPT). We therefore collected and analyzed RPT documents from a representative sample of 129 universities from the United States and Canada and 381 of their academic units. We found that 40% of doctoral, research-intensive (R-type) institutions and 18% of master's, or comprehensive (M-type) institutions explicitly mentioned the JIF, or closely related terms, in their RPT documents. Undergraduate, or baccalaureate (B-type) institutions did not mention it at all. A detailed reading of these documents suggests that institutions may also be using a variety of terms to indirectly refer to the JIF. Our qualitative analysis shows that 87% of the institutions that mentioned the JIF supported the metric's use in at least one of their RPT documents, while 13% of institutions expressed caution about the JIF's use in evaluations. None of the RPT documents we analyzed heavily criticized the JIF or prohibited its use in evaluations. Of the institutions that mentioned the JIF, 63% associated it with quality, 40% with impact, importance, or significance, and 20% with prestige, reputation, or status.",
    "is_useful": true,
    "question": "What role does the Journal Impact Factor play in the evaluation of research articles in academic institutions?"
  },
  {
    "text": "Undergraduate, or baccalaureate (B-type) institutions did not mention it at all. A detailed reading of these documents suggests that institutions may also be using a variety of terms to indirectly refer to the JIF. Our qualitative analysis shows that 87% of the institutions that mentioned the JIF supported the metric's use in at least one of their RPT documents, while 13% of institutions expressed caution about the JIF's use in evaluations. None of the RPT documents we analyzed heavily criticized the JIF or prohibited its use in evaluations. Of the institutions that mentioned the JIF, 63% associated it with quality, 40% with impact, importance, or significance, and 20% with prestige, reputation, or status. In sum, our results show that the use of the JIF is encouraged in RPT evaluations, especially at research-intensive universities, and indicates there is work to be done to improve evaluation processes to avoid the potential misuse of metrics like the JIF.\n\n!",
    "is_useful": true,
    "question": "What is the general attitude of institutions towards the use of Journal Impact Factor (JIF) in evaluation processes based on recent analyses?"
  },
  {
    "text": "A detailed reading of these documents suggests that institutions may also be using a variety of terms to indirectly refer to the JIF. Our qualitative analysis shows that 87% of the institutions that mentioned the JIF supported the metric's use in at least one of their RPT documents, while 13% of institutions expressed caution about the JIF's use in evaluations. None of the RPT documents we analyzed heavily criticized the JIF or prohibited its use in evaluations. Of the institutions that mentioned the JIF, 63% associated it with quality, 40% with impact, importance, or significance, and 20% with prestige, reputation, or status. In sum, our results show that the use of the JIF is encouraged in RPT evaluations, especially at research-intensive universities, and indicates there is work to be done to improve evaluation processes to avoid the potential misuse of metrics like the JIF.\n\n![](_page_1_Picture_2.jpeg)\n\n## 23 **Introduction**\n\n Originally developed to help libraries make indexing and purchasing decisions for their journal collections (Archambault & Larivi\u00e8re, 2009; Garfield, 2006; Haustein & Larivi\u00e8re, 2015), the Journal Impact Factor (JIF) has moved beyond libraries and into the realm of research evaluation, despite the wide criticisms and well-documented limitations of the metric (e.g., Brembs et al., 2013; Haustein & Larivi\u00e8re, 2015; Kurmis, 2003; Moustafa, 2015; PLoS Medicine Editors, 2006; Seglen, 1997; Sugimoto & Larivi\u00e8re, 2018; The Analogue University, 2019).",
    "is_useful": true,
    "question": "What percentage of institutions expressed caution about the use of the Journal Impact Factor in evaluations, according to recent analyses of research evaluation documents?"
  },
  {
    "text": "![](_page_1_Picture_2.jpeg)\n\n## 23 **Introduction**\n\n Originally developed to help libraries make indexing and purchasing decisions for their journal collections (Archambault & Larivi\u00e8re, 2009; Garfield, 2006; Haustein & Larivi\u00e8re, 2015), the Journal Impact Factor (JIF) has moved beyond libraries and into the realm of research evaluation, despite the wide criticisms and well-documented limitations of the metric (e.g., Brembs et al., 2013; Haustein & Larivi\u00e8re, 2015; Kurmis, 2003; Moustafa, 2015; PLoS Medicine Editors, 2006; Seglen, 1997; Sugimoto & Larivi\u00e8re, 2018; The Analogue University, 2019). Even the metric's own creator, Eugene Garfield, made it clear that the JIF is not appropriate for evaluating individuals or for assessing the importance and significance of individual works (Garfield, 1963). Yet, substantial increases in publication rates and the number of academics competing for grants, jobs, and promotions over the past few decades (i.e., 'hypercompetition') have in part led academics to rely on the JIF as a proxy measure to quickly rank journals and, by extension, the articles published in these journals and the individuals authoring them (Casadevall & Fang, 2014).",
    "is_useful": true,
    "question": "What is the primary purpose of the Journal Impact Factor (JIF) and how has its use evolved in the context of research evaluation?"
  },
  {
    "text": "Even the metric's own creator, Eugene Garfield, made it clear that the JIF is not appropriate for evaluating individuals or for assessing the importance and significance of individual works (Garfield, 1963). Yet, substantial increases in publication rates and the number of academics competing for grants, jobs, and promotions over the past few decades (i.e., 'hypercompetition') have in part led academics to rely on the JIF as a proxy measure to quickly rank journals and, by extension, the articles published in these journals and the individuals authoring them (Casadevall & Fang, 2014). The association between the JIF, journal prestige, and selectivity is strong, and has led academics to covet publications in journals with high JIFs (Harley et al., 2010). Publishers, in turn, promote their JIF to attract academic authors (Hecht\n\n- 38 et al., 1998; SpringerNature, 2018; Sugimoto & Larivi\u00e8re, 2018).\n In some academic disciplines, it is considered necessary to have publications in journals with high JIFs to succeed, especially for those on the tenure track (for review see Schimanski & Alperin, 2018). There are even institutions in some countries that financially reward their faculty for publishing in journals with high JIFs (Fuyuno & Cyranoski, 2006; Quan et al., 2017), demonstrating an extreme but important example of how reliance on this metric may be distorting academic incentives.",
    "is_useful": true,
    "question": "What are the potential consequences of relying on journal impact factors (JIF) in academia?"
  },
  {
    "text": "The association between the JIF, journal prestige, and selectivity is strong, and has led academics to covet publications in journals with high JIFs (Harley et al., 2010). Publishers, in turn, promote their JIF to attract academic authors (Hecht\n\n- 38 et al., 1998; SpringerNature, 2018; Sugimoto & Larivi\u00e8re, 2018).\n In some academic disciplines, it is considered necessary to have publications in journals with high JIFs to succeed, especially for those on the tenure track (for review see Schimanski & Alperin, 2018). There are even institutions in some countries that financially reward their faculty for publishing in journals with high JIFs (Fuyuno & Cyranoski, 2006; Quan et al., 2017), demonstrating an extreme but important example of how reliance on this metric may be distorting academic incentives. Even when the incentives are not so clear-cut, faculty still often report intense pressure to publish in these venues (Harley et al., 2010; Tijdink et al., 2016; Walker et al., 2010). Faculty also report that concerns about the JIF and journals' perceived prestige are limiting factors in their adoption\n\n47 of open access publishing (of California Libraries; Schroter et al., 2005; Swan & Brown, 2004), 48 indicating how the effects of the JIF permeate to the broader scholarly publishing ecosystem.",
    "is_useful": true,
    "question": "How does the Journal Impact Factor (JIF) influence academic publishing and the adoption of open access in scholarly communication?"
  },
  {
    "text": "There are even institutions in some countries that financially reward their faculty for publishing in journals with high JIFs (Fuyuno & Cyranoski, 2006; Quan et al., 2017), demonstrating an extreme but important example of how reliance on this metric may be distorting academic incentives. Even when the incentives are not so clear-cut, faculty still often report intense pressure to publish in these venues (Harley et al., 2010; Tijdink et al., 2016; Walker et al., 2010). Faculty also report that concerns about the JIF and journals' perceived prestige are limiting factors in their adoption\n\n47 of open access publishing (of California Libraries; Schroter et al., 2005; Swan & Brown, 2004), 48 indicating how the effects of the JIF permeate to the broader scholarly publishing ecosystem.\n\n49 This use \u2014 and potential misuse \u2014 of the JIF to evaluate research and researchers is often raised\n\n50 in broader discussions about the many problems with current academic evaluation systems (Moher\n\n51 et al., 2018). However, while anecdotal information or even formal surveys of faculty are useful in\n\n52 gauging its effect on the academic system, there is still a lot we do not know about the extent to\n\n53 which the JIF is used in formal academic evaluations. To our knowledge, there have been no studies\n\n54 analyzing the content of university review, promotion, and tenure (RPT) guidelines to determine\n\n55 the extent to which the JIF is being used to evaluate faculty, or in what ways.",
    "is_useful": true,
    "question": "How does the Journal Impact Factor (JIF) influence academic publishing decisions and evaluation systems in scholarly research?"
  },
  {
    "text": "49 This use \u2014 and potential misuse \u2014 of the JIF to evaluate research and researchers is often raised\n\n50 in broader discussions about the many problems with current academic evaluation systems (Moher\n\n51 et al., 2018). However, while anecdotal information or even formal surveys of faculty are useful in\n\n52 gauging its effect on the academic system, there is still a lot we do not know about the extent to\n\n53 which the JIF is used in formal academic evaluations. To our knowledge, there have been no studies\n\n54 analyzing the content of university review, promotion, and tenure (RPT) guidelines to determine\n\n55 the extent to which the JIF is being used to evaluate faculty, or in what ways. We therefore sought\n\n56 to answer the following questions: (1) How often is the JIF, and closely related terms, mentioned\n\n57 in RPT documents? (2) Are the JIF mentions supportive or cautionary? and (3) What do RPT\n\n58 documents assume the JIF measures? In the process of answering these questions, our study\n\n59 offered an opportunity to explore the context surrounding mentions of the JIF to qualitatively assess\n\n60 its use in the documents that guide formal evaluation.\n\n!",
    "is_useful": true,
    "question": "What are the potential issues surrounding the use of Journal Impact Factor (JIF) in academic evaluation systems?"
  },
  {
    "text": "To our knowledge, there have been no studies\n\n54 analyzing the content of university review, promotion, and tenure (RPT) guidelines to determine\n\n55 the extent to which the JIF is being used to evaluate faculty, or in what ways. We therefore sought\n\n56 to answer the following questions: (1) How often is the JIF, and closely related terms, mentioned\n\n57 in RPT documents? (2) Are the JIF mentions supportive or cautionary? and (3) What do RPT\n\n58 documents assume the JIF measures? In the process of answering these questions, our study\n\n59 offered an opportunity to explore the context surrounding mentions of the JIF to qualitatively assess\n\n60 its use in the documents that guide formal evaluation.\n\n![](_page_2_Picture_2.jpeg)\n\n## 61 **Methods**\n\n3\n\n### 62 **Document collection**\n\n This paper reports a set of findings from a larger study (Alperin et al., 2019) for which we collected documents related to the RPT process from a representative sample of universities in the United States and Canada and many of their academic units. A detailed description of the methods for selecting institutions to include in our sample, how we classified them, how we collected documents, and the analysis approach is included in Alperin et al. (2019) and in the methodological note accompanying the public dataset Alperin et al. (2018).",
    "is_useful": true,
    "question": "What are some potential areas of investigation regarding the use of Journal Impact Factor (JIF) in university review, promotion, and tenure guidelines?"
  },
  {
    "text": "In the process of answering these questions, our study\n\n59 offered an opportunity to explore the context surrounding mentions of the JIF to qualitatively assess\n\n60 its use in the documents that guide formal evaluation.\n\n![](_page_2_Picture_2.jpeg)\n\n## 61 **Methods**\n\n3\n\n### 62 **Document collection**\n\n This paper reports a set of findings from a larger study (Alperin et al., 2019) for which we collected documents related to the RPT process from a representative sample of universities in the United States and Canada and many of their academic units. A detailed description of the methods for selecting institutions to include in our sample, how we classified them, how we collected documents, and the analysis approach is included in Alperin et al. (2019) and in the methodological note accompanying the public dataset Alperin et al. (2018). Briefly, we used the 2015 edition of the Carnegie Classification of Institutions of Higher Education (Carnegie Foundation for the Advancement of Teaching, 2015) and the 2016 edition of the Maclean's University Rankings (Rogers Digital Media, 2016), which respectively group U.S. and Canadian universities into those focused on doctoral programs (i.e., research intensive; R-type), those that predominantly grant master's degrees (M-type), and those that focus on undergraduate programs (i.e., baccalaureate; B-type).",
    "is_useful": true,
    "question": "What methods are typically used to assess the impact of research evaluation documents in universities?"
  },
  {
    "text": "A detailed description of the methods for selecting institutions to include in our sample, how we classified them, how we collected documents, and the analysis approach is included in Alperin et al. (2019) and in the methodological note accompanying the public dataset Alperin et al. (2018). Briefly, we used the 2015 edition of the Carnegie Classification of Institutions of Higher Education (Carnegie Foundation for the Advancement of Teaching, 2015) and the 2016 edition of the Maclean's University Rankings (Rogers Digital Media, 2016), which respectively group U.S. and Canadian universities into those focused on doctoral programs (i.e., research intensive; R-type), those that predominantly grant master's degrees (M-type), and those that focus on undergraduate programs (i.e., baccalaureate; B-type). We classified academic units (e.g., department, school, or faculty) within an institution by discipline using the National Academies Taxonomy (The National Academies of Sciences, Engineering, and Medicine, 2006) into three major areas: Life Sciences (LS); Physical Sciences and Mathematics (PSM); and Social Sciences and Humanities (SSH). Additional units that could not be classified as belonging to a single area (e.g., a College of Arts & Sciences) were designated as multidisciplinary. We then used a combination of web searches, crowdsourcing, and targeted emailing to request documents related to the RPT process, including but not limited to collective agreements, faculty handbooks, guidelines, and forms.",
    "is_useful": true,
    "question": "What methods are used to classify and select institutions for research in open science?"
  },
  {
    "text": "and Canadian universities into those focused on doctoral programs (i.e., research intensive; R-type), those that predominantly grant master's degrees (M-type), and those that focus on undergraduate programs (i.e., baccalaureate; B-type). We classified academic units (e.g., department, school, or faculty) within an institution by discipline using the National Academies Taxonomy (The National Academies of Sciences, Engineering, and Medicine, 2006) into three major areas: Life Sciences (LS); Physical Sciences and Mathematics (PSM); and Social Sciences and Humanities (SSH). Additional units that could not be classified as belonging to a single area (e.g., a College of Arts & Sciences) were designated as multidisciplinary. We then used a combination of web searches, crowdsourcing, and targeted emailing to request documents related to the RPT process, including but not limited to collective agreements, faculty handbooks, guidelines, and forms. Some of these documents applied to the institution as a whole, while others applied only to specific academic units.\n\n In the end, we obtained 864 documents related to the RPT process of 129 universities and of 381 academic units. These included documents from 57 R-type, 39 M-type, and 33 B-type institutions.",
    "is_useful": true,
    "question": "What types of academic institutions and their classifications were analyzed in the context of the RPT process?"
  },
  {
    "text": "Additional units that could not be classified as belonging to a single area (e.g., a College of Arts & Sciences) were designated as multidisciplinary. We then used a combination of web searches, crowdsourcing, and targeted emailing to request documents related to the RPT process, including but not limited to collective agreements, faculty handbooks, guidelines, and forms. Some of these documents applied to the institution as a whole, while others applied only to specific academic units.\n\n In the end, we obtained 864 documents related to the RPT process of 129 universities and of 381 academic units. These included documents from 57 R-type, 39 M-type, and 33 B-type institutions. The documents from the 381 academic units came from 60 of the 129 universities in the sample and included documents from 98 (25.7%) LS units, 69 (18.1%) PSM units, 187 (49.1%) SSH units, and 27 (7.1%) multidisciplinary units. However, to avoid pooling academic units from different institution types, and based on sample size considerations, we limited our disciplinary analysis to academic units from R-type institutions: 33 (28%) LS units, 21 (18%) PSM units, 39 (34%) SSH units, and 23 (20%) multidisciplinary units.\n\n## 91 **Document analysis and coding terminology**\n\n The RPT documents were loaded into QSR International's NVivo 12 qualitative data analysis software, where text queries were used to identify documents that mention specific terms.",
    "is_useful": true,
    "question": "What methods were used to gather documents related to the RPT process in universities?"
  },
  {
    "text": "The documents from the 381 academic units came from 60 of the 129 universities in the sample and included documents from 98 (25.7%) LS units, 69 (18.1%) PSM units, 187 (49.1%) SSH units, and 27 (7.1%) multidisciplinary units. However, to avoid pooling academic units from different institution types, and based on sample size considerations, we limited our disciplinary analysis to academic units from R-type institutions: 33 (28%) LS units, 21 (18%) PSM units, 39 (34%) SSH units, and 23 (20%) multidisciplinary units.\n\n## 91 **Document analysis and coding terminology**\n\n The RPT documents were loaded into QSR International's NVivo 12 qualitative data analysis software, where text queries were used to identify documents that mention specific terms. Because the language in RPT documents varies, we first searched all the documents for the words \"impact\" and \"journal\", and read each mention to identify terms that may be referencing the JIF. We classified these terms into three groups: (1) direct references to the JIF as a metric; (2) those that reference\n\n![](_page_3_Picture_2.jpeg)\n\n journal impact in some way; and (3) indirect but possible references to the JIF. In the first group, we included the terms \"impact factor\", \"impact score\", \"impact metric\", and \"impact index\".",
    "is_useful": true,
    "question": "What types of academic units were included in the analysis of documents related to impact metrics in open science?"
  },
  {
    "text": "## 91 **Document analysis and coding terminology**\n\n The RPT documents were loaded into QSR International's NVivo 12 qualitative data analysis software, where text queries were used to identify documents that mention specific terms. Because the language in RPT documents varies, we first searched all the documents for the words \"impact\" and \"journal\", and read each mention to identify terms that may be referencing the JIF. We classified these terms into three groups: (1) direct references to the JIF as a metric; (2) those that reference\n\n![](_page_3_Picture_2.jpeg)\n\n journal impact in some way; and (3) indirect but possible references to the JIF. In the first group, we included the terms \"impact factor\", \"impact score\", \"impact metric\", and \"impact index\". In the second group, we included the terms \"high-impact journal\", \"impact of the journal\", and \"journal('s) impact\". The third group contains a larger number and variety of terms, such as \"high-ranking journal\", \"top-tier journal\", and \"prestigious journal\". For all terms, we considered both singular and plural equivalents. A map of the terms we found and their grouping into the three categories can be seen in Fig. 1. In our analysis, we looked at only the first two groups of terms, as we considered them to be unambiguously about the JIF (group 1) or sufficiently close to the notion of JIF (group 2).",
    "is_useful": true,
    "question": "What are the classifications used for terms referencing Journal Impact Factor (JIF) in qualitative data analysis of research documents?"
  },
  {
    "text": "In the first group, we included the terms \"impact factor\", \"impact score\", \"impact metric\", and \"impact index\". In the second group, we included the terms \"high-impact journal\", \"impact of the journal\", and \"journal('s) impact\". The third group contains a larger number and variety of terms, such as \"high-ranking journal\", \"top-tier journal\", and \"prestigious journal\". For all terms, we considered both singular and plural equivalents. A map of the terms we found and their grouping into the three categories can be seen in Fig. 1. In our analysis, we looked at only the first two groups of terms, as we considered them to be unambiguously about the JIF (group 1) or sufficiently close to the notion of JIF (group 2). The terms in the third group, however, may or may not refer to the JIF. So while these terms could represent examples of ways in which the idea of the JIF is invoked without begin explicit, their mentions were not analyzed further for this study.\n\n The results of each text query for the terms in groups 1 and 2 were placed in an NVivo \"node\" that contained the text surrounding each of the mentions.",
    "is_useful": true,
    "question": "What are the different groups of terms associated with journal impact and metrics examined in open science research? "
  },
  {
    "text": "For all terms, we considered both singular and plural equivalents. A map of the terms we found and their grouping into the three categories can be seen in Fig. 1. In our analysis, we looked at only the first two groups of terms, as we considered them to be unambiguously about the JIF (group 1) or sufficiently close to the notion of JIF (group 2). The terms in the third group, however, may or may not refer to the JIF. So while these terms could represent examples of ways in which the idea of the JIF is invoked without begin explicit, their mentions were not analyzed further for this study.\n\n The results of each text query for the terms in groups 1 and 2 were placed in an NVivo \"node\" that contained the text surrounding each of the mentions. We then performed a \"matrix coding query\" to produce a table with institutions and academic units as rows, terms of interests as columns, and a 1 or a 0 indicating whether the institution or academic unit made mention of the term or not, with the ability to distinguish if the mention appeared in documents that pertain to the whole institution, to one or more academic units, or both. We considered an institution as making mention of a term if the term was present in at least one document from that institution or any of its academic units. More details on this process can be found in Alperin et al. (2019).",
    "is_useful": true,
    "question": "How are terms related to the Journal Impact Factor categorized and analyzed in the study?"
  },
  {
    "text": "The results of each text query for the terms in groups 1 and 2 were placed in an NVivo \"node\" that contained the text surrounding each of the mentions. We then performed a \"matrix coding query\" to produce a table with institutions and academic units as rows, terms of interests as columns, and a 1 or a 0 indicating whether the institution or academic unit made mention of the term or not, with the ability to distinguish if the mention appeared in documents that pertain to the whole institution, to one or more academic units, or both. We considered an institution as making mention of a term if the term was present in at least one document from that institution or any of its academic units. More details on this process can be found in Alperin et al. (2019).\n\n## 116 **Qualitative analysis**\n\n We also exported the content of each node for a qualitative analysis of the JIF mentions. In some cases, the software extracted complete sentences, while in other cases it pulled only fragments and we retrieved the rest of the text manually to provide better context. Based on a detailed reading of the text, we classified each of the JIF mentions along two dimensions. First, we classified each mention as either: (1) *supportive* of the JIF's use in evaluations; (2) *cautious*, meaning the document expresses some reservations about the use of the JIF in evaluations; or (3) *neutral*, meaning the mention was neither supportive nor cautious, or not enough information was present in the document to make a judgement.",
    "is_useful": true,
    "question": "How do institutions and academic units participate in discussions about the use of Journal Impact Factors (JIF) in evaluations?"
  },
  {
    "text": "More details on this process can be found in Alperin et al. (2019).\n\n## 116 **Qualitative analysis**\n\n We also exported the content of each node for a qualitative analysis of the JIF mentions. In some cases, the software extracted complete sentences, while in other cases it pulled only fragments and we retrieved the rest of the text manually to provide better context. Based on a detailed reading of the text, we classified each of the JIF mentions along two dimensions. First, we classified each mention as either: (1) *supportive* of the JIF's use in evaluations; (2) *cautious*, meaning the document expresses some reservations about the use of the JIF in evaluations; or (3) *neutral*, meaning the mention was neither supportive nor cautious, or not enough information was present in the document to make a judgement. In addition, we read each mention to determine what aspects of research were being measured with the JIF, if specified. Using categories we arrived at inductively, we classified each mention of the JIF as associating the metric with one or more of the following: (i) quality of the research and/or journal; (ii) impact, importance, or significance of the research or publication; (iii) prestige, reputation, or status of the journal or publication; or (iv) left unspecified, meaning the document mentions the JIF, but does not state what the metric is intended to measure. If an institution contained multiple mentions (for example, in two different academic units), it was counted under all the relevant categories.",
    "is_useful": true,
    "question": "How are mentions of journal impact factors (JIF) classified in qualitative analyses related to open science?"
  },
  {
    "text": "In addition, we read each mention to determine what aspects of research were being measured with the JIF, if specified. Using categories we arrived at inductively, we classified each mention of the JIF as associating the metric with one or more of the following: (i) quality of the research and/or journal; (ii) impact, importance, or significance of the research or publication; (iii) prestige, reputation, or status of the journal or publication; or (iv) left unspecified, meaning the document mentions the JIF, but does not state what the metric is intended to measure. If an institution contained multiple mentions (for example, in two different academic units), it was counted under all the relevant categories.\n\n132 To arrive at the classification, each mention was independently coded by two of the authors (EM 133 and LM) using the definitions above. After an initial pass, the two coders agreed on all of the\n\n![](_page_4_Picture_2.jpeg)\n\n**Figure 1: Grouping of terms related to the JIF**. Terms found in RPT documents were classified as either: (1) referring directly to the JIF (inner ring); (2) referring in some way to journal impact (middle ring); or (3) indirect but probable references to the JIF. For simplicity, singular versions of each term are shown, but searches included their plural equivalents. Our analysis is based only on those terms found in groups 1 and 2 (the two innermost rings).\n\n134 classifications for 86% of all mentions.",
    "is_useful": true,
    "question": "What categories can research be classified into based on the Journal Impact Factor as mentioned in research assessments?"
  },
  {
    "text": "132 To arrive at the classification, each mention was independently coded by two of the authors (EM 133 and LM) using the definitions above. After an initial pass, the two coders agreed on all of the\n\n![](_page_4_Picture_2.jpeg)\n\n**Figure 1: Grouping of terms related to the JIF**. Terms found in RPT documents were classified as either: (1) referring directly to the JIF (inner ring); (2) referring in some way to journal impact (middle ring); or (3) indirect but probable references to the JIF. For simplicity, singular versions of each term are shown, but searches included their plural equivalents. Our analysis is based only on those terms found in groups 1 and 2 (the two innermost rings).\n\n134 classifications for 86% of all mentions. The remaining mentions were independently coded by a\n\n135 third author (LS). In all instances, the third coder agreed with one of the previous two, and this\n\n- 136 agreement was taken as the final code.",
    "is_useful": true,
    "question": "What methodology is used to classify terms related to journal impact factors in open science research? "
  },
  {
    "text": "After an initial pass, the two coders agreed on all of the\n\n![](_page_4_Picture_2.jpeg)\n\n**Figure 1: Grouping of terms related to the JIF**. Terms found in RPT documents were classified as either: (1) referring directly to the JIF (inner ring); (2) referring in some way to journal impact (middle ring); or (3) indirect but probable references to the JIF. For simplicity, singular versions of each term are shown, but searches included their plural equivalents. Our analysis is based only on those terms found in groups 1 and 2 (the two innermost rings).\n\n134 classifications for 86% of all mentions. The remaining mentions were independently coded by a\n\n135 third author (LS). In all instances, the third coder agreed with one of the previous two, and this\n\n- 136 agreement was taken as the final code.\n# 137 **Data availability**\n\n6\n\n- 138 We have shared the data on which this paper is based in two different formats: (1) a spreadsheet\n- 139 with all the JIF-related mentions (including repetitions) extracted from the RPT documents, available\n- 140 as part of the larger public dataset (Alperin et al., 2018), and (2) a text document containing the\n- 141 mentions (minus repetitions), with terms of interest color coded and a qualitative assessment of\n- 142 each quote, available as supplemental information.",
    "is_useful": true,
    "question": "How is data related to journal impact factor made available for open science?"
  },
  {
    "text": "Our analysis is based only on those terms found in groups 1 and 2 (the two innermost rings).\n\n134 classifications for 86% of all mentions. The remaining mentions were independently coded by a\n\n135 third author (LS). In all instances, the third coder agreed with one of the previous two, and this\n\n- 136 agreement was taken as the final code.\n# 137 **Data availability**\n\n6\n\n- 138 We have shared the data on which this paper is based in two different formats: (1) a spreadsheet\n- 139 with all the JIF-related mentions (including repetitions) extracted from the RPT documents, available\n- 140 as part of the larger public dataset (Alperin et al., 2018), and (2) a text document containing the\n- 141 mentions (minus repetitions), with terms of interest color coded and a qualitative assessment of\n- 142 each quote, available as supplemental information. We are not able to share the original RPT\n- 143 documents collected for this study, since the copyrights are held by the universities and academic\n- 144 units that created them. However, for publicly available documents, we included Wayback Machine\n- 145 web archive links to them in the shared spreadsheet.\n\n## 146 **Results**\n\n#### 147 **How often is the JIF mentioned in RPT documents?",
    "is_useful": true,
    "question": "What measures are taken to ensure data availability in open science research?"
  },
  {
    "text": "We are not able to share the original RPT\n- 143 documents collected for this study, since the copyrights are held by the universities and academic\n- 144 units that created them. However, for publicly available documents, we included Wayback Machine\n- 145 web archive links to them in the shared spreadsheet.\n\n## 146 **Results**\n\n#### 147 **How often is the JIF mentioned in RPT documents?**\n\n- 148 While metrics in general are mentioned in RPT documents from 50% of institutions in our sample\n- 149 (Alperin et al., 2019), only 23% (30 of 129) of the institutions mentioned the JIF explicitly or used\n- 150 one of the JIF-related terms (see groups 1 and 2 in Fig. 1) in their RPT documents. The percentage\n- 151 was higher for R-type institutions (23 of 57; 40%) than for either M-type (7 of 39: 18%) or B-type (0\n- 152 of 33; 0%) institutions (Table 1). Some mentions were found in the institutional-level documents,\n- 153 while others were found at the level of the academic unit (e.g., college, school, or department).\n- 154 Many of the mentions were from different academic units within the same university.",
    "is_useful": true,
    "question": "What percentage of institutions in a study mentioned the Journal Impact Factor in their RPT documents?"
  },
  {
    "text": "1) in their RPT documents. The percentage\n- 151 was higher for R-type institutions (23 of 57; 40%) than for either M-type (7 of 39: 18%) or B-type (0\n- 152 of 33; 0%) institutions (Table 1). Some mentions were found in the institutional-level documents,\n- 153 while others were found at the level of the academic unit (e.g., college, school, or department).\n- 154 Many of the mentions were from different academic units within the same university. Within the\n- 155 R-type institutions, the percentage of academic units that mention JIF-related terms was higher for\n- 156 LS (11 of 33; 33%) and PSM (6 of 21; 29%) than for SSH (8 of 39; 21%) or multidisciplinary units\n- 157 (4 of 23; 17%).\n\n#### 158 **Are the JIF mentions supportive or cautionary?**\n\n- 159 The majority of mentions of the JIF were supportive of the metric's use in evaluations. Overall,\n- 160 87% (26 of 30) of institutions that mentioned the JIF did so supportively in at least one of their\n- 161 RPT documents from our sample. Breaking down by institution type, 83% (19 of 23) of R-type and\n- 162 100% (7 of 7) of M-type institutions had supportive mentions (Table 1).",
    "is_useful": true,
    "question": "What percentage of institutions that mentioned the Journal Impact Factor (JIF) did so in a supportive manner in their evaluation documents?"
  },
  {
    "text": "#### 158 **Are the JIF mentions supportive or cautionary?**\n\n- 159 The majority of mentions of the JIF were supportive of the metric's use in evaluations. Overall,\n- 160 87% (26 of 30) of institutions that mentioned the JIF did so supportively in at least one of their\n- 161 RPT documents from our sample. Breaking down by institution type, 83% (19 of 23) of R-type and\n- 162 100% (7 of 7) of M-type institutions had supportive mentions (Table 1). In contrast, just 13% (4\n- 163 of 30) of institutions overall had at least one mention which expressed caution about using the\n- 164 JIF in evaluations (13% R-type; 14% M-type). Two institutions (University of Central Florida and\n- 165 University of Guelph) had both supportive and cautious mentions of the JIF, but originating from\n- 166 different academic units. Overall, 17% (5 of 30) of institutions had at least one neutral mention\n- 167 (17% R-type; 14% M-type). Examples of supportive and cautious mentions can be found in the\n- 168 following two sections. Examples of neutral mentions are in the supplemental information.\n\n!",
    "is_useful": true,
    "question": "What is the general attitude towards the use of Journal Impact Factor (JIF) in evaluations across various institutions?"
  },
  {
    "text": "In contrast, just 13% (4\n- 163 of 30) of institutions overall had at least one mention which expressed caution about using the\n- 164 JIF in evaluations (13% R-type; 14% M-type). Two institutions (University of Central Florida and\n- 165 University of Guelph) had both supportive and cautious mentions of the JIF, but originating from\n- 166 different academic units. Overall, 17% (5 of 30) of institutions had at least one neutral mention\n- 167 (17% R-type; 14% M-type). Examples of supportive and cautious mentions can be found in the\n- 168 following two sections. Examples of neutral mentions are in the supplemental information.\n\n![](_page_6_Picture_2.jpeg)\n\n**Table 1:** Mentions of the JIF in RPT documents, overall and by institution type\n\n|  |  | All | R-type | M-type | B-type |\n| --- | --- | --- | --- | --- | --- |\n| How many institutions |  |  |  |  |  |\n| mention the JIF? |  |  |  |  |  |\n|  | n | 129 | 57 | 39 | 33 |\n|  | JIF mentioned | 30 (23%) | 23 (40%) | 7 (18%) | 0 (0%) |\n| Are the JIF mentions |  |  |  |  |  |\n| supportive or cautionary?",
    "is_useful": true,
    "question": "What percentage of institutions expressed caution about using the Journal Impact Factor in evaluations?"
  },
  {
    "text": "Examples of supportive and cautious mentions can be found in the\n- 168 following two sections. Examples of neutral mentions are in the supplemental information.\n\n![](_page_6_Picture_2.jpeg)\n\n**Table 1:** Mentions of the JIF in RPT documents, overall and by institution type\n\n|  |  | All | R-type | M-type | B-type |\n| --- | --- | --- | --- | --- | --- |\n| How many institutions |  |  |  |  |  |\n| mention the JIF? |  |  |  |  |  |\n|  | n | 129 | 57 | 39 | 33 |\n|  | JIF mentioned | 30 (23%) | 23 (40%) | 7 (18%) | 0 (0%) |\n| Are the JIF mentions |  |  |  |  |  |\n| supportive or cautionary? |  |  |  |  |  |\n|  | n | 30 | 23 | 7 | 0 |\n|  | supportive | 26 (87%) | 19 (83%) | 7 (100%) | - |\n|  | cautious | 4 (13%) | 3 (13%) | 1 (14%) | - |\n|  | neutral | 5 (17%) | 4 (17%) | 1 (14%) | - |\n| What do institutions |  |  |  |  |  |\n| measure with the JIF?",
    "is_useful": true,
    "question": "What do institutions measure with the Journal Impact Factor (JIF)?"
  },
  {
    "text": "For example, an institution was marked as having a supportive mention if at least one RPT document from that institution, or any of its academic units, had a supportive mention. The same institution could also be counted under 'cautious' if a different academic unit within that institution had such a mention.\n\n#### 169 **What do RPT documents assume the JIF measures?**\n\n#### 170 **Associating the JIF with quality**\n\n The most common specified association we observed in these RPT documents was between the JIF and quality. Overall, 61% (14 of 23) of R-type and 71% (5 of 7) of M-type institutions that mention the JIF in our sample associate the metric with quality (Table 1). This association can be seen clearly in the guidelines from the Faculty of Science at the University of Alberta (University of Alberta, 2012) that state:\n\n![](_page_7_Picture_2.jpeg)\n\n\" Of all the criteria listed, the one used most extensively, and generally the most reliable, is the quality and quantity of published work in refereed venues of international stature. Impact factors and/or acceptance rates of refereed venues are useful measures of venue quality... \"\n\n While some RPT documents recommend using the JIF to determine the quality of a journal, others suggest that this metric can be used to indicate the quality of individual publications.",
    "is_useful": true,
    "question": "How do RPT documents generally associate the Journal Impact Factor (JIF) with the assessment of quality in academic work?"
  },
  {
    "text": "Overall, 61% (14 of 23) of R-type and 71% (5 of 7) of M-type institutions that mention the JIF in our sample associate the metric with quality (Table 1). This association can be seen clearly in the guidelines from the Faculty of Science at the University of Alberta (University of Alberta, 2012) that state:\n\n![](_page_7_Picture_2.jpeg)\n\n\" Of all the criteria listed, the one used most extensively, and generally the most reliable, is the quality and quantity of published work in refereed venues of international stature. Impact factors and/or acceptance rates of refereed venues are useful measures of venue quality... \"\n\n While some RPT documents recommend using the JIF to determine the quality of a journal, others suggest that this metric can be used to indicate the quality of individual publications. An example of the latter comes from the College of Health Sciences and Professions at Ohio University (Ohio University, 2014):\n\n\" Markers of quality of publications may include impact factors of journals, number of citations of published work, and audience of journal. \"\n\n182 Other guidelines create their own metrics using the JIF in their calculations and suggest this will in-183 centivize high quality research, as seen in the following example from the Institute of Environmental 184 Sustainability at Loyola University (Loyola University Chicago, 2015):\n\n\" For promotion to Professor, the candidate must have an average publication rate of at least one article per year published in peer-reviewed journals in the five-year period preceding the application for promotion.",
    "is_useful": true,
    "question": "What percentage of institutions associate the Journal Impact Factor (JIF) with quality in their guidelines?"
  },
  {
    "text": "An example of the latter comes from the College of Health Sciences and Professions at Ohio University (Ohio University, 2014):\n\n\" Markers of quality of publications may include impact factors of journals, number of citations of published work, and audience of journal. \"\n\n182 Other guidelines create their own metrics using the JIF in their calculations and suggest this will in-183 centivize high quality research, as seen in the following example from the Institute of Environmental 184 Sustainability at Loyola University (Loyola University Chicago, 2015):\n\n\" For promotion to Professor, the candidate must have an average publication rate of at least one article per year published in peer-reviewed journals in the five-year period preceding the application for promotion. These articles should be regularly cited by other researchers in the field. We will consider both the quality of the journal (as measured by the journal's impact factor, or JIF) as well as the number of citations of each publication. We will employ the metric: Article Impact Factor (AIF) = (JIF * citations) where \"citations\" represents the number of citations for the particular publication. Employing this metric, faculty have incentive to publish in the highest quality journals (which will increase the JIF) and simultaneously produce the highest quality research manuscripts, potentially\n\nincreasing the number of citations, and increasing the AIF. \" 186 In sum, there are repeated links made in the sampled RPT documents between the JIF, and 187 research, publication, or journal quality.",
    "is_useful": true,
    "question": "What factors are considered indicators of the quality of academic publications in the context of promotion within academic institutions?"
  },
  {
    "text": "These articles should be regularly cited by other researchers in the field. We will consider both the quality of the journal (as measured by the journal's impact factor, or JIF) as well as the number of citations of each publication. We will employ the metric: Article Impact Factor (AIF) = (JIF * citations) where \"citations\" represents the number of citations for the particular publication. Employing this metric, faculty have incentive to publish in the highest quality journals (which will increase the JIF) and simultaneously produce the highest quality research manuscripts, potentially\n\nincreasing the number of citations, and increasing the AIF. \" 186 In sum, there are repeated links made in the sampled RPT documents between the JIF, and 187 research, publication, or journal quality.\n\n#### 188 **Associating the JIF with impact, importance, or significance**\n\n189 The second most common specified association we observed in these RPT documents was 190 between the JIF and the impact, importance, or significance of faculty research or publications,\n\n176 181 185\n\n![](_page_8_Picture_2.jpeg)\n\n191 found in 40% (12 of 30) of institutions in our sample. By institution type, 35% (8 of 23) of R-type\n\n192 and 57% (4 of 7) of M-type institutions made this association (Table 1).",
    "is_useful": true,
    "question": "What metric is used to link journal quality with the impact of research publications?"
  },
  {
    "text": "186 In sum, there are repeated links made in the sampled RPT documents between the JIF, and 187 research, publication, or journal quality.\n\n#### 188 **Associating the JIF with impact, importance, or significance**\n\n189 The second most common specified association we observed in these RPT documents was 190 between the JIF and the impact, importance, or significance of faculty research or publications,\n\n176 181 185\n\n![](_page_8_Picture_2.jpeg)\n\n191 found in 40% (12 of 30) of institutions in our sample. By institution type, 35% (8 of 23) of R-type\n\n192 and 57% (4 of 7) of M-type institutions made this association (Table 1). For example, guidelines\n\n193 from the Department of Psychology at Simon Fraser University (Simon Fraser University, 2015)\n\n194 link the JIF with impact:\n\n\" The TPC [Tenure and Promotion Committee] may additionally consider metrics such as citation figures, impact factors, or other such measures of the reach and impact of the candidate's scholarship. \"\n\n195\n\n196 Promotion and tenure criteria from the University of Windsor (University of Windsor, 2016) link the 197 JIF to publication importance:\n\n\" Candidates will be encouraged to submit a statement that explains the importance of their publications, which may include factors such as journal impact factors, citation rates, publication in journals with low acceptance rates, high levels of readership, demonstrated importance to their field. \"",
    "is_useful": true,
    "question": "What associations are commonly made in academic guidelines regarding journal impact factors and the quality or significance of research publications?"
  },
  {
    "text": "By institution type, 35% (8 of 23) of R-type\n\n192 and 57% (4 of 7) of M-type institutions made this association (Table 1). For example, guidelines\n\n193 from the Department of Psychology at Simon Fraser University (Simon Fraser University, 2015)\n\n194 link the JIF with impact:\n\n\" The TPC [Tenure and Promotion Committee] may additionally consider metrics such as citation figures, impact factors, or other such measures of the reach and impact of the candidate's scholarship. \"\n\n195\n\n196 Promotion and tenure criteria from the University of Windsor (University of Windsor, 2016) link the 197 JIF to publication importance:\n\n\" Candidates will be encouraged to submit a statement that explains the importance of their publications, which may include factors such as journal impact factors, citation rates, publication in journals with low acceptance rates, high levels of readership, demonstrated importance to their field. \"\n\n198\n\n199 Guidelines from the Institute of Environmental Sustainability at Loyola University (Loyola University 200 Chicago, 2015) associate the JIF with scientific significance:\n\n\" Candidates should have at least four manuscripts in peer-reviewed journals published or in-press in the five years preceding application for tenure and promotion to Associate Professor. The length of articles and scientific significance, as measured by citations and journal impact factor, will also be considered, as will authorship on contributions to other scholarly works (e.g., reference and text books). \"",
    "is_useful": true,
    "question": "How do academic institutions incorporate journal impact factors into their promotion and tenure criteria?"
  },
  {
    "text": "195\n\n196 Promotion and tenure criteria from the University of Windsor (University of Windsor, 2016) link the 197 JIF to publication importance:\n\n\" Candidates will be encouraged to submit a statement that explains the importance of their publications, which may include factors such as journal impact factors, citation rates, publication in journals with low acceptance rates, high levels of readership, demonstrated importance to their field. \"\n\n198\n\n199 Guidelines from the Institute of Environmental Sustainability at Loyola University (Loyola University 200 Chicago, 2015) associate the JIF with scientific significance:\n\n\" Candidates should have at least four manuscripts in peer-reviewed journals published or in-press in the five years preceding application for tenure and promotion to Associate Professor. The length of articles and scientific significance, as measured by citations and journal impact factor, will also be considered, as will authorship on contributions to other scholarly works (e.g., reference and text books). \"\n\n- 201\n202 In all of the above cases, the value of faculty research or individual publications is being evaluated, 203 at least in part, based on the JIF.\n\n#### 204 **Associating the JIF with prestige, reputation, or status**\n\n A third set of mentions of the JIF associated the metric with prestige, reputation, or status, typically referring to the publication venue. Overall, 20% (6 of 30) of institutions in our sample that mentioned the JIF made such an association.",
    "is_useful": true,
    "question": "How do institutions assess the value of faculty research or individual publications in connection to journal impact factors?"
  },
  {
    "text": "The length of articles and scientific significance, as measured by citations and journal impact factor, will also be considered, as will authorship on contributions to other scholarly works (e.g., reference and text books). \"\n\n- 201\n202 In all of the above cases, the value of faculty research or individual publications is being evaluated, 203 at least in part, based on the JIF.\n\n#### 204 **Associating the JIF with prestige, reputation, or status**\n\n A third set of mentions of the JIF associated the metric with prestige, reputation, or status, typically referring to the publication venue. Overall, 20% (6 of 30) of institutions in our sample that mentioned the JIF made such an association. As with other concepts, there was variability by institution type, with 22% (5 of 23) of the R-type and 14% (1 of 7) of the M-type having at least one instance of this\n\n![](_page_9_Picture_2.jpeg)\n\n209 association (Table 1). For example, guidelines from the Department of Sociology at the University 210 of Central Florida (University of Central Florida, 2015) link the JIF with prestige:\n\n\" It is also true that some refereed journal outlets count for more than others. Publication in respected, highly cited journals, that is, counts for more than publication in unranked journals.",
    "is_useful": true,
    "question": "How is the Journal Impact Factor (JIF) related to the evaluation of faculty research and publications in terms of prestige and reputation?"
  },
  {
    "text": "Overall, 20% (6 of 30) of institutions in our sample that mentioned the JIF made such an association. As with other concepts, there was variability by institution type, with 22% (5 of 23) of the R-type and 14% (1 of 7) of the M-type having at least one instance of this\n\n![](_page_9_Picture_2.jpeg)\n\n209 association (Table 1). For example, guidelines from the Department of Sociology at the University 210 of Central Florida (University of Central Florida, 2015) link the JIF with prestige:\n\n\" It is also true that some refereed journal outlets count for more than others. Publication in respected, highly cited journals, that is, counts for more than publication in unranked journals. The top journals in sociology and all other social sciences are ranked in the Thompson/ISI citation data base (which generates the well-known Impact Factors), in the Scopus data base, and in certain other citation data bases. In general, it behooves faculty to be aware of the prestige rankings of the field's journals and to publish in the highest-ranked journals possible. It is also advisable to include in one's tenure and promotion file information about the Impact Factors or related metrics for the journals\n\nwhere one's papers appear. \"",
    "is_useful": true,
    "question": "What percentage of institutions in the sample made an association with the Journal Impact Factor (JIF)?"
  },
  {
    "text": "For example, guidelines from the Department of Sociology at the University 210 of Central Florida (University of Central Florida, 2015) link the JIF with prestige:\n\n\" It is also true that some refereed journal outlets count for more than others. Publication in respected, highly cited journals, that is, counts for more than publication in unranked journals. The top journals in sociology and all other social sciences are ranked in the Thompson/ISI citation data base (which generates the well-known Impact Factors), in the Scopus data base, and in certain other citation data bases. In general, it behooves faculty to be aware of the prestige rankings of the field's journals and to publish in the highest-ranked journals possible. It is also advisable to include in one's tenure and promotion file information about the Impact Factors or related metrics for the journals\n\nwhere one's papers appear. \" 212 An evaluation rubric from the University of Windsor (University of Windsor, 2016) links the JIF with 213 journal reputation:\n\n\" a) Publishes in journals or with publishing houses with a strong academic reputation2\n\n2Departments may wish to provide quantitative metrics such as journal impact factors as an element of their standards. Factors such as low acceptance rates, high levels of readership, importance to the field are also suggestive indicators in assessing quality\n\nand reputation. \"",
    "is_useful": true,
    "question": "What is the importance of publishing in highly cited journals within academic disciplines according to open science guidelines?"
  },
  {
    "text": "In general, it behooves faculty to be aware of the prestige rankings of the field's journals and to publish in the highest-ranked journals possible. It is also advisable to include in one's tenure and promotion file information about the Impact Factors or related metrics for the journals\n\nwhere one's papers appear. \" 212 An evaluation rubric from the University of Windsor (University of Windsor, 2016) links the JIF with 213 journal reputation:\n\n\" a) Publishes in journals or with publishing houses with a strong academic reputation2\n\n2Departments may wish to provide quantitative metrics such as journal impact factors as an element of their standards. Factors such as low acceptance rates, high levels of readership, importance to the field are also suggestive indicators in assessing quality\n\nand reputation. \" 215 Similarly, promotion and tenure forms from the University of Vermont (University of Vermont, 2016) 216 associate the JIF with journal status:\n\n\" List all works reviewed prior to publication by peers / editorial boards in the field, such as journal articles in refereed journals, juried presentations, books, etc. Indicate up to five of the most important contributions with a double asterisk and briefly explain why these choices have been made. Include a description of the stature of journals and other scholarly venues and how this is known (e.g., impact factors, percentage of submitted work that is accepted, together with an explanation of the interpretation of these measures). \"",
    "is_useful": true,
    "question": "What factors should faculty consider when choosing where to publish their research to enhance their academic reputation?"
  },
  {
    "text": "Factors such as low acceptance rates, high levels of readership, importance to the field are also suggestive indicators in assessing quality\n\nand reputation. \" 215 Similarly, promotion and tenure forms from the University of Vermont (University of Vermont, 2016) 216 associate the JIF with journal status:\n\n\" List all works reviewed prior to publication by peers / editorial boards in the field, such as journal articles in refereed journals, juried presentations, books, etc. Indicate up to five of the most important contributions with a double asterisk and briefly explain why these choices have been made. Include a description of the stature of journals and other scholarly venues and how this is known (e.g., impact factors, percentage of submitted work that is accepted, together with an explanation of the interpretation of these measures). \"\n\n217\n\n218 Overall, these documents show a focus on publication venue and use the JIF as a proxy measure 219 for determining how much individual publications should count in evaluations based on where they\n\n211\n\n- \n214\n\n![](_page_10_Picture_2.jpeg)\n\n220 are published.\n\n#### 221 **Many mentions do not specify what is measured with the JIF**\n\n Lastly, we were left with many instances where the JIF was mentioned without additional information on what it is intended to measure. Such unspecified mentions were found in the RPT documents of 77% (23 of 30) of institutions that mentioned the JIF.",
    "is_useful": true,
    "question": "What factors are considered indicators of quality and reputation in academic journal publications?"
  },
  {
    "text": "Include a description of the stature of journals and other scholarly venues and how this is known (e.g., impact factors, percentage of submitted work that is accepted, together with an explanation of the interpretation of these measures). \"\n\n217\n\n218 Overall, these documents show a focus on publication venue and use the JIF as a proxy measure 219 for determining how much individual publications should count in evaluations based on where they\n\n211\n\n- \n214\n\n![](_page_10_Picture_2.jpeg)\n\n220 are published.\n\n#### 221 **Many mentions do not specify what is measured with the JIF**\n\n Lastly, we were left with many instances where the JIF was mentioned without additional information on what it is intended to measure. Such unspecified mentions were found in the RPT documents of 77% (23 of 30) of institutions that mentioned the JIF. These correspond to 74% (17 of 23) of the R- type institutions and 86% (6 of 7) of the M-type institutions with mentions (Table 1). These mentions were often found in research and scholarship sections that ask faculty to list their publications and accompanying information about the publication venues, such as the JIF or journal rank. Some of these documents simply suggest the JIF be included, while others make it a requirement.",
    "is_useful": true,
    "question": "What metrics are commonly used to evaluate the stature of academic journals and publication venues in the context of open science?"
  },
  {
    "text": "[](_page_10_Picture_2.jpeg)\n\n220 are published.\n\n#### 221 **Many mentions do not specify what is measured with the JIF**\n\n Lastly, we were left with many instances where the JIF was mentioned without additional information on what it is intended to measure. Such unspecified mentions were found in the RPT documents of 77% (23 of 30) of institutions that mentioned the JIF. These correspond to 74% (17 of 23) of the R- type institutions and 86% (6 of 7) of the M-type institutions with mentions (Table 1). These mentions were often found in research and scholarship sections that ask faculty to list their publications and accompanying information about the publication venues, such as the JIF or journal rank. Some of these documents simply suggest the JIF be included, while others make it a requirement. For example, guidelines from the Russ College of Engineering and Technology at Ohio University (Ohio University, 2015) request the JIF in the following way:\n\n\" List relevant peer-reviewed journal and conference papers published over the last five years (or since last promotion or initial appointment, whichever is less) related to pedagogy or other relevant areas of education. Include the journal's impact factor (or equivalent journal ranking data) and the number of citations of the article(s). \"",
    "is_useful": true,
    "question": "What percentage of institutions that mention the Journal Impact Factor (JIF) do so without specifying what it measures?"
  },
  {
    "text": "These correspond to 74% (17 of 23) of the R- type institutions and 86% (6 of 7) of the M-type institutions with mentions (Table 1). These mentions were often found in research and scholarship sections that ask faculty to list their publications and accompanying information about the publication venues, such as the JIF or journal rank. Some of these documents simply suggest the JIF be included, while others make it a requirement. For example, guidelines from the Russ College of Engineering and Technology at Ohio University (Ohio University, 2015) request the JIF in the following way:\n\n\" List relevant peer-reviewed journal and conference papers published over the last five years (or since last promotion or initial appointment, whichever is less) related to pedagogy or other relevant areas of education. Include the journal's impact factor (or equivalent journal ranking data) and the number of citations of the article(s). \"\n\n## 232 **Not all mentions of the JIF support its use**\n\n While the majority of the mentions found in our sample of RPT documents were either neutral or supportive of the JIF, we find that 13% of institutions had at least one mention which cautioned against or discouraged use of the JIF in evaluations. We observed varying levels of caution in these mentions. Some do not critique use of the JIF in general, but rather express concern that JIF data are not as relevant for their discipline as for others.",
    "is_useful": true,
    "question": "What percentage of institutions expressed caution or discouraged the use of Journal Impact Factor (JIF) in evaluations?"
  },
  {
    "text": "Include the journal's impact factor (or equivalent journal ranking data) and the number of citations of the article(s). \"\n\n## 232 **Not all mentions of the JIF support its use**\n\n While the majority of the mentions found in our sample of RPT documents were either neutral or supportive of the JIF, we find that 13% of institutions had at least one mention which cautioned against or discouraged use of the JIF in evaluations. We observed varying levels of caution in these mentions. Some do not critique use of the JIF in general, but rather express concern that JIF data are not as relevant for their discipline as for others. For example, criteria for promotion and tenure from the School of Social Work at the University of Central Florida (University of Central Florida, 2014) state:\n\n\" Journal impact factors will not be a primary criteria for the measurement of scholarly activity and prominence as the academic depth and breadth of the profession requires publication in a multitude of journals that may not have high impact factors, especially when compared to the stem [sic] disciplines. \"\n\n240\n\n231\n\n241 Similarly, guidelines from the Department of Human Health and Nutritional Sciences at the Univer-242 sity of Guelph (University of Guelph, 2008) call the JIF a 'problematic' index and discourage its use 243 while again highlighting disciplinary differences:\n\n![](_page_11_Figure_2.jpeg)\n\n- \" Discussion of journal quality (by those familiar with the field) may be included in the assessment in addition to consideration of the quality of individual research contributions.",
    "is_useful": true,
    "question": "What are the concerns regarding the use of the Journal Impact Factor (JIF) in evaluating scholarly activity across different academic disciplines?"
  },
  {
    "text": "For example, criteria for promotion and tenure from the School of Social Work at the University of Central Florida (University of Central Florida, 2014) state:\n\n\" Journal impact factors will not be a primary criteria for the measurement of scholarly activity and prominence as the academic depth and breadth of the profession requires publication in a multitude of journals that may not have high impact factors, especially when compared to the stem [sic] disciplines. \"\n\n240\n\n231\n\n241 Similarly, guidelines from the Department of Human Health and Nutritional Sciences at the Univer-242 sity of Guelph (University of Guelph, 2008) call the JIF a 'problematic' index and discourage its use 243 while again highlighting disciplinary differences:\n\n![](_page_11_Figure_2.jpeg)\n\n- \" Discussion of journal quality (by those familiar with the field) may be included in the assessment in addition to consideration of the quality of individual research contributions. However, citation analyses and impact factors are problematic indices, particularly in comparisons across fields, and their use in the review process is not encouraged. \"\n244\n\n245 Other guidelines, such as those from the Faculty of Veterinary Medicine at the University of Calgary 246 (University of Calgary, 2008), caution against relying solely on the JIF as a measure of quality, but 247 still allow it to be considered:\n\n\" Special consideration is to be given to the quality of the publication and the nature of the authorship. Contributions of the applicant must be clearly documented.",
    "is_useful": true,
    "question": "What are the views of certain academic institutions regarding the use of journal impact factors in evaluating scholarly activity and contributions?"
  },
  {
    "text": "[](_page_11_Figure_2.jpeg)\n\n- \" Discussion of journal quality (by those familiar with the field) may be included in the assessment in addition to consideration of the quality of individual research contributions. However, citation analyses and impact factors are problematic indices, particularly in comparisons across fields, and their use in the review process is not encouraged. \"\n244\n\n245 Other guidelines, such as those from the Faculty of Veterinary Medicine at the University of Calgary 246 (University of Calgary, 2008), caution against relying solely on the JIF as a measure of quality, but 247 still allow it to be considered:\n\n\" Special consideration is to be given to the quality of the publication and the nature of the authorship. Contributions of the applicant must be clearly documented. The reputation and impact of the journal or other publication format will be considered, but takes secondary consideration to the quality of the publication and the nature of the contributions. Impact factors of journals should not be used as the sole or deciding criteria in assessing quality. \"\n\n248\n\n249 Some RPT documents even seem to show disagreement within evaluation committees on the use 250 of the JIF. For example, a document from the Committee on Academic Personnel at the University 251 of California, San Diego (University of California, San Diego, 2015-2016) reads:\n\n- \" CAP [Committee on Academic Personnel] welcomes data on journal acceptance rates and impact factors, citation rates and H-index, but some CAP members (as do senior staff of scholarly societies) retain various degrees of skepticism about such measures. \"",
    "is_useful": true,
    "question": "What considerations should be taken into account when assessing the quality of scientific publications in the context of open science?"
  },
  {
    "text": "Contributions of the applicant must be clearly documented. The reputation and impact of the journal or other publication format will be considered, but takes secondary consideration to the quality of the publication and the nature of the contributions. Impact factors of journals should not be used as the sole or deciding criteria in assessing quality. \"\n\n248\n\n249 Some RPT documents even seem to show disagreement within evaluation committees on the use 250 of the JIF. For example, a document from the Committee on Academic Personnel at the University 251 of California, San Diego (University of California, San Diego, 2015-2016) reads:\n\n- \" CAP [Committee on Academic Personnel] welcomes data on journal acceptance rates and impact factors, citation rates and H-index, but some CAP members (as do senior staff of scholarly societies) retain various degrees of skepticism about such measures. \"\n- 252\n\n253 None of the RPT documents we analyzed heavily criticize the JIF or prohibit its use in evalua-254 tions.\n\n## 255 **Discussion**\n\n To our knowledge, this is the first large-scale study of RPT documents from a representative sample of U.S. and Canadian universities to analyze the use of the JIF in academic evaluations. We found that 23% of institutions in our sample mentioned the JIF or related terms in their RPT documents. The percentage was highest for R-type institutions at 40%, versus either M-type (18%) of B-type (0%) institutions. Mentions were largely supportive of JIF use, with 87% of institutions having at least one supportive mention.",
    "is_useful": true,
    "question": "What considerations are essential in evaluating the quality of academic publications according to recent studies on research performance evaluation?"
  },
  {
    "text": "- 252\n\n253 None of the RPT documents we analyzed heavily criticize the JIF or prohibit its use in evalua-254 tions.\n\n## 255 **Discussion**\n\n To our knowledge, this is the first large-scale study of RPT documents from a representative sample of U.S. and Canadian universities to analyze the use of the JIF in academic evaluations. We found that 23% of institutions in our sample mentioned the JIF or related terms in their RPT documents. The percentage was highest for R-type institutions at 40%, versus either M-type (18%) of B-type (0%) institutions. Mentions were largely supportive of JIF use, with 87% of institutions having at least one supportive mention. In contrast, just 13% of institutions had mentions which expressed\n\n![](_page_12_Picture_2.jpeg)\n\n caution about use of the JIF in evaluations. None of the RPT documents we analyzed prohibit its use. With respect to what is being measured with the JIF, the most common positive association we observed was between the JIF and quality, with 63% of institutions making this link. Less common though still observed were associations made between the JIF and impact, importance, or significance (40% of institutions), and prestige, reputation, or status (20%).\n\n## 267 **How prevalent is the use of the JIF in evaluations?",
    "is_useful": true,
    "question": "What is the prevalence and institutional support for the use of Journal Impact Factor in academic evaluations?"
  },
  {
    "text": "The percentage was highest for R-type institutions at 40%, versus either M-type (18%) of B-type (0%) institutions. Mentions were largely supportive of JIF use, with 87% of institutions having at least one supportive mention. In contrast, just 13% of institutions had mentions which expressed\n\n![](_page_12_Picture_2.jpeg)\n\n caution about use of the JIF in evaluations. None of the RPT documents we analyzed prohibit its use. With respect to what is being measured with the JIF, the most common positive association we observed was between the JIF and quality, with 63% of institutions making this link. Less common though still observed were associations made between the JIF and impact, importance, or significance (40% of institutions), and prestige, reputation, or status (20%).\n\n## 267 **How prevalent is the use of the JIF in evaluations?**\n\n Mentions of the JIF and related terms in RPT documents are not as ubiquitous as the amount of discussion of current evaluation systems would suggest \u2013 23% of institutions in our sample used these terms explicitly. However, the results differ depending on institution type, which might suggest that the experiences at R-type universities (where mentions of the JIF were most prevalent) play an outsized role in discussions about evaluation. Furthermore, the analysis we present on the terms in groups 1 and 2 of our coding terminology (see Fig. 1) may represent only the tip of the iceberg.",
    "is_useful": true,
    "question": "What percentage of institutions expressed support for the use of the Journal Impact Factor (JIF) in evaluations?"
  },
  {
    "text": "Less common though still observed were associations made between the JIF and impact, importance, or significance (40% of institutions), and prestige, reputation, or status (20%).\n\n## 267 **How prevalent is the use of the JIF in evaluations?**\n\n Mentions of the JIF and related terms in RPT documents are not as ubiquitous as the amount of discussion of current evaluation systems would suggest \u2013 23% of institutions in our sample used these terms explicitly. However, the results differ depending on institution type, which might suggest that the experiences at R-type universities (where mentions of the JIF were most prevalent) play an outsized role in discussions about evaluation. Furthermore, the analysis we present on the terms in groups 1 and 2 of our coding terminology (see Fig. 1) may represent only the tip of the iceberg. That is, while we analyzed only those terms that were very closely related to the JIF, we also observed (but did not analyze) terms such as 'major', 'prestigious', 'prominent', 'highly respected', 'highly ranked', and 'top tier' that may be associated with high JIFs in the minds of evaluators. It is impossible to know how RPT committee members interpret such phrases on the basis of the documents alone, but we suspect that some of these additional terms serve to invoke the JIF without explictly naming it.",
    "is_useful": true,
    "question": "What is the prevalence of explicit mentions of the Journal Impact Factor in evaluations across different types of institutions?"
  },
  {
    "text": "However, the results differ depending on institution type, which might suggest that the experiences at R-type universities (where mentions of the JIF were most prevalent) play an outsized role in discussions about evaluation. Furthermore, the analysis we present on the terms in groups 1 and 2 of our coding terminology (see Fig. 1) may represent only the tip of the iceberg. That is, while we analyzed only those terms that were very closely related to the JIF, we also observed (but did not analyze) terms such as 'major', 'prestigious', 'prominent', 'highly respected', 'highly ranked', and 'top tier' that may be associated with high JIFs in the minds of evaluators. It is impossible to know how RPT committee members interpret such phrases on the basis of the documents alone, but we suspect that some of these additional terms serve to invoke the JIF without explictly naming it. Take the following examples that leave open for interpretation what measure is used for determining a journal's status (emphasis added):\n\n281 From the Department of Health Management & Informatics at the University of Central Florida 282 (University of Central Florida, 2014):\n\n- \" Both quality and quantity of publications are important. Conventional evidence for quality\n- includes publications in **high-ranking journals** and citation by other scholars. \" 284 From the College of Arts and Sciences, University of Vermont (University of Vermont, 2015):\n\t- \" Excellence in scholarly research is often demonstrated by the presence of works published in **top tier journals** and academic presses. \"",
    "is_useful": true,
    "question": "How do institutional types influence the discussion of journal evaluation metrics in open science?"
  },
  {
    "text": "Conventional evidence for quality\n- includes publications in **high-ranking journals** and citation by other scholars. \" 284 From the College of Arts and Sciences, University of Vermont (University of Vermont, 2015):\n\t- \" Excellence in scholarly research is often demonstrated by the presence of works published in **top tier journals** and academic presses. \"\n- 285\n\n283\n\n Both of these examples do not explicitly mention the JIF (and thus are not counted in our analysis), but do imply the need for some measure for ranking journals. It seems likely, given the ubiquity of the JIF, that some committee members will rely on this metric, at least in part, for such a ranking. In short, counting mentions of a restricted set of terms, as we have done here, is likely\n\n![](_page_13_Picture_2.jpeg)\n\n an underestimate of the extent of the use of the JIF in RPT processes. However, we believe the in-depth analysis presented herein provides a glimpse into the current use of the JIF and may indicate how faculty are considering the metric in evaluations, particularly with respect to assessments of quality.\n\n294 **The JIF does not measure quality**\n\n295 The association between the JIF and quality was found in 63% of institutions in our sample. This 296 raises the question, is there evidence that the JIF is a good indicator of quality?",
    "is_useful": true,
    "question": "What are some conventional indicators of excellence in scholarly research?"
  },
  {
    "text": "It seems likely, given the ubiquity of the JIF, that some committee members will rely on this metric, at least in part, for such a ranking. In short, counting mentions of a restricted set of terms, as we have done here, is likely\n\n![](_page_13_Picture_2.jpeg)\n\n an underestimate of the extent of the use of the JIF in RPT processes. However, we believe the in-depth analysis presented herein provides a glimpse into the current use of the JIF and may indicate how faculty are considering the metric in evaluations, particularly with respect to assessments of quality.\n\n294 **The JIF does not measure quality**\n\n295 The association between the JIF and quality was found in 63% of institutions in our sample. This 296 raises the question, is there evidence that the JIF is a good indicator of quality? Although quality\n\n is hard to define, and even harder to measure, there are some aspects of methodological rigor which could be considered indicative of quality, such as sample sizes, experimental design, and reproducibility (Brembs, 2018). What is the relationship between these aspects of a study and the 300 JIF?\n\n Evidence suggests that methodological indicators of quality are not always found in journals with high JIFs. For example, Fraley & Vazire (2014) found that social and personality psychology journals with the highest JIFs tend to publish studies with smaller sample sizes and lower statistical power. Similarly, Munaf\u00f2 et al.",
    "is_useful": true,
    "question": "What factors could be considered indicative of the quality of research, and how does the Journal Impact Factor (JIF) relate to them?"
  },
  {
    "text": "294 **The JIF does not measure quality**\n\n295 The association between the JIF and quality was found in 63% of institutions in our sample. This 296 raises the question, is there evidence that the JIF is a good indicator of quality? Although quality\n\n is hard to define, and even harder to measure, there are some aspects of methodological rigor which could be considered indicative of quality, such as sample sizes, experimental design, and reproducibility (Brembs, 2018). What is the relationship between these aspects of a study and the 300 JIF?\n\n Evidence suggests that methodological indicators of quality are not always found in journals with high JIFs. For example, Fraley & Vazire (2014) found that social and personality psychology journals with the highest JIFs tend to publish studies with smaller sample sizes and lower statistical power. Similarly, Munaf\u00f2 et al. (2009) report that higher-ranked journals tend to publish gene-association studies with lower sample sizes and overestimate effect sizes. Analyses of neuroscience and/or psychology studies show either no correlation (Brembs et al., 2013) or a negative correlation (Szucs & Ioannidis, 2017) between statistical power and the JIF. Charles et al. (2009) found that two thirds of a sample of clinical trial studies published in medical journals with high JIFs did not report all the parameters necessary to justify sample size calculations, or had problems with their calculations.",
    "is_useful": true,
    "question": "What factors may indicate the quality of a research study, and how do they relate to journal impact factors?"
  },
  {
    "text": "Evidence suggests that methodological indicators of quality are not always found in journals with high JIFs. For example, Fraley & Vazire (2014) found that social and personality psychology journals with the highest JIFs tend to publish studies with smaller sample sizes and lower statistical power. Similarly, Munaf\u00f2 et al. (2009) report that higher-ranked journals tend to publish gene-association studies with lower sample sizes and overestimate effect sizes. Analyses of neuroscience and/or psychology studies show either no correlation (Brembs et al., 2013) or a negative correlation (Szucs & Ioannidis, 2017) between statistical power and the JIF. Charles et al. (2009) found that two thirds of a sample of clinical trial studies published in medical journals with high JIFs did not report all the parameters necessary to justify sample size calculations, or had problems with their calculations.\n\n Several studies have also looked at different aspects of experimental design to assess method- ological rigor and quality of a study. Chess & Gagnier (2013) analyzed clinical trial studies for ten different indicators of quality, including randomization and blinding, and found that less than 1% of studies met all ten quality criteria, while the JIF of the journals did not significantly predict whether a larger number of quality criteria were met. Barbui et al. (2006) also looked at clinical trial studies and used three different scales that take into account experimental design, bias, randomization, and more to assess quality.",
    "is_useful": true,
    "question": "How do methodological indicators of quality relate to journal impact factors in the context of open science?"
  },
  {
    "text": "Charles et al. (2009) found that two thirds of a sample of clinical trial studies published in medical journals with high JIFs did not report all the parameters necessary to justify sample size calculations, or had problems with their calculations.\n\n Several studies have also looked at different aspects of experimental design to assess method- ological rigor and quality of a study. Chess & Gagnier (2013) analyzed clinical trial studies for ten different indicators of quality, including randomization and blinding, and found that less than 1% of studies met all ten quality criteria, while the JIF of the journals did not significantly predict whether a larger number of quality criteria were met. Barbui et al. (2006) also looked at clinical trial studies and used three different scales that take into account experimental design, bias, randomization, and more to assess quality. The authors found no clear relationship between the JIF and study quality (Barbui et al., 2006).\n\n Others have suggested that reproducibility be used as a measure of quality, since it requires work to provide sufficient methodological care and detail. For example, Bustin et al. (2013) analyzed molecular biology studies and found key methodological details lacking, reporting a negative correlation between the JIF of the journal where the study was published and the amount of information provided in the work. Vasilevsky et al.",
    "is_useful": true,
    "question": "What factors are assessed to determine the methodological rigor and quality of clinical trial studies?"
  },
  {
    "text": "Barbui et al. (2006) also looked at clinical trial studies and used three different scales that take into account experimental design, bias, randomization, and more to assess quality. The authors found no clear relationship between the JIF and study quality (Barbui et al., 2006).\n\n Others have suggested that reproducibility be used as a measure of quality, since it requires work to provide sufficient methodological care and detail. For example, Bustin et al. (2013) analyzed molecular biology studies and found key methodological details lacking, reporting a negative correlation between the JIF of the journal where the study was published and the amount of information provided in the work. Vasilevsky et al. (2013) analyzed articles from multiple disciplines and found that many resources (e.g., antibodies, cell lines) were not 'uniquely identifiable', reporting no relationship between the JIF and resource identifiability. Mobley et al. (2013) found that around\n\n![](_page_14_Picture_2.jpeg)\n\n326 half of biomedical researchers surveyed reported they had been unable to reproduce a published\n\n- 327 finding, some from journals with a JIF over 20. Prinz et al. (2011) found, \"that the reproducibility of\n- 328 published data did not significantly correlate with journal impact factors\" (pg. 2).",
    "is_useful": true,
    "question": "What is the relationship between journal impact factors (JIF) and the quality or reproducibility of published research studies? "
  },
  {
    "text": "(2013) analyzed molecular biology studies and found key methodological details lacking, reporting a negative correlation between the JIF of the journal where the study was published and the amount of information provided in the work. Vasilevsky et al. (2013) analyzed articles from multiple disciplines and found that many resources (e.g., antibodies, cell lines) were not 'uniquely identifiable', reporting no relationship between the JIF and resource identifiability. Mobley et al. (2013) found that around\n\n![](_page_14_Picture_2.jpeg)\n\n326 half of biomedical researchers surveyed reported they had been unable to reproduce a published\n\n- 327 finding, some from journals with a JIF over 20. Prinz et al. (2011) found, \"that the reproducibility of\n- 328 published data did not significantly correlate with journal impact factors\" (pg. 2).\n- 329 Thus, at least as viewed through the aspects above, there is little to no evidence to justify a\n\n330 relationship between the JIF and research quality. A more comprensive review of these issues can\n\n- 331 be found in Brembs (2018).\n#### 332 **Improving academic evaluation**\n\n333 The lack of evidence for linking the JIF with quality, along with the clearly prevalent association that\n\n334 the academic community makes between the two, has given rise to a number of proposals and\n\n335 initiatives to challenge the use of the JIF, promote the responsible use of metrics, and otherwise\n\n336 improve academic evaluations.",
    "is_useful": true,
    "question": "What findings have been reported regarding the relationship between journal impact factor (JIF) and research quality in scientific studies?"
  },
  {
    "text": "Prinz et al. (2011) found, \"that the reproducibility of\n- 328 published data did not significantly correlate with journal impact factors\" (pg. 2).\n- 329 Thus, at least as viewed through the aspects above, there is little to no evidence to justify a\n\n330 relationship between the JIF and research quality. A more comprensive review of these issues can\n\n- 331 be found in Brembs (2018).\n#### 332 **Improving academic evaluation**\n\n333 The lack of evidence for linking the JIF with quality, along with the clearly prevalent association that\n\n334 the academic community makes between the two, has given rise to a number of proposals and\n\n335 initiatives to challenge the use of the JIF, promote the responsible use of metrics, and otherwise\n\n336 improve academic evaluations. These include the Leiden Manifesto (Hicks et al., 2015), the Metric\n\n337 Tide report (Wilsdon et al., 2015), the Next-Generation Metrics report (Wildson et al., 2017), and\n\n338 HuMetricsHSS (humetricshss.org), among others (for a review, see Moher et al. (2018)). Inasmuch\n\n339 as this project can be said to be contributing to these efforts by answering questions about the use\n\n340 of the JIF, we provide a brief description of a few of these projects and efforts.",
    "is_useful": true,
    "question": "What is the relationship between journal impact factors and research quality, and what initiatives have been proposed to improve academic evaluation?"
  },
  {
    "text": "These include the Leiden Manifesto (Hicks et al., 2015), the Metric\n\n337 Tide report (Wilsdon et al., 2015), the Next-Generation Metrics report (Wildson et al., 2017), and\n\n338 HuMetricsHSS (humetricshss.org), among others (for a review, see Moher et al. (2018)). Inasmuch\n\n339 as this project can be said to be contributing to these efforts by answering questions about the use\n\n340 of the JIF, we provide a brief description of a few of these projects and efforts.\n\n#### 341 **Declaration on Research Assessment**\n\n342 Probably the most well-known such project is the Declaration on Research Assessment (DORA;\n\n343 sfdora.org). DORA outlines some of the limitations of the JIF, and puts forward a general recom-\n\n344 mendation that those evaluating academics and their research not use it, especially as a \"surrogate\n\n345 measure of the quality of individual research articles\" (sfdora.org/read). Particularly relevant to our\n\n346 current research is the DORA recommendation that asks institutions to:\n\n- \" Be explicit about the criteria used to reach hiring, tenure, and promotion decisions, clearly highlighting, especially for early-stage investigators, that the scientific content of a paper is much more important than publication metrics or the identity of the journal in which it was published. \"\n347\n\n348 In June of 2018, DORA released its two-year strategic plan to expand its work towards improving\n\n349 academic evaluations (DORA Steering Committee, 2018).",
    "is_useful": true,
    "question": "What is the purpose of the Declaration on Research Assessment (DORA) in the context of evaluating academic research? "
  },
  {
    "text": "DORA outlines some of the limitations of the JIF, and puts forward a general recom-\n\n344 mendation that those evaluating academics and their research not use it, especially as a \"surrogate\n\n345 measure of the quality of individual research articles\" (sfdora.org/read). Particularly relevant to our\n\n346 current research is the DORA recommendation that asks institutions to:\n\n- \" Be explicit about the criteria used to reach hiring, tenure, and promotion decisions, clearly highlighting, especially for early-stage investigators, that the scientific content of a paper is much more important than publication metrics or the identity of the journal in which it was published. \"\n347\n\n348 In June of 2018, DORA released its two-year strategic plan to expand its work towards improving\n\n349 academic evaluations (DORA Steering Committee, 2018). This work includes spreading awareness 350 of alternatives to the JIF and collecting examples of good evaluation practices from funders,\n\n351 academic societies, and institutions (sfdora.org/good-practices).\n\n352 To date, DORA has been signed by over 1,200 organizations and nearly 14,000 individuals\n\n353 worldwide. None of the institutions in our sample are DORA signatories, so we were unable to do\n\n![](_page_15_Picture_2.jpeg)\n\n354 any analysis on this, but it would be interesting to study if and how commitment to DORA might be 355 reflected in changes to an institution's RPT documents and evaluation processes.",
    "is_useful": true,
    "question": "What general recommendation does DORA provide regarding the evaluation criteria for hiring, tenure, and promotion decisions in academia?"
  },
  {
    "text": "347\n\n348 In June of 2018, DORA released its two-year strategic plan to expand its work towards improving\n\n349 academic evaluations (DORA Steering Committee, 2018). This work includes spreading awareness 350 of alternatives to the JIF and collecting examples of good evaluation practices from funders,\n\n351 academic societies, and institutions (sfdora.org/good-practices).\n\n352 To date, DORA has been signed by over 1,200 organizations and nearly 14,000 individuals\n\n353 worldwide. None of the institutions in our sample are DORA signatories, so we were unable to do\n\n![](_page_15_Picture_2.jpeg)\n\n354 any analysis on this, but it would be interesting to study if and how commitment to DORA might be 355 reflected in changes to an institution's RPT documents and evaluation processes.\n\n#### 356 **Libraries taking the lead on responsible metrics**\n\n Libraries are at the forefront of promoting the responsible use of metrics. Academic libraries have developed online guides to help faculty learn about the correct uses of different metrics, including the JIF (e.g., Duke University Medical Center Library & Archives; University of Illinois at Urbana Champaign Library; University of Surrey Library; University of York Library). Libraries are also providing in-person advising and training for faculty in publishing and bibliometrics.\n\n There are also several larger-scale library-led efforts.",
    "is_useful": true,
    "question": "What initiatives have organizations undertaken to improve academic evaluations and promote responsible use of metrics in the context of open science?"
  },
  {
    "text": "None of the institutions in our sample are DORA signatories, so we were unable to do\n\n![](_page_15_Picture_2.jpeg)\n\n354 any analysis on this, but it would be interesting to study if and how commitment to DORA might be 355 reflected in changes to an institution's RPT documents and evaluation processes.\n\n#### 356 **Libraries taking the lead on responsible metrics**\n\n Libraries are at the forefront of promoting the responsible use of metrics. Academic libraries have developed online guides to help faculty learn about the correct uses of different metrics, including the JIF (e.g., Duke University Medical Center Library & Archives; University of Illinois at Urbana Champaign Library; University of Surrey Library; University of York Library). Libraries are also providing in-person advising and training for faculty in publishing and bibliometrics.\n\n There are also several larger-scale library-led efforts. For example, the Association of College & Research Libraries (ACRL) has developed a Scholarly Communication Toolkit on evaluating journals (Association of College & Research Libraries), which outlines several ways to assess journal quality that go beyond metrics like the JIF. LIBER (Ligue des Biblioth\u00e8ques Europ\u00e9ennes de Recherche) has established a Working Group on Metrics, and recently recommended increased training in metrics and their responsible uses (Coombs & Peters, 2017). The Measuring your Research Impact (MyRI) project (http://myri.conul.ie/) is a joint effort by three Irish academic libraries to provide open educational resources on bibliometrics.",
    "is_useful": true,
    "question": "How are libraries contributing to the promotion of responsible metrics in scholarly communication?"
  },
  {
    "text": "Libraries are also providing in-person advising and training for faculty in publishing and bibliometrics.\n\n There are also several larger-scale library-led efforts. For example, the Association of College & Research Libraries (ACRL) has developed a Scholarly Communication Toolkit on evaluating journals (Association of College & Research Libraries), which outlines several ways to assess journal quality that go beyond metrics like the JIF. LIBER (Ligue des Biblioth\u00e8ques Europ\u00e9ennes de Recherche) has established a Working Group on Metrics, and recently recommended increased training in metrics and their responsible uses (Coombs & Peters, 2017). The Measuring your Research Impact (MyRI) project (http://myri.conul.ie/) is a joint effort by three Irish academic libraries to provide open educational resources on bibliometrics. The Metrics Toolkit is a collaborative project by librarians and information professionals to provide educational information on a variety of metrics, both traditional and alternative, that can be used to evaluate different aspects of research (www.metrics-toolkit.org). In particular, their guide on the JIF outlines the metric's limitations, along with appropriate and inappropriate use cases (http://www.metrics-toolkit.org/journal-impact-factor/).\n\n## 375 **Conclusions**\n\n Overall, our results support the claims of faculty that the JIF features in evaluations of their research, though perhaps less prominently than previously thought, at least with respect to formal RPT guidelines.",
    "is_useful": true,
    "question": "What initiatives are libraries undertaking to support faculty in understanding and utilizing publishing metrics responsibly?"
  },
  {
    "text": "The Measuring your Research Impact (MyRI) project (http://myri.conul.ie/) is a joint effort by three Irish academic libraries to provide open educational resources on bibliometrics. The Metrics Toolkit is a collaborative project by librarians and information professionals to provide educational information on a variety of metrics, both traditional and alternative, that can be used to evaluate different aspects of research (www.metrics-toolkit.org). In particular, their guide on the JIF outlines the metric's limitations, along with appropriate and inappropriate use cases (http://www.metrics-toolkit.org/journal-impact-factor/).\n\n## 375 **Conclusions**\n\n Overall, our results support the claims of faculty that the JIF features in evaluations of their research, though perhaps less prominently than previously thought, at least with respect to formal RPT guidelines. Importantly, our analysis does not estimate use of the JIF beyond what is found in formal RPT documents, e.g., faculty members who serve on review committees and pay attention to this metric despite it not being explicitly mentioned in guidelines. Future work will include surveying faculty members, particularly those who have served on RPT committees, to learn more about how they interpret and apply RPT guidelines in evaluations and investigate some of the more subjective issues not addressed in this study.",
    "is_useful": true,
    "question": "What resources are provided by the MyRI project and the Metrics Toolkit to assist in understanding research impact metrics?"
  },
  {
    "text": "In particular, their guide on the JIF outlines the metric's limitations, along with appropriate and inappropriate use cases (http://www.metrics-toolkit.org/journal-impact-factor/).\n\n## 375 **Conclusions**\n\n Overall, our results support the claims of faculty that the JIF features in evaluations of their research, though perhaps less prominently than previously thought, at least with respect to formal RPT guidelines. Importantly, our analysis does not estimate use of the JIF beyond what is found in formal RPT documents, e.g., faculty members who serve on review committees and pay attention to this metric despite it not being explicitly mentioned in guidelines. Future work will include surveying faculty members, particularly those who have served on RPT committees, to learn more about how they interpret and apply RPT guidelines in evaluations and investigate some of the more subjective issues not addressed in this study.\n\n Our results also raise specific concerns that the JIF is being used to evaluate the quality and significance of research, despite the numerous warnings against such use (Brembs, 2018; Brembs et al., 2013; Haustein & Larivi\u00e8re, 2015; Kurmis, 2003; Moustafa, 2015; Seglen, 1997; Sugimoto & Larivi\u00e8re, 2018; The Analogue University, 2019). We hope our work will draw attention to this\n\n!",
    "is_useful": true,
    "question": "What are the concerns raised about the use of the Journal Impact Factor (JIF) in evaluating research quality?"
  },
  {
    "text": "Future work will include surveying faculty members, particularly those who have served on RPT committees, to learn more about how they interpret and apply RPT guidelines in evaluations and investigate some of the more subjective issues not addressed in this study.\n\n Our results also raise specific concerns that the JIF is being used to evaluate the quality and significance of research, despite the numerous warnings against such use (Brembs, 2018; Brembs et al., 2013; Haustein & Larivi\u00e8re, 2015; Kurmis, 2003; Moustafa, 2015; Seglen, 1997; Sugimoto & Larivi\u00e8re, 2018; The Analogue University, 2019). We hope our work will draw attention to this\n\n![](_page_16_Picture_3.jpeg)\n\n388 issue, and that increased educational and outreach efforts, like DORA and the library-led initiatives 389 mentioned above, will help academics make better decisions regarding the use of metrics like the 390 JIF.\n\n## 391 **Funding**\n\n- 392 This study was supported by the Open Society Foundations [OR2016-29841].\n## 393 **Acknowledgements**\n\n- 394 We are grateful to SPARC, the OpenCon community, the DORA Steering Committee (especially\n- 395 Catriona MacCallum and Anna Hatch), Chealsye Bowley, and Abigail Goben for discussions that\n- 396 shaped and improved this work.",
    "is_useful": true,
    "question": "What concerns are raised regarding the use of journal impact factors in evaluating research quality?"
  },
  {
    "text": "bias; citation bias; depression; psychotherapy; reporting bias\n\nAuthor for correspondence: Y. A. de Vries, E-mail: y.a.de.vries@rug.nl\n\n# The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: the case of depression\n\n# Y. A. de Vries1,2, A. M. Roest1,2, P. de Jonge1,2, P. Cuijpers3, M. R. Munaf\u00f24,5 and J. A. Bastiaansen1,6\n\n1 Department of Psychiatry, Interdisciplinary Center Psychopathology and Emotion regulation, University of Groningen, University Medical Center Groningen, Groningen, the Netherlands; 2 Developmental Psychology, Department of Psychology, University of Groningen, Groningen, the Netherlands; 3 Department of Clinical, Neuro-, and Developmental Psychology, Amsterdam Public Health Research Institute, Vrije Universiteit Amsterdam, Amsterdam, the Netherlands; 4 MRC Integrative Epidemiology Unit, University of Bristol, Bristol, UK; 5 UK Centre for Tobacco and Alcohol Studies, School of Experimental Psychology, University of Bristol, Bristol, UK and 6 Department of Education and Research, Friesland Mental Health Care Services, Leeuwarden, The Netherlands\n\n#### Introduction\n\nEvidence-based medicine is the cornerstone of clinical practice, but it is dependent on the quality of evidence upon which it is based.",
    "is_useful": true,
    "question": "What is essential for the validity of evidence-based medicine in clinical practice?"
  },
  {
    "text": "R. Munaf\u00f24,5 and J. A. Bastiaansen1,6\n\n1 Department of Psychiatry, Interdisciplinary Center Psychopathology and Emotion regulation, University of Groningen, University Medical Center Groningen, Groningen, the Netherlands; 2 Developmental Psychology, Department of Psychology, University of Groningen, Groningen, the Netherlands; 3 Department of Clinical, Neuro-, and Developmental Psychology, Amsterdam Public Health Research Institute, Vrije Universiteit Amsterdam, Amsterdam, the Netherlands; 4 MRC Integrative Epidemiology Unit, University of Bristol, Bristol, UK; 5 UK Centre for Tobacco and Alcohol Studies, School of Experimental Psychology, University of Bristol, Bristol, UK and 6 Department of Education and Research, Friesland Mental Health Care Services, Leeuwarden, The Netherlands\n\n#### Introduction\n\nEvidence-based medicine is the cornerstone of clinical practice, but it is dependent on the quality of evidence upon which it is based. Unfortunately, up to half of all randomized controlled trials (RCTs) have never been published, and trials with statistically significant findings are more likely to be published than those without (Dwan et al., 2013). Importantly, negative trials face additional hurdles beyond study publication bias that can result in the disappearance of non-significant results (Boutron et al., 2010; Dwan et al., 2013; Duyx et al., 2017).",
    "is_useful": true,
    "question": "What are some challenges faced by evidence-based medicine regarding the publication of randomized controlled trials?"
  },
  {
    "text": "Amsterdam, the Netherlands; 4 MRC Integrative Epidemiology Unit, University of Bristol, Bristol, UK; 5 UK Centre for Tobacco and Alcohol Studies, School of Experimental Psychology, University of Bristol, Bristol, UK and 6 Department of Education and Research, Friesland Mental Health Care Services, Leeuwarden, The Netherlands\n\n#### Introduction\n\nEvidence-based medicine is the cornerstone of clinical practice, but it is dependent on the quality of evidence upon which it is based. Unfortunately, up to half of all randomized controlled trials (RCTs) have never been published, and trials with statistically significant findings are more likely to be published than those without (Dwan et al., 2013). Importantly, negative trials face additional hurdles beyond study publication bias that can result in the disappearance of non-significant results (Boutron et al., 2010; Dwan et al., 2013; Duyx et al., 2017). Here, we analyze the cumulative impact of biases on apparent efficacy, and discuss possible remedies, using the evidence base for two effective treatments for depression: antidepressants and psychotherapy.\n\n## Reporting and citation biases\n\nWe distinguish among four major biases, although others exist: study publication bias, outcome reporting bias, spin, and citation bias. While study publication bias involves nonpublication of an entire study, outcome reporting bias refers to non-publication of negative outcomes within a published article or to switching the status of (non-significant) primary and (significant) secondary outcomes (Dwan et al., 2013).",
    "is_useful": true,
    "question": "What are the major biases that affect the publication and reporting of clinical trials?"
  },
  {
    "text": "Importantly, negative trials face additional hurdles beyond study publication bias that can result in the disappearance of non-significant results (Boutron et al., 2010; Dwan et al., 2013; Duyx et al., 2017). Here, we analyze the cumulative impact of biases on apparent efficacy, and discuss possible remedies, using the evidence base for two effective treatments for depression: antidepressants and psychotherapy.\n\n## Reporting and citation biases\n\nWe distinguish among four major biases, although others exist: study publication bias, outcome reporting bias, spin, and citation bias. While study publication bias involves nonpublication of an entire study, outcome reporting bias refers to non-publication of negative outcomes within a published article or to switching the status of (non-significant) primary and (significant) secondary outcomes (Dwan et al., 2013). Both biases pose an important threat to the validity of meta-analyses (Kicinski, 2014).\n\nTrials that faithfully report non-significant results will yield accurate effect size estimates, but results interpretation can still be positively biased, which may affect apparent efficacy. Reporting strategies that could distort the interpretation of results and mislead readers are defined as spin (Boutron et al., 2010). Spin occurs when authors conclude that the treatment is effective despite non-significant results on the primary outcome, for instance by focusing on statistically significant, but secondary, analyses (e.g.",
    "is_useful": true,
    "question": "What are some biases that can affect the validity of meta-analyses in clinical trials?"
  },
  {
    "text": "While study publication bias involves nonpublication of an entire study, outcome reporting bias refers to non-publication of negative outcomes within a published article or to switching the status of (non-significant) primary and (significant) secondary outcomes (Dwan et al., 2013). Both biases pose an important threat to the validity of meta-analyses (Kicinski, 2014).\n\nTrials that faithfully report non-significant results will yield accurate effect size estimates, but results interpretation can still be positively biased, which may affect apparent efficacy. Reporting strategies that could distort the interpretation of results and mislead readers are defined as spin (Boutron et al., 2010). Spin occurs when authors conclude that the treatment is effective despite non-significant results on the primary outcome, for instance by focusing on statistically significant, but secondary, analyses (e.g. instead of concluding that treatment X was not more effective than placebo, concluding that treatment X was well tolerated and was effective in patients who had not received prior therapy). If an article has been spun, treatments are perceived as more beneficial (Boutron et al., 2014). Finally, citation bias is an obstacle to ensuring that negative findings are discoverable. Studies with positive results receive more citations than negative studies (Duyx et al., 2017), leading to a heightened visibility of positive results.",
    "is_useful": true,
    "question": "What are the types of biases that threaten the validity of meta-analyses in research studies?"
  },
  {
    "text": "Reporting strategies that could distort the interpretation of results and mislead readers are defined as spin (Boutron et al., 2010). Spin occurs when authors conclude that the treatment is effective despite non-significant results on the primary outcome, for instance by focusing on statistically significant, but secondary, analyses (e.g. instead of concluding that treatment X was not more effective than placebo, concluding that treatment X was well tolerated and was effective in patients who had not received prior therapy). If an article has been spun, treatments are perceived as more beneficial (Boutron et al., 2014). Finally, citation bias is an obstacle to ensuring that negative findings are discoverable. Studies with positive results receive more citations than negative studies (Duyx et al., 2017), leading to a heightened visibility of positive results.\n\n#### The evidence base for antidepressants\n\nWe assembled a cohort of 105 depression trials, of which 74 were also included in a previous study on publication bias (Turner et al., 2008); we added 31 trials of novel antidepressants (approved after 2008) from the Food and Drug Administration (FDA) database (see online Supplementary materials). Pharmaceutical companies must preregister all trials they intend to use to obtain FDA approval; hence, trials with non-significant results, even if unpublished, are still accessible.\n\nFigure 1 demonstrates the cumulative impact of reporting and citation biases.",
    "is_useful": true,
    "question": "What impacts the visibility of positive and negative findings in scientific research?"
  },
  {
    "text": "If an article has been spun, treatments are perceived as more beneficial (Boutron et al., 2014). Finally, citation bias is an obstacle to ensuring that negative findings are discoverable. Studies with positive results receive more citations than negative studies (Duyx et al., 2017), leading to a heightened visibility of positive results.\n\n#### The evidence base for antidepressants\n\nWe assembled a cohort of 105 depression trials, of which 74 were also included in a previous study on publication bias (Turner et al., 2008); we added 31 trials of novel antidepressants (approved after 2008) from the Food and Drug Administration (FDA) database (see online Supplementary materials). Pharmaceutical companies must preregister all trials they intend to use to obtain FDA approval; hence, trials with non-significant results, even if unpublished, are still accessible.\n\nFigure 1 demonstrates the cumulative impact of reporting and citation biases. Of 105 antidepressant trials, 53 (50%) trials were considered positive by the FDA and 52 (50%) were considered negative or questionable (Fig. 1a). While all but one of the positive trials (98%) were published, only 25 (48%) of the negative trials were published. Hence, 77 trials were published, of which 25 (32%) were negative (Fig. 1b). Ten negative trials, however, became 'positive' in the published literature, by omitting unfavorable outcomes or switching the status of the primary\n\n\u00a9 Cambridge University Press 2018.",
    "is_useful": true,
    "question": "What factors contribute to the bias in the publication and citation of clinical trial results related to antidepressants?"
  },
  {
    "text": "Pharmaceutical companies must preregister all trials they intend to use to obtain FDA approval; hence, trials with non-significant results, even if unpublished, are still accessible.\n\nFigure 1 demonstrates the cumulative impact of reporting and citation biases. Of 105 antidepressant trials, 53 (50%) trials were considered positive by the FDA and 52 (50%) were considered negative or questionable (Fig. 1a). While all but one of the positive trials (98%) were published, only 25 (48%) of the negative trials were published. Hence, 77 trials were published, of which 25 (32%) were negative (Fig. 1b). Ten negative trials, however, became 'positive' in the published literature, by omitting unfavorable outcomes or switching the status of the primary\n\n\u00a9 Cambridge University Press 2018. This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence (http://creativecommons.org/licenses/ by/4.0/), which permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited.\n\n![](_page_0_Picture_20.jpeg)\n\n![](_page_1_Figure_2.jpeg)\n\nFig. 1. The cumulative impact of reporting and citation biases on the evidence base for antidepressants. (a) displays the initial, complete cohort of trials, while (b) through (e) show the cumulative effect of biases. Each circle indicates a trial, while the color indicates the results or the presence of spin.",
    "is_useful": true,
    "question": "What are the implications of trial registration on the accessibility of pharmaceutical trial results, particularly in the context of reporting biases?"
  },
  {
    "text": "1b). Ten negative trials, however, became 'positive' in the published literature, by omitting unfavorable outcomes or switching the status of the primary\n\n\u00a9 Cambridge University Press 2018. This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence (http://creativecommons.org/licenses/ by/4.0/), which permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited.\n\n![](_page_0_Picture_20.jpeg)\n\n![](_page_1_Figure_2.jpeg)\n\nFig. 1. The cumulative impact of reporting and citation biases on the evidence base for antidepressants. (a) displays the initial, complete cohort of trials, while (b) through (e) show the cumulative effect of biases. Each circle indicates a trial, while the color indicates the results or the presence of spin. Circles connected by a grey line indicate trials that were published together in a pooled publication. In (e), the size of the circle indicates the (relative) number of citations received by that category of studies.\n\nand secondary outcomes (Fig. 1c). Without access to the FDA reviews, it would not have been possible to conclude that these trials, when analyzed according to protocol, were not positive. Among the remaining 15 (19%) negative trials, five were published with spin in the abstract (i.e. concluding that the treatment was effective).",
    "is_useful": true,
    "question": "What are some issues related to bias in the reporting and publication of clinical trials that affect the integrity of scientific literature?"
  },
  {
    "text": "[](_page_1_Figure_2.jpeg)\n\nFig. 1. The cumulative impact of reporting and citation biases on the evidence base for antidepressants. (a) displays the initial, complete cohort of trials, while (b) through (e) show the cumulative effect of biases. Each circle indicates a trial, while the color indicates the results or the presence of spin. Circles connected by a grey line indicate trials that were published together in a pooled publication. In (e), the size of the circle indicates the (relative) number of citations received by that category of studies.\n\nand secondary outcomes (Fig. 1c). Without access to the FDA reviews, it would not have been possible to conclude that these trials, when analyzed according to protocol, were not positive. Among the remaining 15 (19%) negative trials, five were published with spin in the abstract (i.e. concluding that the treatment was effective). For instance, one article reported non-significant results for the primary outcome ( p = 0.10), yet concluded that the trial 'demonstrates an antidepressant effect for fluoxetine that is significantly more marked than the effect produced by placebo' (Rickels et al., 1986). Five additional articles contained mild spin (e.g. suggesting the treatment is at least numerically better than placebo). One article lacked an abstract, but the discussion section concluded that there was a 'trend for efficacy'.",
    "is_useful": true,
    "question": "What are the implications of reporting and citation biases on the evidence base for antidepressants?"
  },
  {
    "text": "and secondary outcomes (Fig. 1c). Without access to the FDA reviews, it would not have been possible to conclude that these trials, when analyzed according to protocol, were not positive. Among the remaining 15 (19%) negative trials, five were published with spin in the abstract (i.e. concluding that the treatment was effective). For instance, one article reported non-significant results for the primary outcome ( p = 0.10), yet concluded that the trial 'demonstrates an antidepressant effect for fluoxetine that is significantly more marked than the effect produced by placebo' (Rickels et al., 1986). Five additional articles contained mild spin (e.g. suggesting the treatment is at least numerically better than placebo). One article lacked an abstract, but the discussion section concluded that there was a 'trend for efficacy'. Hence, only four (5%) of 77 published trials unambiguously reported that the treatment was not more effective than placebo in that particular trial (Fig. 1d). Compounding the problem, positive trials were cited three times as frequently as negative trials (92 v. 32 citations in Web of Science, January 2016, p < 0.001, see online Supplementary material for further details) (Fig. 1e). Among negative trials, those with (mild) spin in the abstract received an average of 36 citations, while those with a clearly negative abstract received 25 citations.",
    "is_useful": true,
    "question": "What challenges are associated with the publication and citation of clinical trial results in the context of open science?"
  },
  {
    "text": "Five additional articles contained mild spin (e.g. suggesting the treatment is at least numerically better than placebo). One article lacked an abstract, but the discussion section concluded that there was a 'trend for efficacy'. Hence, only four (5%) of 77 published trials unambiguously reported that the treatment was not more effective than placebo in that particular trial (Fig. 1d). Compounding the problem, positive trials were cited three times as frequently as negative trials (92 v. 32 citations in Web of Science, January 2016, p < 0.001, see online Supplementary material for further details) (Fig. 1e). Among negative trials, those with (mild) spin in the abstract received an average of 36 citations, while those with a clearly negative abstract received 25 citations. While this might suggest a synergistic effect between spin and citation biases, where negatively presented negative studies receive especially few citations (de Vries et al., 2016), this difference was not statistically significant ( p = 0.50), likely due to the small sample size. Altogether, these results show that the effects of different biases accumulate to hide nonsignificant results from view.\n\n#### The evidence base for psychotherapy\n\nWhile the pharmaceutical industry has a financial motive for suppressing unfavorable results, these biases are also present in the other areas of research, such as psychotherapy. Without a standardized trial registry, however, they are more difficult to detect and disentangle.",
    "is_useful": true,
    "question": "What issues arise from the presence of biases in the reporting of clinical trial results, particularly in relation to psychotherapy research?"
  },
  {
    "text": "1e). Among negative trials, those with (mild) spin in the abstract received an average of 36 citations, while those with a clearly negative abstract received 25 citations. While this might suggest a synergistic effect between spin and citation biases, where negatively presented negative studies receive especially few citations (de Vries et al., 2016), this difference was not statistically significant ( p = 0.50), likely due to the small sample size. Altogether, these results show that the effects of different biases accumulate to hide nonsignificant results from view.\n\n#### The evidence base for psychotherapy\n\nWhile the pharmaceutical industry has a financial motive for suppressing unfavorable results, these biases are also present in the other areas of research, such as psychotherapy. Without a standardized trial registry, however, they are more difficult to detect and disentangle. Statistical tests suggest an excess of positive findings in the psychotherapy literature, due to either study publication bias or outcome reporting bias (Flint et al., 2015). Of 55 National Institutes of Health-funded psychotherapy trials, 13 (24%) remained unpublished (Driessen et al., 2015), and these had a markedly lower effect size than the published trials.\n\nRegarding spin, 49 (35%) of 142 papers were considered negative in a recent meta-analysis (Flint et al., 2015), but we found that only 12 (8%) abstracts concluded that psychotherapy was not more effective than a control condition.",
    "is_useful": true,
    "question": "What factors contribute to the bias in the publication of psychotherapy trial results?"
  },
  {
    "text": "#### The evidence base for psychotherapy\n\nWhile the pharmaceutical industry has a financial motive for suppressing unfavorable results, these biases are also present in the other areas of research, such as psychotherapy. Without a standardized trial registry, however, they are more difficult to detect and disentangle. Statistical tests suggest an excess of positive findings in the psychotherapy literature, due to either study publication bias or outcome reporting bias (Flint et al., 2015). Of 55 National Institutes of Health-funded psychotherapy trials, 13 (24%) remained unpublished (Driessen et al., 2015), and these had a markedly lower effect size than the published trials.\n\nRegarding spin, 49 (35%) of 142 papers were considered negative in a recent meta-analysis (Flint et al., 2015), but we found that only 12 (8%) abstracts concluded that psychotherapy was not more effective than a control condition. The remaining abstracts were either positive (73%) or mixed (19%) (e.g. concluding that the treatment was effective for one outcome but not another). Although we could not establish the pre-specified primary outcome for these trials, and therefore cannot determine whether a specific abstract is biased, published psychotherapy trials, as a whole, clearly provide a more positive impression of the effectiveness of psychotherapy than is justified by available evidence. Positive psychotherapy trials were also cited nearly twice as frequently as negative trials (111 citations v. 58, p = 0.003).",
    "is_useful": true,
    "question": "What are some of the biases present in psychotherapy research that affect the perceived effectiveness of treatments?"
  },
  {
    "text": "Regarding spin, 49 (35%) of 142 papers were considered negative in a recent meta-analysis (Flint et al., 2015), but we found that only 12 (8%) abstracts concluded that psychotherapy was not more effective than a control condition. The remaining abstracts were either positive (73%) or mixed (19%) (e.g. concluding that the treatment was effective for one outcome but not another). Although we could not establish the pre-specified primary outcome for these trials, and therefore cannot determine whether a specific abstract is biased, published psychotherapy trials, as a whole, clearly provide a more positive impression of the effectiveness of psychotherapy than is justified by available evidence. Positive psychotherapy trials were also cited nearly twice as frequently as negative trials (111 citations v. 58, p = 0.003). Negative trials with a positive or mixed abstract were cited more often than those with a negative abstract (59 and 87 citations, respectively v. 26, p = 0.05); however, the small sample size precludes definitive conclusions on the effects of spin on citation rates.\n\n#### Preventing bias\n\nMandatory prospective registration has long been advocated as a solution for study publication and outcome reporting bias. The International Committee of Medical Journal Editors (ICMJE) began requiring prospective registration of clinical trials as a precondition for publication in 2005, but many journals do not require registration (Kn\u00fcppel et al., 2013) and others allow retrospective registration (Harriman and Patel, 2016).",
    "is_useful": true,
    "question": "What measures have been proposed to prevent publication and outcome reporting bias in clinical trials?"
  },
  {
    "text": "Positive psychotherapy trials were also cited nearly twice as frequently as negative trials (111 citations v. 58, p = 0.003). Negative trials with a positive or mixed abstract were cited more often than those with a negative abstract (59 and 87 citations, respectively v. 26, p = 0.05); however, the small sample size precludes definitive conclusions on the effects of spin on citation rates.\n\n#### Preventing bias\n\nMandatory prospective registration has long been advocated as a solution for study publication and outcome reporting bias. The International Committee of Medical Journal Editors (ICMJE) began requiring prospective registration of clinical trials as a precondition for publication in 2005, but many journals do not require registration (Kn\u00fcppel et al., 2013) and others allow retrospective registration (Harriman and Patel, 2016). Since 2007, the FDA also requires prospective registration of most drug trials. This increasing pressure may explain why recently completed, negative antidepressant trials are more frequently published than older negative trials: all negative trials that remained unpublished were completed before 2004, while the 25 trials completed in 2004 or later (including 14 for which registration was legally required) were all published, even though nine were negative. A regulatory requirement is likely to be one of the most effective measures to ensure universal registration; unfortunately, the 2007 law excludes trials of behavioral interventions (e.g. psychotherapy) and phase 1 (healthy volunteer) trials.\n\nNevertheless, registration seems insufficient to ensure complete and accurate reporting of a trial.",
    "is_useful": true,
    "question": "What method has been advocated to prevent bias in the publication of study outcomes?"
  },
  {
    "text": "Since 2007, the FDA also requires prospective registration of most drug trials. This increasing pressure may explain why recently completed, negative antidepressant trials are more frequently published than older negative trials: all negative trials that remained unpublished were completed before 2004, while the 25 trials completed in 2004 or later (including 14 for which registration was legally required) were all published, even though nine were negative. A regulatory requirement is likely to be one of the most effective measures to ensure universal registration; unfortunately, the 2007 law excludes trials of behavioral interventions (e.g. psychotherapy) and phase 1 (healthy volunteer) trials.\n\nNevertheless, registration seems insufficient to ensure complete and accurate reporting of a trial. Only around half of all trials registered at ClinicalTrials.gov were published within two years of completion (Ross et al., 2009), and non-reporting of protocol-specified outcomes or the silent addition of new outcomes is also common (Jones et al., 2015, http://www.comparetrials.org). Close examination of registries by independent researchers may be necessary for registration to be a truly effective deterrent to study publication and outcome reporting bias. An alternative (or addition) to registration could be publication of study protocols or 'registered reports', in which journals accept a study for publication based on the introduction and methods, before the results are known. Widespread adoption of this format might also help to prevent spin, by reducing the pressure that researchers might feel to 'oversell' their results to get published.",
    "is_useful": true,
    "question": "What measures are suggested to improve the transparency and reporting accuracy of clinical trials in open science?"
  },
  {
    "text": "psychotherapy) and phase 1 (healthy volunteer) trials.\n\nNevertheless, registration seems insufficient to ensure complete and accurate reporting of a trial. Only around half of all trials registered at ClinicalTrials.gov were published within two years of completion (Ross et al., 2009), and non-reporting of protocol-specified outcomes or the silent addition of new outcomes is also common (Jones et al., 2015, http://www.comparetrials.org). Close examination of registries by independent researchers may be necessary for registration to be a truly effective deterrent to study publication and outcome reporting bias. An alternative (or addition) to registration could be publication of study protocols or 'registered reports', in which journals accept a study for publication based on the introduction and methods, before the results are known. Widespread adoption of this format might also help to prevent spin, by reducing the pressure that researchers might feel to 'oversell' their results to get published. Furthermore, in our analysis, positive studies were published in journals with a higher median impact factor (and thus higher visibility) than negative studies (3.5 v. 2.4 for antidepressant trials and 3.1 v. 2.6 for psychotherapy trials), which may be one driver behind the difference in citation rates. Hence, adoption of registered reports might also reduce citation bias by reducing the tendency for positive studies to be published in higher impact journals. Peer reviewers could also play a crucial role in ensuring that abstracts accurately report trial results and that important negative studies are cited.",
    "is_useful": true,
    "question": "What are some methods suggested to improve the transparency and accuracy of trial reporting in open science?"
  },
  {
    "text": "An alternative (or addition) to registration could be publication of study protocols or 'registered reports', in which journals accept a study for publication based on the introduction and methods, before the results are known. Widespread adoption of this format might also help to prevent spin, by reducing the pressure that researchers might feel to 'oversell' their results to get published. Furthermore, in our analysis, positive studies were published in journals with a higher median impact factor (and thus higher visibility) than negative studies (3.5 v. 2.4 for antidepressant trials and 3.1 v. 2.6 for psychotherapy trials), which may be one driver behind the difference in citation rates. Hence, adoption of registered reports might also reduce citation bias by reducing the tendency for positive studies to be published in higher impact journals. Peer reviewers could also play a crucial role in ensuring that abstracts accurately report trial results and that important negative studies are cited. Finally, the prevalence of spin and citation biases also shows the importance of assessing a study's actual results (rather than relying on the authors' conclusions) and of conducting independent literature searches, since reference lists may yield a disproportionate number of positive (and positively presented) studies.\n\n#### Conclusions\n\nThe problem of study publication bias is well-known. Our examination of antidepressant trials, however, shows the pernicious cumulative effect of additional reporting and citation biases, which together eliminated most negative results from the antidepressant literature and left the few published negative results difficult to discover. These biases are unlikely to be unique to antidepressant trials.",
    "is_useful": true,
    "question": "What are some proposed solutions to mitigate publication and citation biases in scientific research?"
  },
  {
    "text": "Hence, adoption of registered reports might also reduce citation bias by reducing the tendency for positive studies to be published in higher impact journals. Peer reviewers could also play a crucial role in ensuring that abstracts accurately report trial results and that important negative studies are cited. Finally, the prevalence of spin and citation biases also shows the importance of assessing a study's actual results (rather than relying on the authors' conclusions) and of conducting independent literature searches, since reference lists may yield a disproportionate number of positive (and positively presented) studies.\n\n#### Conclusions\n\nThe problem of study publication bias is well-known. Our examination of antidepressant trials, however, shows the pernicious cumulative effect of additional reporting and citation biases, which together eliminated most negative results from the antidepressant literature and left the few published negative results difficult to discover. These biases are unlikely to be unique to antidepressant trials. We have already shown that similar processes, though more difficult to assess, occur within the psychotherapy literature, and it seems likely that the effect of these biases accumulates whenever they are present. Consequently, researchers and clinicians across medical fields must be aware of the potential for bias to distort apparent treatment efficacy, which poses a threat to the practice of evidence-based medicine.\n\nSupplementary material. The supplementary material for this article can be found at https://doi.org/10.1017/S0033291718001873.\n\nAcknowledgements. Marcus R. Munaf\u00f2 is a member of the United Kingdom Centre for Tobacco and Alcohol Studies, a UKCRC Public Health Research: Centre of Excellence.",
    "is_useful": true,
    "question": "What impact do publication and citation biases have on the perception of treatment efficacy in medical research?"
  },
  {
    "text": "![](_page_0_Picture_1.jpeg)\n\n# Reporting in Experimental Philosophy: Current Standards and Recommendations for Future Practice\n\nAndrea Polonioli1 & Mariana Vega-Mendoza2 & Brittany Blankinship2 & David Carmel2,3\n\n# The Author(s) 2018 Published online: 29 July 2018\n\nAbstract Recent replication crises in psychology and other fields have led to intense reflection about the validity of common research practices. Much of this reflection has focussed on reporting standards, and how they may be related to the questionable research practices that could underlie a high proportion of irreproducible findings in the published record. As a developing field, it is particularly important for Experimental Philosophy to avoid some of the pitfalls that have beset other disciplines. To this end, here we provide a detailed, comprehensive assessment of current reporting practices in Experimental Philosophy. We focus on the quality of statistical reporting and the disclosure of information about study methodology. We assess all the articles using quantitative methods (n = 134) that were published over the years 2013\u20132016 in 29 leading philosophy journals. We find that null hypothesis significance testing is the prevalent statistical practice in Experimental Philosophy, although relying solely on this approach has been criticised in the psychological literature. To augment this approach, various additional measures have become commonplace in other fields, but we find that Experimental Philosophy has adopted these only partially: 53% of the papers report an effect size, 28% confidence intervals, 1% examined prospective statistical power and 5% report observed statistical power.",
    "is_useful": true,
    "question": "What are the current reporting practices in Experimental Philosophy, particularly in relation to statistical reporting and methodology disclosure?"
  },
  {
    "text": "As a developing field, it is particularly important for Experimental Philosophy to avoid some of the pitfalls that have beset other disciplines. To this end, here we provide a detailed, comprehensive assessment of current reporting practices in Experimental Philosophy. We focus on the quality of statistical reporting and the disclosure of information about study methodology. We assess all the articles using quantitative methods (n = 134) that were published over the years 2013\u20132016 in 29 leading philosophy journals. We find that null hypothesis significance testing is the prevalent statistical practice in Experimental Philosophy, although relying solely on this approach has been criticised in the psychological literature. To augment this approach, various additional measures have become commonplace in other fields, but we find that Experimental Philosophy has adopted these only partially: 53% of the papers report an effect size, 28% confidence intervals, 1% examined prospective statistical power and 5% report observed statistical power. Importantly, we find no direct relation between an article's reporting quality and its impact (numbers of citations). We conclude with recommendations for authors, reviewers and editors in Experimental Philosophy, to facilitate making research statistically-transparent and reproducible.\n\nAndrea Polonioli and Mariana Vega-Mendoza contributed equally to this work.",
    "is_useful": true,
    "question": "What are the recommendations made to improve the transparency and reproducibility of research in Experimental Philosophy?"
  },
  {
    "text": "We assess all the articles using quantitative methods (n = 134) that were published over the years 2013\u20132016 in 29 leading philosophy journals. We find that null hypothesis significance testing is the prevalent statistical practice in Experimental Philosophy, although relying solely on this approach has been criticised in the psychological literature. To augment this approach, various additional measures have become commonplace in other fields, but we find that Experimental Philosophy has adopted these only partially: 53% of the papers report an effect size, 28% confidence intervals, 1% examined prospective statistical power and 5% report observed statistical power. Importantly, we find no direct relation between an article's reporting quality and its impact (numbers of citations). We conclude with recommendations for authors, reviewers and editors in Experimental Philosophy, to facilitate making research statistically-transparent and reproducible.\n\nAndrea Polonioli and Mariana Vega-Mendoza contributed equally to this work.\n\n* Andrea Polonioli andrea_polonioli@hotmail.com\n\n- 2 Department of Psychology, University of Edinburgh, Edinburgh, UK\n<sup>1</sup> Department of Philosophy, University of Birmingham, 3 Elms Road, Edgbaston, Birmingham B15 2TT, UK\n\n<sup>3</sup> School of Psychology, Victoria University of Wellington, Wellington, New Zealand\n\n## 1 Introduction\n\nPhilosophers have recently started to adopt empirical methods to address research questions of philosophical relevance. This practice is often referred to as Experimental Philosophy (Knobe and Nichols 2008; Alexander 2012; Knobe et al.",
    "is_useful": true,
    "question": "What are some common statistical practices identified in Experimental Philosophy as of 2016?"
  },
  {
    "text": "Importantly, we find no direct relation between an article's reporting quality and its impact (numbers of citations). We conclude with recommendations for authors, reviewers and editors in Experimental Philosophy, to facilitate making research statistically-transparent and reproducible.\n\nAndrea Polonioli and Mariana Vega-Mendoza contributed equally to this work.\n\n* Andrea Polonioli andrea_polonioli@hotmail.com\n\n- 2 Department of Psychology, University of Edinburgh, Edinburgh, UK\n<sup>1</sup> Department of Philosophy, University of Birmingham, 3 Elms Road, Edgbaston, Birmingham B15 2TT, UK\n\n<sup>3</sup> School of Psychology, Victoria University of Wellington, Wellington, New Zealand\n\n## 1 Introduction\n\nPhilosophers have recently started to adopt empirical methods to address research questions of philosophical relevance. This practice is often referred to as Experimental Philosophy (Knobe and Nichols 2008; Alexander 2012; Knobe et al. 2012; Machery and O'Neill 2014; Sytsma and Buckwalter 2016), although it incorporates both experimental and correlational studies. More generally, it seems that what best characterizes this recent trend is an attempt to employ quantitative methods to make progress in philosophy (Knobe 2015).\n\nThe application of quantitative methods raises a number of important issues for this field. Previous research has discussed a constellation of ethical issues that have arisen since philosophers started to conduct empirical research (Polonioli 2017).",
    "is_useful": true,
    "question": "What recommendations are made to enhance the quality and transparency of research in Experimental Philosophy?"
  },
  {
    "text": "This practice is often referred to as Experimental Philosophy (Knobe and Nichols 2008; Alexander 2012; Knobe et al. 2012; Machery and O'Neill 2014; Sytsma and Buckwalter 2016), although it incorporates both experimental and correlational studies. More generally, it seems that what best characterizes this recent trend is an attempt to employ quantitative methods to make progress in philosophy (Knobe 2015).\n\nThe application of quantitative methods raises a number of important issues for this field. Previous research has discussed a constellation of ethical issues that have arisen since philosophers started to conduct empirical research (Polonioli 2017). Yet experimental philosophers should also be concerned with ongoing discussions, in several empirical fields, about whether common scientific practices in design, analysis, and reporting ought to be revised (Begley and Ellis 2012; Ioannidis 2005; Miguel et al. 2014; Simmons et al. 2011).\n\nSuch discussions are particularly heated in psychology, where a substantial number of findings have recently failed to replicate (Makel et al. 2012; Maxwell et al. 2015; Open Science Collaboration 2012, 2015; Pashler and Harris 2012; Simons 2014). The 'crisis of confidence' within psychology (Pashler and Wagenmakers 2012) arose from finding that many published results could not be replicated when competent independent researchers executed high-powered replication attempts that duplicated the original methodology as closely as possible.",
    "is_useful": true,
    "question": "What challenges does the application of quantitative methods in philosophy, particularly in experimental philosophy, face regarding ethical issues and scientific practices?"
  },
  {
    "text": "Yet experimental philosophers should also be concerned with ongoing discussions, in several empirical fields, about whether common scientific practices in design, analysis, and reporting ought to be revised (Begley and Ellis 2012; Ioannidis 2005; Miguel et al. 2014; Simmons et al. 2011).\n\nSuch discussions are particularly heated in psychology, where a substantial number of findings have recently failed to replicate (Makel et al. 2012; Maxwell et al. 2015; Open Science Collaboration 2012, 2015; Pashler and Harris 2012; Simons 2014). The 'crisis of confidence' within psychology (Pashler and Wagenmakers 2012) arose from finding that many published results could not be replicated when competent independent researchers executed high-powered replication attempts that duplicated the original methodology as closely as possible. For example, the Reproducibility Project was set up as an open large-scale attempt at estimating the replicability of psychological science (Open Science Collaboration 2012); it attempted to replicate 100 studies selected from 2008 issues of three leading psychology journals (Psychological Science, Journal of Personality and Social Psychology, and Journal of Experimental Psychology: Learning, Memory, and Cognition). The results of this effort, published a few years later (Open Science Collaboration 2015) revealed that only 36% of the replications had statistically significant results, as opposed to 97% of the original studies; and effects in replications had, on average, half the magnitude of those originally reported.",
    "is_useful": true,
    "question": "What challenges does the field of psychology face regarding the reliability and reproducibility of research findings?"
  },
  {
    "text": "The 'crisis of confidence' within psychology (Pashler and Wagenmakers 2012) arose from finding that many published results could not be replicated when competent independent researchers executed high-powered replication attempts that duplicated the original methodology as closely as possible. For example, the Reproducibility Project was set up as an open large-scale attempt at estimating the replicability of psychological science (Open Science Collaboration 2012); it attempted to replicate 100 studies selected from 2008 issues of three leading psychology journals (Psychological Science, Journal of Personality and Social Psychology, and Journal of Experimental Psychology: Learning, Memory, and Cognition). The results of this effort, published a few years later (Open Science Collaboration 2015) revealed that only 36% of the replications had statistically significant results, as opposed to 97% of the original studies; and effects in replications had, on average, half the magnitude of those originally reported.\n\nThe problem does not seem to be limited to psychology: replication projects in medicine (Prinz et al. 2011) and behavioral economics (Camerer et al. 2016) have also delivered relatively low success rates. Replication crises are arguably multifaceted and many different factors are likely to contribute to low reproducibility rates. Individual misconduct or even outright fraud are known to occur, but are likely to be the exception rather than the rule and cannot account for the problem (Fanelli 2009).",
    "is_useful": true,
    "question": "What challenges has the field of psychology faced regarding the reproducibility of research findings?"
  },
  {
    "text": "The results of this effort, published a few years later (Open Science Collaboration 2015) revealed that only 36% of the replications had statistically significant results, as opposed to 97% of the original studies; and effects in replications had, on average, half the magnitude of those originally reported.\n\nThe problem does not seem to be limited to psychology: replication projects in medicine (Prinz et al. 2011) and behavioral economics (Camerer et al. 2016) have also delivered relatively low success rates. Replication crises are arguably multifaceted and many different factors are likely to contribute to low reproducibility rates. Individual misconduct or even outright fraud are known to occur, but are likely to be the exception rather than the rule and cannot account for the problem (Fanelli 2009). Other possible sources of irreproducibility include factors that arise at various stages of the research process: design (e.g., selection biases), analysis (e.g., questionable research practices such as p-hacking) and publication (e.g., a preference for the publication of statistically significant findings) (Nosek et al. 2015; Ioannidis 2005, 2014; Ioannidis et al. 2014; Fanelli 2010; Simmons et al. 2011; John et al. 2012). In Psychology, at least, increased awareness of these sources of poor reproducibility has led to recent changes, including the large-scale adoption of several new practices in analysing and reporting research, which give reason for optimism (Nelson et al.",
    "is_useful": true,
    "question": "What are some of the factors contributing to low reproducibility rates in research across various fields?"
  },
  {
    "text": "Individual misconduct or even outright fraud are known to occur, but are likely to be the exception rather than the rule and cannot account for the problem (Fanelli 2009). Other possible sources of irreproducibility include factors that arise at various stages of the research process: design (e.g., selection biases), analysis (e.g., questionable research practices such as p-hacking) and publication (e.g., a preference for the publication of statistically significant findings) (Nosek et al. 2015; Ioannidis 2005, 2014; Ioannidis et al. 2014; Fanelli 2010; Simmons et al. 2011; John et al. 2012). In Psychology, at least, increased awareness of these sources of poor reproducibility has led to recent changes, including the large-scale adoption of several new practices in analysing and reporting research, which give reason for optimism (Nelson et al. 2018). These desirable practices described in further detail below\u2014are highly relevant to Experimental Philosophy research, which has mostly attempted to apply psychological methods in tackling philosophical questions. Notably, as a field, Experimental Philosophy seems aware that reproducibility can and should be monitored, with both organized replication projects (Cova et al. 2018) and online resources tracking replicability (http://experimentalphilosophy.yale.edu/xphipage/Experimental%20Philosophy-Replications.html).\n\nCritiques of research practices and the editorial handling of research outputs are not a recent phenomenon.",
    "is_useful": true,
    "question": "What factors contribute to irreproducibility in research, and what steps have been taken to improve the situation?"
  },
  {
    "text": "2014; Fanelli 2010; Simmons et al. 2011; John et al. 2012). In Psychology, at least, increased awareness of these sources of poor reproducibility has led to recent changes, including the large-scale adoption of several new practices in analysing and reporting research, which give reason for optimism (Nelson et al. 2018). These desirable practices described in further detail below\u2014are highly relevant to Experimental Philosophy research, which has mostly attempted to apply psychological methods in tackling philosophical questions. Notably, as a field, Experimental Philosophy seems aware that reproducibility can and should be monitored, with both organized replication projects (Cova et al. 2018) and online resources tracking replicability (http://experimentalphilosophy.yale.edu/xphipage/Experimental%20Philosophy-Replications.html).\n\nCritiques of research practices and the editorial handling of research outputs are not a recent phenomenon. For instance, within psychology Gigerenzer (2004) argued that statistical inference is an incoherent hybrid of the ideas of Fisher and of Neyman and Pearson. Others have drawn attention to the impact of publication biases on the literature, namely the publication or non-publication of research findings depending on the nature and direction of the results. Whilst publication biases seem to be common in many fields (Ioannidis 2005; Ioannidis et al. 2011), psychology is one of the fields where their impact has generated most discussion recently (Ferguson and Brannick 2012; Francis 2012, 2015; Francis et al.",
    "is_useful": true,
    "question": "What recent changes have been made in Psychology to address poor reproducibility in research?"
  },
  {
    "text": "2018) and online resources tracking replicability (http://experimentalphilosophy.yale.edu/xphipage/Experimental%20Philosophy-Replications.html).\n\nCritiques of research practices and the editorial handling of research outputs are not a recent phenomenon. For instance, within psychology Gigerenzer (2004) argued that statistical inference is an incoherent hybrid of the ideas of Fisher and of Neyman and Pearson. Others have drawn attention to the impact of publication biases on the literature, namely the publication or non-publication of research findings depending on the nature and direction of the results. Whilst publication biases seem to be common in many fields (Ioannidis 2005; Ioannidis et al. 2011), psychology is one of the fields where their impact has generated most discussion recently (Ferguson and Brannick 2012; Francis 2012, 2015; Francis et al. 2014; Ioannidis 2012). But again, this is not new: Rosenthal (1979) pointed out almost four decades ago that the preference for publishing positive (statistically significant) results is conducive to a file-drawer effect, resulting in an over-representation of false positives in the published record. The recent replication crisis has led an increasing number of scholars to issue warnings about the shortcomings of common practices. Consequently, it has become easier\u2014though still by no means easy\u2014to publish non-significant results (at least in psychology) and initiatives such as Open Science Framework1 have promoted greater transparency of research by encouraging openness of data and allowing pre-registration of studies.",
    "is_useful": true,
    "question": "What initiatives have been taken to promote greater transparency and openness in research practices?"
  },
  {
    "text": "2011), psychology is one of the fields where their impact has generated most discussion recently (Ferguson and Brannick 2012; Francis 2012, 2015; Francis et al. 2014; Ioannidis 2012). But again, this is not new: Rosenthal (1979) pointed out almost four decades ago that the preference for publishing positive (statistically significant) results is conducive to a file-drawer effect, resulting in an over-representation of false positives in the published record. The recent replication crisis has led an increasing number of scholars to issue warnings about the shortcomings of common practices. Consequently, it has become easier\u2014though still by no means easy\u2014to publish non-significant results (at least in psychology) and initiatives such as Open Science Framework1 have promoted greater transparency of research by encouraging openness of data and allowing pre-registration of studies.\n\nImportantly, consensus has also emerged that current reporting practices are problematic because insufficient details are often provided, preventing accurate interpretation and evaluation of findings (Miguel et al. 2014; Simmons et al. 2011).\n\nSome researchers have also stressed that statistical standards need to be revised. For instance, Benjamin et al. (2018) suggested a stricter threshold for defining statistical significance, whilst others stress instead that p values should not be treated as reliable measures given that they might vary (sometimes dramatically) across replications, even when the effects are real (Halsey et al. 2015; see also Cumming 2008).",
    "is_useful": true,
    "question": "What issues related to research practices have been highlighted in the context of psychology and the replication crisis?"
  },
  {
    "text": "Consequently, it has become easier\u2014though still by no means easy\u2014to publish non-significant results (at least in psychology) and initiatives such as Open Science Framework1 have promoted greater transparency of research by encouraging openness of data and allowing pre-registration of studies.\n\nImportantly, consensus has also emerged that current reporting practices are problematic because insufficient details are often provided, preventing accurate interpretation and evaluation of findings (Miguel et al. 2014; Simmons et al. 2011).\n\nSome researchers have also stressed that statistical standards need to be revised. For instance, Benjamin et al. (2018) suggested a stricter threshold for defining statistical significance, whilst others stress instead that p values should not be treated as reliable measures given that they might vary (sometimes dramatically) across replications, even when the effects are real (Halsey et al. 2015; see also Cumming 2008). Moreover, some have argued that null-hypothesis significance testing (NHST) and Bayesian inference may lead researchers to draw different conclusions in certain cases, and that the use of Bayesian statistics should be preferred when possible (Dienes and McLatchie 2018; Wagenmakers et al. 2018).\n\nThe concerns regarding statistical analysis and reporting, reviewed above, have led to various suggestions regarding the kinds of analyses that should be done and reported in order to improve the reproducibility of findings. These suggestions include a greater focus on full reporting of descriptive statistics, the use of confidence intervals and effect sizes, and the employment of power calculations.",
    "is_useful": true,
    "question": "What practices and approaches have been suggested to improve transparency, reproducibility, and statistical reporting in research?"
  },
  {
    "text": "(2018) suggested a stricter threshold for defining statistical significance, whilst others stress instead that p values should not be treated as reliable measures given that they might vary (sometimes dramatically) across replications, even when the effects are real (Halsey et al. 2015; see also Cumming 2008). Moreover, some have argued that null-hypothesis significance testing (NHST) and Bayesian inference may lead researchers to draw different conclusions in certain cases, and that the use of Bayesian statistics should be preferred when possible (Dienes and McLatchie 2018; Wagenmakers et al. 2018).\n\nThe concerns regarding statistical analysis and reporting, reviewed above, have led to various suggestions regarding the kinds of analyses that should be done and reported in order to improve the reproducibility of findings. These suggestions include a greater focus on full reporting of descriptive statistics, the use of confidence intervals and effect sizes, and the employment of power calculations. A growing body of literature provides details on the justification for these suggestions and how they should be implemented practically (Tellez et al. 2015; Fritz et al. 2012; Tressoldi and Giofr\u00e9 2015), discussing for instance which type of effect size to use in different circumstances (Sullivan and Feinn 2012; Lakens 2013).\n\nThese views and principles have become mainstream in psychological research.",
    "is_useful": true,
    "question": "What are some recommended practices to improve the reproducibility of research findings in psychological studies?"
  },
  {
    "text": "2018).\n\nThe concerns regarding statistical analysis and reporting, reviewed above, have led to various suggestions regarding the kinds of analyses that should be done and reported in order to improve the reproducibility of findings. These suggestions include a greater focus on full reporting of descriptive statistics, the use of confidence intervals and effect sizes, and the employment of power calculations. A growing body of literature provides details on the justification for these suggestions and how they should be implemented practically (Tellez et al. 2015; Fritz et al. 2012; Tressoldi and Giofr\u00e9 2015), discussing for instance which type of effect size to use in different circumstances (Sullivan and Feinn 2012; Lakens 2013).\n\nThese views and principles have become mainstream in psychological research. The 6th edition of the American Psychological Association Publication Manual greatly emphasizes the importance of reporting elements such as effect sizes, confidence intervals, and extensive description of procedures, which help convey the most complete meaning of the results (2010, p. 33).",
    "is_useful": true,
    "question": "What practices are recommended to enhance the reproducibility of findings in research?"
  },
  {
    "text": "These suggestions include a greater focus on full reporting of descriptive statistics, the use of confidence intervals and effect sizes, and the employment of power calculations. A growing body of literature provides details on the justification for these suggestions and how they should be implemented practically (Tellez et al. 2015; Fritz et al. 2012; Tressoldi and Giofr\u00e9 2015), discussing for instance which type of effect size to use in different circumstances (Sullivan and Feinn 2012; Lakens 2013).\n\nThese views and principles have become mainstream in psychological research. The 6th edition of the American Psychological Association Publication Manual greatly emphasizes the importance of reporting elements such as effect sizes, confidence intervals, and extensive description of procedures, which help convey the most complete meaning of the results (2010, p. 33). In addition, from January 2014 Psychological Science, the flagship journal of the Association for Psychological Science, recommends the use of the Bnew statistics^\u2014meta-analyses, effect sizes and confidence intervals\u2014to avoid problems associated with null-hypothesis significance testing2 (notably, confidence intervals are directly related to p-values\u2014see, for example, Altman and Bland 2011\u2014so the recommendation to replace one with the other is controversial; however, it is a testament to the current goal, within experimental psychology, of searching for more robust ways to find and report results). Psychological Science also encourages complete reporting of study design and methods.",
    "is_useful": true,
    "question": "What are some recommended reporting practices in psychological research to enhance the transparency and robustness of results?"
  },
  {
    "text": "These views and principles have become mainstream in psychological research. The 6th edition of the American Psychological Association Publication Manual greatly emphasizes the importance of reporting elements such as effect sizes, confidence intervals, and extensive description of procedures, which help convey the most complete meaning of the results (2010, p. 33). In addition, from January 2014 Psychological Science, the flagship journal of the Association for Psychological Science, recommends the use of the Bnew statistics^\u2014meta-analyses, effect sizes and confidence intervals\u2014to avoid problems associated with null-hypothesis significance testing2 (notably, confidence intervals are directly related to p-values\u2014see, for example, Altman and Bland 2011\u2014so the recommendation to replace one with the other is controversial; however, it is a testament to the current goal, within experimental psychology, of searching for more robust ways to find and report results). Psychological Science also encourages complete reporting of study design and methods. To allow authors to provide clear, complete, self-contained descriptions of their studies, the Methods and Results sections no longer count towards the total word limit. Overall, there is recent consensus in the literature around the following two recommendations:\n\n- a) Estimation based on effect sizes, confidence intervals, and meta-analysis usually provides a more informative analysis of empirical results than does null-hypothesis significance testing alone.\n- b) Transparency in the methodologies and procedures used to obtain research findings is required to allow for replication of such findings.",
    "is_useful": true,
    "question": "What are the current recommendations in psychological research regarding the reporting of study findings and methodologies?"
  },
  {
    "text": "Psychological Science also encourages complete reporting of study design and methods. To allow authors to provide clear, complete, self-contained descriptions of their studies, the Methods and Results sections no longer count towards the total word limit. Overall, there is recent consensus in the literature around the following two recommendations:\n\n- a) Estimation based on effect sizes, confidence intervals, and meta-analysis usually provides a more informative analysis of empirical results than does null-hypothesis significance testing alone.\n- b) Transparency in the methodologies and procedures used to obtain research findings is required to allow for replication of such findings.\n\nIn other words, a reader should be able to assess the merit of a study's findings through a full, statistically-transparent set of reported results, and identify the conditions necessary to conduct a replication (either exact or conceptual) of the original research design. Notably, the APA manual from 2010 and the Psychological Science submission guidelines from 2014 emphasize these aspects of reporting, but the need to fulfil these requirements has been recognized for decades; the problem was not that researchers were unaware of that it is better to report full methods and statistical measures like effect sizes (e.g., Cohen 1994), but that in practice, they have not tended to do so. We know this because several studies have assessed reporting standards over the years, generally finding a need for improvement: Matthews et al.",
    "is_useful": true,
    "question": "What are the key recommendations for improving the transparency and reporting of research methodologies in psychological studies?"
  },
  {
    "text": "- b) Transparency in the methodologies and procedures used to obtain research findings is required to allow for replication of such findings.\n\nIn other words, a reader should be able to assess the merit of a study's findings through a full, statistically-transparent set of reported results, and identify the conditions necessary to conduct a replication (either exact or conceptual) of the original research design. Notably, the APA manual from 2010 and the Psychological Science submission guidelines from 2014 emphasize these aspects of reporting, but the need to fulfil these requirements has been recognized for decades; the problem was not that researchers were unaware of that it is better to report full methods and statistical measures like effect sizes (e.g., Cohen 1994), but that in practice, they have not tended to do so. We know this because several studies have assessed reporting standards over the years, generally finding a need for improvement: Matthews et al. (2008) analyzed 101 articles published between 1996 and 2005 in 5 education journals, and found that the proportion of articles that reported effect sizes in their results increased from an average of 26% across journals in 1996\u20132000 to 46% in 2001\u20132005. As Matthews et al. (2008) noted, this shows a gradual improvement but the numbers were still low (and did not exceed 60% in any single journal), despite the fact that the APA manual already called for reporting effect sizes during the surveyed period. In line with this, Sun et al.",
    "is_useful": true,
    "question": "What is the importance of transparency in research methodologies for the purpose of replicating findings?"
  },
  {
    "text": "We know this because several studies have assessed reporting standards over the years, generally finding a need for improvement: Matthews et al. (2008) analyzed 101 articles published between 1996 and 2005 in 5 education journals, and found that the proportion of articles that reported effect sizes in their results increased from an average of 26% across journals in 1996\u20132000 to 46% in 2001\u20132005. As Matthews et al. (2008) noted, this shows a gradual improvement but the numbers were still low (and did not exceed 60% in any single journal), despite the fact that the APA manual already called for reporting effect sizes during the surveyed period. In line with this, Sun et al. (2010) surveyed articles published between 2005 and 2007 in six American Psychological Association (APA) journals, and Fritz et al. (2012) examined articles published a few years later (2009\u20132010) in a leading psychology journal (Journal of Experimental Psychology: General); both investigations found that only around 40% of surveyed articles reported effect sizes. A significant change\u2014at least in psychology seems to have come about, however, in the 2010s, possibly following the field's replication crisis (Nelson et al. 2018). For example, Tressoldi et al.",
    "is_useful": true,
    "question": "What trend has been observed in the reporting of effect sizes in scholarly articles over the years, particularly in the field of psychology?"
  },
  {
    "text": "As Matthews et al. (2008) noted, this shows a gradual improvement but the numbers were still low (and did not exceed 60% in any single journal), despite the fact that the APA manual already called for reporting effect sizes during the surveyed period. In line with this, Sun et al. (2010) surveyed articles published between 2005 and 2007 in six American Psychological Association (APA) journals, and Fritz et al. (2012) examined articles published a few years later (2009\u20132010) in a leading psychology journal (Journal of Experimental Psychology: General); both investigations found that only around 40% of surveyed articles reported effect sizes. A significant change\u2014at least in psychology seems to have come about, however, in the 2010s, possibly following the field's replication crisis (Nelson et al. 2018). For example, Tressoldi et al. (2013) examined papers published in 2011 in four high-impact and three field-specific psychology journals, and found that in most journals (the notable exceptions were Nature and\n\n<sup>2</sup> https://www.psychologicalscience.org/publications/psychological_science/ps-submissions#STAT\n\nScience) effect sizes and/or confidence intervals were reported in a majority of articles\u2014in the Journal of Experimental Psychology: Applied, for example, these measures were included in 90% of papers.",
    "is_useful": true,
    "question": "What trends have been observed in the reporting of effect sizes in psychology journals over the years?"
  },
  {
    "text": "A significant change\u2014at least in psychology seems to have come about, however, in the 2010s, possibly following the field's replication crisis (Nelson et al. 2018). For example, Tressoldi et al. (2013) examined papers published in 2011 in four high-impact and three field-specific psychology journals, and found that in most journals (the notable exceptions were Nature and\n\n<sup>2</sup> https://www.psychologicalscience.org/publications/psychological_science/ps-submissions#STAT\n\nScience) effect sizes and/or confidence intervals were reported in a majority of articles\u2014in the Journal of Experimental Psychology: Applied, for example, these measures were included in 90% of papers. Corroborating this trend, Counsell and Harlow (2017) recently reported that over 90% of papers published in four Canadian Psychology journals in 2013 reported measures of effect size, suggesting that the constant calls for reporting effect sizes appear to have had an effect.\n\nHowever, reporting practices may differ across research fields, as well as across the hierarchy of publication venues. In particular, as noted in the previous paragraph, there has been criticism of the sparse reporting standards imposed by some high-visibility, highimpact journals: Tressoldi et al. (2013) documented widespread use of null hypothesis significance testing without any use of confidence intervals, effect size, prospective power and model estimation in high-impact journals such as Science and Nature.",
    "is_useful": true,
    "question": "What significant change in reporting practices in the field of psychology emerged in the 2010s following the replication crisis?"
  },
  {
    "text": "Corroborating this trend, Counsell and Harlow (2017) recently reported that over 90% of papers published in four Canadian Psychology journals in 2013 reported measures of effect size, suggesting that the constant calls for reporting effect sizes appear to have had an effect.\n\nHowever, reporting practices may differ across research fields, as well as across the hierarchy of publication venues. In particular, as noted in the previous paragraph, there has been criticism of the sparse reporting standards imposed by some high-visibility, highimpact journals: Tressoldi et al. (2013) documented widespread use of null hypothesis significance testing without any use of confidence intervals, effect size, prospective power and model estimation in high-impact journals such as Science and Nature.\n\nIn addition, a number of studies\u2014including recent ones\u2014have documented that researchers often fail to provide sufficient methods information for conducting replications (e.g., Sifers et al. 2002; Raad et al. 2007; Bouwmeester et al. 2012; Pierce et al. 2014). For instance, Sifers et al. (2002) explored reporting of demographic and methodological information in four major paediatric and child psychology journals. They found that information about sample size and age was almost always reported, yet providing details about ethnicity, socioeconomic status (SES) and the exclusion/ inclusion of participants was far from being the norm. More recently, Pierce et al. (2014) also found that the reporting of ethnicity information in three major autism research journals was largely unsatisfactory.",
    "is_useful": true,
    "question": "What are some common issues in reporting practices within scientific research?"
  },
  {
    "text": "In addition, a number of studies\u2014including recent ones\u2014have documented that researchers often fail to provide sufficient methods information for conducting replications (e.g., Sifers et al. 2002; Raad et al. 2007; Bouwmeester et al. 2012; Pierce et al. 2014). For instance, Sifers et al. (2002) explored reporting of demographic and methodological information in four major paediatric and child psychology journals. They found that information about sample size and age was almost always reported, yet providing details about ethnicity, socioeconomic status (SES) and the exclusion/ inclusion of participants was far from being the norm. More recently, Pierce et al. (2014) also found that the reporting of ethnicity information in three major autism research journals was largely unsatisfactory.\n\nAll the above is of particular relevance to Experimental Philosophy, a new field in the process of establishing its methodologies and reporting conventions. Furthermore, Experimental Philosophy is interested in people's attitudes and behaviors, and largely employs methods and analysis strategies commonly used in Experimental Psychology, making it a sub-field, or at least sister-field, of that older discipline. It would thus be advisable for Experimental Philosophers to take heed of both established experimental design principles (Carmel 2011) and the recent turmoil regarding analysis and reporting practices that science in general, and Psychology in particular, have been undergoing.",
    "is_useful": true,
    "question": "What challenges do researchers face in providing sufficient methods information for replicating studies, particularly in fields like Experimental Philosophy?"
  },
  {
    "text": "They found that information about sample size and age was almost always reported, yet providing details about ethnicity, socioeconomic status (SES) and the exclusion/ inclusion of participants was far from being the norm. More recently, Pierce et al. (2014) also found that the reporting of ethnicity information in three major autism research journals was largely unsatisfactory.\n\nAll the above is of particular relevance to Experimental Philosophy, a new field in the process of establishing its methodologies and reporting conventions. Furthermore, Experimental Philosophy is interested in people's attitudes and behaviors, and largely employs methods and analysis strategies commonly used in Experimental Psychology, making it a sub-field, or at least sister-field, of that older discipline. It would thus be advisable for Experimental Philosophers to take heed of both established experimental design principles (Carmel 2011) and the recent turmoil regarding analysis and reporting practices that science in general, and Psychology in particular, have been undergoing. The need to be vigilant in trying to avoid other fields' past mistakes is particularly important because Experimental Philosophy research is conducted mostly by philosophers, who are often not trained in experimental work and only rarely receive any training in statistics. To their credit, most Experimental Philosophers (to the best of our knowledge) do make an effort to acquire some statistical expertise, and many collaborate with trained psychologists. We also acknowledge that experimental philosophers are a heterogeneous group, and that some have gained considerable quantitative competence by informal means. But does the publication record in Experimental Philosophy demonstrate that these efforts are sufficient?",
    "is_useful": true,
    "question": "What challenges does Experimental Philosophy face in terms of methodology and reporting conventions in comparison to established fields like Psychology?"
  },
  {
    "text": "It would thus be advisable for Experimental Philosophers to take heed of both established experimental design principles (Carmel 2011) and the recent turmoil regarding analysis and reporting practices that science in general, and Psychology in particular, have been undergoing. The need to be vigilant in trying to avoid other fields' past mistakes is particularly important because Experimental Philosophy research is conducted mostly by philosophers, who are often not trained in experimental work and only rarely receive any training in statistics. To their credit, most Experimental Philosophers (to the best of our knowledge) do make an effort to acquire some statistical expertise, and many collaborate with trained psychologists. We also acknowledge that experimental philosophers are a heterogeneous group, and that some have gained considerable quantitative competence by informal means. But does the publication record in Experimental Philosophy demonstrate that these efforts are sufficient?\n\nThe present investigation aims to contribute to the literature on reporting practices in scientific research and to the healthy development of Experimental Philosophy, by empirically exploring the reporting of methods, analyses, and results in Experimental Philosophy publications. Assessing the overall quality of research in a field is complicated and may be impossible; there is no consensus, for example, on how to quantify the quality of research designs, so opinions may (and frequently do) differ on this aspect of any given study. But\u2014as the literature cited above indicates\u2014many of the proposed solutions for poor replicability in empirical research emphasize the consistent adoption of appropriate reporting practices.",
    "is_useful": true,
    "question": "What practices are suggested for improving the quality of research and reporting in Experimental Philosophy?"
  },
  {
    "text": "To their credit, most Experimental Philosophers (to the best of our knowledge) do make an effort to acquire some statistical expertise, and many collaborate with trained psychologists. We also acknowledge that experimental philosophers are a heterogeneous group, and that some have gained considerable quantitative competence by informal means. But does the publication record in Experimental Philosophy demonstrate that these efforts are sufficient?\n\nThe present investigation aims to contribute to the literature on reporting practices in scientific research and to the healthy development of Experimental Philosophy, by empirically exploring the reporting of methods, analyses, and results in Experimental Philosophy publications. Assessing the overall quality of research in a field is complicated and may be impossible; there is no consensus, for example, on how to quantify the quality of research designs, so opinions may (and frequently do) differ on this aspect of any given study. But\u2014as the literature cited above indicates\u2014many of the proposed solutions for poor replicability in empirical research emphasize the consistent adoption of appropriate reporting practices. Use of these practices\u2014specifically, the reporting of sufficient method information that would allow others to replicate the study, and the use of statistical measures such as effect sizes and confidence intervals to complement p-values\u2014can be quantified; furthermore, at least in Psychology, improvements in reporting standards (e.g., Counsell and Harlow 2017) have occurred alongside an overall shift in a host of practices that has led to optimism about resolving the field's replication crisis (Nelson et al. 2018).",
    "is_useful": true,
    "question": "What practices are highlighted as essential for improving transparency and replicability in scientific research?"
  },
  {
    "text": "Assessing the overall quality of research in a field is complicated and may be impossible; there is no consensus, for example, on how to quantify the quality of research designs, so opinions may (and frequently do) differ on this aspect of any given study. But\u2014as the literature cited above indicates\u2014many of the proposed solutions for poor replicability in empirical research emphasize the consistent adoption of appropriate reporting practices. Use of these practices\u2014specifically, the reporting of sufficient method information that would allow others to replicate the study, and the use of statistical measures such as effect sizes and confidence intervals to complement p-values\u2014can be quantified; furthermore, at least in Psychology, improvements in reporting standards (e.g., Counsell and Harlow 2017) have occurred alongside an overall shift in a host of practices that has led to optimism about resolving the field's replication crisis (Nelson et al. 2018). Although it is too soon to tell whether psychological findings have become more reproducible in recent years, and whether any such improvements can be causally linked to changes in reporting practices, it is not too soon to examine whether Experimental Philosophy has taken heed of these developments and adopted appropriate reporting standards. Therefore, in the present study we focus on experimental philosophers' reporting of effect sizes, confidence intervals and statistical power, as well as the transparency of information about research procedures (see full details in the Method and Supplementary Information (https://osf.io/yp2kg)).",
    "is_useful": true,
    "question": "What practices are emphasized to improve the replicability of empirical research in the context of open science?"
  },
  {
    "text": "2018). Although it is too soon to tell whether psychological findings have become more reproducible in recent years, and whether any such improvements can be causally linked to changes in reporting practices, it is not too soon to examine whether Experimental Philosophy has taken heed of these developments and adopted appropriate reporting standards. Therefore, in the present study we focus on experimental philosophers' reporting of effect sizes, confidence intervals and statistical power, as well as the transparency of information about research procedures (see full details in the Method and Supplementary Information (https://osf.io/yp2kg)). In doing so, we hope to help the Experimental Philosophy community establish appropriate reporting standards and ensure that, over time, the body of work produced in this field is largely reproducible.\n\n# 2 Methods\n\nWe examined design, analysis, and reporting of research in Experimental Philosophy. We defined Experimental Philosophy broadly (Rose and Danks 2013; Machery and O'Neill 2014) and identified relevant Experimental Philosophy papers by following a modified version of Polonioli's (2017) methodology.\n\n#### 2.1 Inclusion Criteria\n\nFirst, we selected a broad sample of peer-reviewed philosophy journals. A natural way to identify the most important journals is by appealing to the journal impact factor (IF), which is the most common measure of a journal's impact and quality (though see criticisms of this measure by Horvat et al. 2016; Brembs et al. 2013; Moustafa 2014).",
    "is_useful": true,
    "question": "What standards are being examined to improve the reproducibility of research in Experimental Philosophy?"
  },
  {
    "text": "In doing so, we hope to help the Experimental Philosophy community establish appropriate reporting standards and ensure that, over time, the body of work produced in this field is largely reproducible.\n\n# 2 Methods\n\nWe examined design, analysis, and reporting of research in Experimental Philosophy. We defined Experimental Philosophy broadly (Rose and Danks 2013; Machery and O'Neill 2014) and identified relevant Experimental Philosophy papers by following a modified version of Polonioli's (2017) methodology.\n\n#### 2.1 Inclusion Criteria\n\nFirst, we selected a broad sample of peer-reviewed philosophy journals. A natural way to identify the most important journals is by appealing to the journal impact factor (IF), which is the most common measure of a journal's impact and quality (though see criticisms of this measure by Horvat et al. 2016; Brembs et al. 2013; Moustafa 2014). Unfortunately, an IF is unavailable for most journals in philosophy; other available classifications of journals were thus considered in our study (Polonioli 2016). One quantitative ranking is provided by the h-index, and it is possible to find a ranking of philosophy journals based on this last metric.3 Informal polls are also a popular way of ranking philosophy journals, and a rather established ranking is published on the blog www.leiterreports.com. Here, we considered all of the journals that both publish original peer-reviewed research and are included in rankings based on h-index and Leiter Report's poll, and selected the top-20 relevant journals from each of these two rankings.",
    "is_useful": true,
    "question": "What is the goal of the Experimental Philosophy community regarding research standards and reproducibility?"
  },
  {
    "text": "Of the journals included in either ranking, only the journal Philosophy Compass was excluded because it publishes only (typically invited) review articles. Because of the partial overlap between the lists, the sample eventually included the 29 journals.\n\nSecond, we selected all papers employing quantitative methods that were published between 2013 and 2016 in the 29 philosophy journals above. Polonioli (2017) considered\n\n<sup>3</sup> https://scholar.google.co.uk/citations?view_op=top_venues&hl=it&vq=hum_philosophy\n\nonly 3 years (2013\u20132015); here we considered an additional, fourth year. Further, unlike Polonioli (2017), we excluded qualitative research articles because in the current study the focus is on the handling of quantitative results. To select the relevant papers, we accessed the PDF version of each article published in the chosen journals between 2013 and 2016, and searched for the keywords 'experiment,' 'empirical,' 'subject(s),' 'participant(s),' 'sample,' 'test,' and 'statistic(al).' In cases where we deemed the keyword-based search strategy to be less effective for discriminating between empirical research and literature reviews, we read the paper. This process resulted in the identification of 134 articles as quantitative research articles. Information regarding the journals in our sample and the number of quantitative articles per journal is listed below (for a full list of the articles, see the Supplementary Information (https://osf.io/yp2kg)):\n\n- 1.",
    "is_useful": true,
    "question": "What was the focus of the study conducted on philosophy journals published between 2013 and 2016?"
  },
  {
    "text": "Further, unlike Polonioli (2017), we excluded qualitative research articles because in the current study the focus is on the handling of quantitative results. To select the relevant papers, we accessed the PDF version of each article published in the chosen journals between 2013 and 2016, and searched for the keywords 'experiment,' 'empirical,' 'subject(s),' 'participant(s),' 'sample,' 'test,' and 'statistic(al).' In cases where we deemed the keyword-based search strategy to be less effective for discriminating between empirical research and literature reviews, we read the paper. This process resulted in the identification of 134 articles as quantitative research articles. Information regarding the journals in our sample and the number of quantitative articles per journal is listed below (for a full list of the articles, see the Supplementary Information (https://osf.io/yp2kg)):\n\n- 1. No\u00fbs (n = 4)\n- 2. Philosophical studies (n = 11)\n- 3. Philosophy and Phenomenological Research (n = 4)\n- 4. Mind (n = 1)\n- 5. Analysis (n = 2)\n- 6. Synthese (n = 13)\n- 7. Mind and language (n = 7)\n- 8. Philosophers' Imprint (n = 3)\n- 9. Australasian journal of philosophy (n = 2)\n- 10. Erkenntnis (n = 2)\n- 11.",
    "is_useful": true,
    "question": "What was the approach taken to identify quantitative research articles in the selected journals?"
  },
  {
    "text": "Journal of Philosophical Logic (n = 0)\n- 24. Pacific Philosophical Quarterly (n = 0)\n- 25. American Philosophical Quarterly (n = 0)\n- 26. Studies in Philosophy and Education (n = 0)\n- 27. European Journal of Political Theory (n = 0)\n- 28. Proceedings of the Aristotelian Society (n = 0)\n- 29. European Journal of Philosophy (n = 0)\n\n#### 2.2 Procedure\n\nThe selected articles were screened according to an adapted and expanded version of the list of categories used by Tressoldi et al. (2013). We searched for reporting of confidence intervals as well as measures of effect size, interpretations of reported effect size and details on prospective statistical power. Presence of these items was coded in a binary fashion (present/absent). Following Tressoldi et al. (2013), we applied lenient criteria\u2014a feature was coded as present if at least one instance of it appeared in the paper (i.e., if a single effect size was reported in a paper, we coded effect sizes as present in that paper, even if there were other points in that paper where an effect size could have been reported but was not). We also screened papers to identify mentions of p values or null hypothesis significance testing, and the use of error bars in figures. Further, we also explored the extent to which papers adopted Bayesian statistics rather than null hypothesis significance testing.",
    "is_useful": true,
    "question": "What statistical reporting practices are commonly evaluated in research articles related to open science?"
  },
  {
    "text": "(2013). We searched for reporting of confidence intervals as well as measures of effect size, interpretations of reported effect size and details on prospective statistical power. Presence of these items was coded in a binary fashion (present/absent). Following Tressoldi et al. (2013), we applied lenient criteria\u2014a feature was coded as present if at least one instance of it appeared in the paper (i.e., if a single effect size was reported in a paper, we coded effect sizes as present in that paper, even if there were other points in that paper where an effect size could have been reported but was not). We also screened papers to identify mentions of p values or null hypothesis significance testing, and the use of error bars in figures. Further, we also explored the extent to which papers adopted Bayesian statistics rather than null hypothesis significance testing. For the full coding instructions we worked with, see the Supplementary Information (https://osf.io/yp2kg).\n\nThe procedures and methodology used in our study also try to complement the perspective offered by Tressoldi et al. (2013) by exploring further aspects of reporting. These include reporting of information on sample size and demographics, details of study design, and criteria for exclusion of participants. Finally, we examine reports of prospective and/or observed statistical power. By covering a wider set of aspects than previous efforts, the present study is better equipped to provide a snapshot of current practices with regard to design, analysis, and reporting of research in Experimental Philosophy.",
    "is_useful": true,
    "question": "What aspects of research design, analysis, and reporting in Experimental Philosophy are assessed in the study?"
  },
  {
    "text": "We also screened papers to identify mentions of p values or null hypothesis significance testing, and the use of error bars in figures. Further, we also explored the extent to which papers adopted Bayesian statistics rather than null hypothesis significance testing. For the full coding instructions we worked with, see the Supplementary Information (https://osf.io/yp2kg).\n\nThe procedures and methodology used in our study also try to complement the perspective offered by Tressoldi et al. (2013) by exploring further aspects of reporting. These include reporting of information on sample size and demographics, details of study design, and criteria for exclusion of participants. Finally, we examine reports of prospective and/or observed statistical power. By covering a wider set of aspects than previous efforts, the present study is better equipped to provide a snapshot of current practices with regard to design, analysis, and reporting of research in Experimental Philosophy. It allows for a health check of practices in the field, informing discussions about findings' replicability as well as the possibility of their inclusion in meta-analytic studies.\n\nWe also examine the relationship between citations and quality of reporting. The association between reporting standards and impact (in the form of citations) is currently unclear. On the one hand, publications may be cited for a variety of reasons. For instance, researchers may cite others to support their own claims, methodology or findings. Other papers are cited in order to criticize their central claims (Harwood 2008).",
    "is_useful": true,
    "question": "What aspects of research reporting does the study aim to explore in relation to current practices in design, analysis, and reporting?"
  },
  {
    "text": "These include reporting of information on sample size and demographics, details of study design, and criteria for exclusion of participants. Finally, we examine reports of prospective and/or observed statistical power. By covering a wider set of aspects than previous efforts, the present study is better equipped to provide a snapshot of current practices with regard to design, analysis, and reporting of research in Experimental Philosophy. It allows for a health check of practices in the field, informing discussions about findings' replicability as well as the possibility of their inclusion in meta-analytic studies.\n\nWe also examine the relationship between citations and quality of reporting. The association between reporting standards and impact (in the form of citations) is currently unclear. On the one hand, publications may be cited for a variety of reasons. For instance, researchers may cite others to support their own claims, methodology or findings. Other papers are cited in order to criticize their central claims (Harwood 2008). Some papers are cited as examples of well-conducted research, while others might be cited as examples of research that is poorly designed or conducted (Aksnes and Sivertsen 2004). Nevertheless, while citation counts are a function of many variables, when a particular paper is cited more than others it is usually assumed that this reflects its higher quality (Bornmann et al. 2012); even when authors disagree with cited research, it is assumed they would not go to the trouble to argue with low-quality work, and that\u2014at least in the long run\u2014low-quality work will be condemned to oblivion while good work continues to get cited (and sometimes debated).",
    "is_useful": true,
    "question": "What factors are examined to assess the quality of reporting and its impact on citations in research practices? "
  },
  {
    "text": "On the one hand, publications may be cited for a variety of reasons. For instance, researchers may cite others to support their own claims, methodology or findings. Other papers are cited in order to criticize their central claims (Harwood 2008). Some papers are cited as examples of well-conducted research, while others might be cited as examples of research that is poorly designed or conducted (Aksnes and Sivertsen 2004). Nevertheless, while citation counts are a function of many variables, when a particular paper is cited more than others it is usually assumed that this reflects its higher quality (Bornmann et al. 2012); even when authors disagree with cited research, it is assumed they would not go to the trouble to argue with low-quality work, and that\u2014at least in the long run\u2014low-quality work will be condemned to oblivion while good work continues to get cited (and sometimes debated). This assumption underlies the importance, in present-day academia, of individual researchers' h-index and the impact factors of the journals they publish in: both citation measures influence careers by direct effects on promotions, tenure decisions, and success in research funding applications (Acuna et al. 2012).\n\nBut is the assumed association between quality and number of citations real? Egghe and Rousseau (1990) suggested that four important assumptions form the basis for all reliance on citation counts.",
    "is_useful": true,
    "question": "What factors influence the citation of academic publications and its perceived quality in research?"
  },
  {
    "text": "Nevertheless, while citation counts are a function of many variables, when a particular paper is cited more than others it is usually assumed that this reflects its higher quality (Bornmann et al. 2012); even when authors disagree with cited research, it is assumed they would not go to the trouble to argue with low-quality work, and that\u2014at least in the long run\u2014low-quality work will be condemned to oblivion while good work continues to get cited (and sometimes debated). This assumption underlies the importance, in present-day academia, of individual researchers' h-index and the impact factors of the journals they publish in: both citation measures influence careers by direct effects on promotions, tenure decisions, and success in research funding applications (Acuna et al. 2012).\n\nBut is the assumed association between quality and number of citations real? Egghe and Rousseau (1990) suggested that four important assumptions form the basis for all reliance on citation counts. First, citing an article implies actual use of that document by the citing author; second, citation reflects the merit (quality, significance, impact) of the cited article; third, the best possible work relevant to any point is cited; and fourth, a cited article is related in content to the one that cites it. In light of these assumptions, it is important to distinguish between the normative claim that quality should be a key factor in determining citations and the empirical claim that quality does correlate with citation counts. For example, Baird and Oppenheim (1994) investigated citations in Information Science, and concluded that Bcitation counts mean a statistical likelihood\n\nof high quality research^.",
    "is_useful": true,
    "question": "What factors are believed to influence citation counts and their relation to perceptions of research quality in academia?"
  },
  {
    "text": "2012).\n\nBut is the assumed association between quality and number of citations real? Egghe and Rousseau (1990) suggested that four important assumptions form the basis for all reliance on citation counts. First, citing an article implies actual use of that document by the citing author; second, citation reflects the merit (quality, significance, impact) of the cited article; third, the best possible work relevant to any point is cited; and fourth, a cited article is related in content to the one that cites it. In light of these assumptions, it is important to distinguish between the normative claim that quality should be a key factor in determining citations and the empirical claim that quality does correlate with citation counts. For example, Baird and Oppenheim (1994) investigated citations in Information Science, and concluded that Bcitation counts mean a statistical likelihood\n\nof high quality research^. Conversely, Nieminen et al. (2006) examined citations of studies in Psychiatry\u2014specifically to determine whether reporting quality and statistical analyses were associated with citations\u2014and found no such correlation. Even if we disregard the normative question of whether reporting quality should be a factor in deciding whether a given paper gets cited, it is useful to examine whether in practice, the reporting quality of studies in Experimental Philosophy is reflected in their impact as indexed by number of citations. We thus examined the number of times each of the analyzed articles has been cited4 and explored whether the citation count was related to our measures of reporting quality.",
    "is_useful": true,
    "question": "What key assumptions are fundamental to the reliance on citation counts in academic research?"
  },
  {
    "text": "# 3 Results\n\nTable 2 shows a summary of descriptive results pertaining to the whole sample of papers.\n\nAuthor Characteristics Although over half of the papers had at least one nonphilosopher co-author, the first and corresponding author of nearly three-quarters of them was a philosopher, suggesting that non-philosophers may often play an advisory role\u2014perhaps on issues of methodology. This raises potential concerns about the large minority of papers that did not have co-authors with methodological and statistical expertise. We are, of course, unable to estimate the number of philosopher authors who had obtained relevant training, but we note again that statistics and experimental design\n\n<sup>4</sup> We used Google Scholar to count citations. Although this is not necessarily the most accurate database, it does incorporate citations from outside the field of philosophy (unlike Philpapers), and is not manually curated (unlike Scopus, Web of Knowledge or PubMed), meaning it offers the widest collection of citations for each of the papers we analysed.\n\n| of Summary 1 Table | for measures agreement | with papers the | coders two |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Variable |  |  | agreement Frequency | Agreement % | k | E. S. Asymp.",
    "is_useful": true,
    "question": "What concerns arise regarding the authorship dynamics and methodological expertise in academic papers?"
  },
  {
    "text": "933]a |\n| Factor Bayes |  |  | 72 | 100 | NaN | NaN | NaNa |  |\n\na At least one variable in the\n\ncross-tabulation\n\n is a constant\n\n| Variable | n | % papers |\n| --- | --- | --- |\n| Author characteristics |  |  |\n| First author a philosopher | 95 | 71% |\n| Corresponding author a philosopher | 97 | 72% |\n| At least one non-philosopher | 73 | 54% |\n| Methods information |  |  |\n| Sample size reported | 133 | 99% |\n| Participant Demographics reported | 100 | 75% |\n| Design reported | 91 | 68% |\n| M-Turk used for data collection | 59 | 44% |\n| Participant exclusions reported | 59 | 44% |\n| Descriptive statistics |  |  |\n| Central tendency measures (means, medians) | 130 | 97% |\n| and/or frequency measures (e.g.",
    "is_useful": true,
    "question": "What percentage of papers reported the sample size in the context of open science?"
  },
  {
    "text": "Of course, the mere fact that a coauthor is not a philosopher does not automatically imply that they possess the relevant quantitative competence. We therefore checked the affiliations of the non-philosopher co-authors of papers in our sample: of the 73 papers that had at least one nonphilosopher co-author, 59 had co-authors affiliated with Psychology or Cognitive Science, 7 with Economics or Business, 2 with Computer Science and 4 with medicine. Overall, the overwhelming majority of papers had a co-author from fields in which the acquisition of statistical and quantitative skills is a standard part of academic training.\n\nMethods Information Details of methods were generally reported in reasonable detail. Nearly all the papers we examined reported sample size; a majority explicitly reported demographics, allowing for assessment of the study's generalizability (at least with regard to age and gender, the most common reported categories). Most studies explicitly described the study's design either in the relevant part of the Methods section or alongside the statistical test employed (we note that an explicit, formal description of the design\u2014i.e., reporting whether the manipulation was within or between participants, specifying the independent and dependent variables, etc.\u2014is not a universal requirement for scientific reports; although it is considered good practice, an attentive reader can usually fathom the design if the procedure is clearly described). A little under half of the studies were conducted online and data were collected using the M-Turk platform; this popular platform allows for efficient data collection, but has also been reported to yield discrepant results with laboratory studies in some experimental psychology paradigms (Crump et al.",
    "is_useful": true,
    "question": "What disciplines are most commonly associated with non-philosopher co-authors in academic papers, and what aspects of study methods are typically reported in these papers?"
  },
  {
    "text": "Nearly all the papers we examined reported sample size; a majority explicitly reported demographics, allowing for assessment of the study's generalizability (at least with regard to age and gender, the most common reported categories). Most studies explicitly described the study's design either in the relevant part of the Methods section or alongside the statistical test employed (we note that an explicit, formal description of the design\u2014i.e., reporting whether the manipulation was within or between participants, specifying the independent and dependent variables, etc.\u2014is not a universal requirement for scientific reports; although it is considered good practice, an attentive reader can usually fathom the design if the procedure is clearly described). A little under half of the studies were conducted online and data were collected using the M-Turk platform; this popular platform allows for efficient data collection, but has also been reported to yield discrepant results with laboratory studies in some experimental psychology paradigms (Crump et al. 2013), suggesting that its usefulness should not be taken for granted but rather evaluated on a case-by-case basis. Finally, fewer than half of the papers provided information about participants removed from analysis (both number of participants removed and reason for exclusion); it is impossible, of course, to know how many of the papers that did not report exclusions actually had any, but we note that exclusions e.g., of outliers\u2014are an extremely common practice in psychology.\n\nDescriptive Statistics Nearly all the papers reported their results in terms of either central tendency measures (means, medians or modes) or frequencies (either raw numbers or percentages).",
    "is_useful": true,
    "question": "What are some common practices in reporting study design and participant demographics in scientific research?"
  },
  {
    "text": "A little under half of the studies were conducted online and data were collected using the M-Turk platform; this popular platform allows for efficient data collection, but has also been reported to yield discrepant results with laboratory studies in some experimental psychology paradigms (Crump et al. 2013), suggesting that its usefulness should not be taken for granted but rather evaluated on a case-by-case basis. Finally, fewer than half of the papers provided information about participants removed from analysis (both number of participants removed and reason for exclusion); it is impossible, of course, to know how many of the papers that did not report exclusions actually had any, but we note that exclusions e.g., of outliers\u2014are an extremely common practice in psychology.\n\nDescriptive Statistics Nearly all the papers reported their results in terms of either central tendency measures (means, medians or modes) or frequencies (either raw numbers or percentages). Just over a quarter reported standard errors, although over 40% included error bars in figures. Of those papers whose figures showed error bars (n = 58), the bars represented either standard errors (n = 22, 38%) or confidence intervals (n = 17, 29%). One paper (~2%) included one figure showing standard errors and another figure showing confidence intervals, and the remaining papers showing error bars did not report what the bars represented in the figure(s) (n = 18, 31%). None of the papers that identified what their error bars represented used them to show standard deviations.",
    "is_useful": true,
    "question": "What are some common practices in reporting statistical results in psychology research?"
  },
  {
    "text": "Descriptive Statistics Nearly all the papers reported their results in terms of either central tendency measures (means, medians or modes) or frequencies (either raw numbers or percentages). Just over a quarter reported standard errors, although over 40% included error bars in figures. Of those papers whose figures showed error bars (n = 58), the bars represented either standard errors (n = 22, 38%) or confidence intervals (n = 17, 29%). One paper (~2%) included one figure showing standard errors and another figure showing confidence intervals, and the remaining papers showing error bars did not report what the bars represented in the figure(s) (n = 18, 31%). None of the papers that identified what their error bars represented used them to show standard deviations.\n\nInferential Statistics Almost all of the papers applied null hypothesis significance testing, but only just over half of them reported measures of effect size, and less than a quarter of the sample provided an interpretation of a reported effect size. Similarly, fewer than a third complemented the reported p-values with confidence intervals. A very small number of studies reported analyses of statistical power\u2014either prospective or observed, despite a growing concern that the preponderance of underpowered studies is contributing to the high proportion of false positives in scientific publishing (Button et al. 2013; Open Science Collaboration 2015). Finally, only two studies employed Bayes Factors. Although this practice is not yet pervasive in related fields such as experimental psychology either, adopting Bayesian approaches has been recommended as a way to address the shortcomings of NHST.",
    "is_useful": true,
    "question": "What statistical practices in research publications reflect the challenges and recommendations associated with open science methodologies?"
  },
  {
    "text": "None of the papers that identified what their error bars represented used them to show standard deviations.\n\nInferential Statistics Almost all of the papers applied null hypothesis significance testing, but only just over half of them reported measures of effect size, and less than a quarter of the sample provided an interpretation of a reported effect size. Similarly, fewer than a third complemented the reported p-values with confidence intervals. A very small number of studies reported analyses of statistical power\u2014either prospective or observed, despite a growing concern that the preponderance of underpowered studies is contributing to the high proportion of false positives in scientific publishing (Button et al. 2013; Open Science Collaboration 2015). Finally, only two studies employed Bayes Factors. Although this practice is not yet pervasive in related fields such as experimental psychology either, adopting Bayesian approaches has been recommended as a way to address the shortcomings of NHST.\n\nAssociation Between Number of Citations and Reporting Practices Are better studies (or at least ones with better reporting) cited more? We performed pointbiserial correlations to explore possible associations between number of citations and statistical reporting practices (specifically, whether or not ES, CI and SE were reported) as well as the association between number of citations and quality indicators of methodology reporting (whether or not descriptive statistics, sample size or demographics were reported).",
    "is_useful": true,
    "question": "What statistical reporting practices are often lacking in scientific publications, and how might this affect the prevalence of false positives?"
  },
  {
    "text": "A very small number of studies reported analyses of statistical power\u2014either prospective or observed, despite a growing concern that the preponderance of underpowered studies is contributing to the high proportion of false positives in scientific publishing (Button et al. 2013; Open Science Collaboration 2015). Finally, only two studies employed Bayes Factors. Although this practice is not yet pervasive in related fields such as experimental psychology either, adopting Bayesian approaches has been recommended as a way to address the shortcomings of NHST.\n\nAssociation Between Number of Citations and Reporting Practices Are better studies (or at least ones with better reporting) cited more? We performed pointbiserial correlations to explore possible associations between number of citations and statistical reporting practices (specifically, whether or not ES, CI and SE were reported) as well as the association between number of citations and quality indicators of methodology reporting (whether or not descriptive statistics, sample size or demographics were reported).\n\nWe calculated correlations separately for each year in our sample, because number of citations is confounded by time since publication: naturally, the older studies had a higher mean number of citations (2013: M = 18.88, SD = 19.62; 2014: M = 20.86, SD = 30.97; 2015: M = 8.47, SD = 9.35; 2016: M = 4.57, SD = 5.10). For the newer studies (2015\u201316), not enough time has passed to assess with any reliability how well they have been cited.",
    "is_useful": true,
    "question": "What statistical practices are recommended to improve the reliability of scientific studies and address issues related to false positives in publishing?"
  },
  {
    "text": "We performed pointbiserial correlations to explore possible associations between number of citations and statistical reporting practices (specifically, whether or not ES, CI and SE were reported) as well as the association between number of citations and quality indicators of methodology reporting (whether or not descriptive statistics, sample size or demographics were reported).\n\nWe calculated correlations separately for each year in our sample, because number of citations is confounded by time since publication: naturally, the older studies had a higher mean number of citations (2013: M = 18.88, SD = 19.62; 2014: M = 20.86, SD = 30.97; 2015: M = 8.47, SD = 9.35; 2016: M = 4.57, SD = 5.10). For the newer studies (2015\u201316), not enough time has passed to assess with any reliability how well they have been cited.\n\nOverall, our analysis indicates very little (if any) association between markers of reporting quality and citations: the majority of correlations were low and far from conventional statistical significance, and the few that reached significance were mostly negative (note that absence of a marker was always coded as'0', and its presence as'1', so a negative correlation suggests more citations when a marker was absent; this was the case in 2013 for reporting of demographics, and in 2014 for reporting effect size and design). Furthermore, none of the correlations replicated across years, and (considering the number of correlation analyses run) none would survive a correction for multiple comparisons (Table 3).",
    "is_useful": true,
    "question": "What is the relationship between statistical reporting practices and the number of citations in scientific literature according to recent analyses?"
  },
  {
    "text": "For the newer studies (2015\u201316), not enough time has passed to assess with any reliability how well they have been cited.\n\nOverall, our analysis indicates very little (if any) association between markers of reporting quality and citations: the majority of correlations were low and far from conventional statistical significance, and the few that reached significance were mostly negative (note that absence of a marker was always coded as'0', and its presence as'1', so a negative correlation suggests more citations when a marker was absent; this was the case in 2013 for reporting of demographics, and in 2014 for reporting effect size and design). Furthermore, none of the correlations replicated across years, and (considering the number of correlation analyses run) none would survive a correction for multiple comparisons (Table 3).\n\nThese findings indicate that there is no evidence that adopting better reporting practices is currently beneficial for authors in terms of getting cited. This may be due to a lack of awareness in the field: If authors are not aware of the need for such practices, they will not give adequate reporting appropriate weight in their assessment of a study's value and their decision on whether to cite it, potentially leading to overcitation of methodologically flawed studies and under-citation of sound ones.\n\nAuthors' Disciplinary Background and Variables of Reporting Quality Is having at least one non-philosopher among the authors associated with any of the reporting quality variables discussed above? We employed Chi-squares tests to explore the association between author composition (i.e., whether the author list included at least one non-philosopher vs all authors being philosophers) and various reporting measures.",
    "is_useful": true,
    "question": "What is the relationship between reporting quality and citation rates in scholarly research?"
  },
  {
    "text": "These findings indicate that there is no evidence that adopting better reporting practices is currently beneficial for authors in terms of getting cited. This may be due to a lack of awareness in the field: If authors are not aware of the need for such practices, they will not give adequate reporting appropriate weight in their assessment of a study's value and their decision on whether to cite it, potentially leading to overcitation of methodologically flawed studies and under-citation of sound ones.\n\nAuthors' Disciplinary Background and Variables of Reporting Quality Is having at least one non-philosopher among the authors associated with any of the reporting quality variables discussed above? We employed Chi-squares tests to explore the association between author composition (i.e., whether the author list included at least one non-philosopher vs all authors being philosophers) and various reporting measures. Reporting of CI was significantly associated with author composition (\u03c7 2 (1) = 8.79, p = .003, \u03c6 = .256); the distribution of frequencies among these two variables is shown on Table 4. Interestingly, the table shows that papers written without the help of nonphilosophers were more, not less, likely to report CIs. Neither of the remaining two tests for associations with reporting statistical measures yielded statistically significant findings (association with reporting ES: (\u03c7 2 (1) < 1, p = .352, \u03c6 = .080; association with reporting SE: (\u03c7 2 (1) = 1.22, p = .270, \u03c6 = .095).",
    "is_useful": true,
    "question": "How does the awareness of reporting practices among authors influence the citation of studies in open science?"
  },
  {
    "text": "We employed Chi-squares tests to explore the association between author composition (i.e., whether the author list included at least one non-philosopher vs all authors being philosophers) and various reporting measures. Reporting of CI was significantly associated with author composition (\u03c7 2 (1) = 8.79, p = .003, \u03c6 = .256); the distribution of frequencies among these two variables is shown on Table 4. Interestingly, the table shows that papers written without the help of nonphilosophers were more, not less, likely to report CIs. Neither of the remaining two tests for associations with reporting statistical measures yielded statistically significant findings (association with reporting ES: (\u03c7 2 (1) < 1, p = .352, \u03c6 = .080; association with reporting SE: (\u03c7 2 (1) = 1.22, p = .270, \u03c6 = .095).\n\nHaving at least one non-philosopher among the authors was also not associated with whether or not papers explicitly reported their design (\u03c7 2 (1) < 1, p = .558, \u03c6 = .051), or demographics (\u03c7 2 (1) < 1, p = .544, \u03c6 = .052).\n\nTesting Associations among Reporting-Quality Variables We also explored possible associations between variables indexing statistical reporting-quality (reporting of ES and CI) and methods-reporting quality (reporting of design and demographics).",
    "is_useful": true,
    "question": "How does author composition relate to the reporting of confidence intervals in research papers?"
  },
  {
    "text": "Testing Associations among Reporting-Quality Variables We also explored possible associations between variables indexing statistical reporting-quality (reporting of ES and CI) and methods-reporting quality (reporting of design and demographics). A set of chi-square tests revealed that papers reporting effect sizes were also significantly more likely to report design details (\u03c7 2 (1) = 13.16, p < .001, \u03c6 = .313) and demographics (\u03c7 2 (1) = 19.20, p < .001, \u03c6 = .378; Tables 5 and 6 respectively). We found no significant association between reporting CI and design details (\u03c7 2 (1) < 1, p = .368, \u03c6 = .078) or demographics (\u03c7 2 (1) < 1, p = .470, \u03c6 = .062).\n\nFinally, use of M-Turk was significantly associated with reporting exclusion of participants (\u03c7 2 (1) = 6.06, p = .014, \u03c6 = .213, Table 7). We expand on the significance of this finding in the Discussion.\n\n## 4 Discussion\n\nOur analyses examined the reporting practices employed in a large sample of empirical studies published in leading philosophy journals over a recent 4-year period.",
    "is_useful": true,
    "question": "What associations were found between statistical reporting quality and methods reporting quality in empirical studies published in philosophy journals?"
  },
  {
    "text": "018 | 0.915 | \u22120.340 | 0.308 |\n| Citations | \u2013 | Confidence Intervals (CI) | \u22120.044 | 0.797 | \u22120.363 | 0.284 |\n\nTable 3 Point-biserial correlations (rpb) between number of citations and methods information, descriptive statistics and inferential statistics by year\n\n*p < .05 (2-tailed)\n\n** Bonferroni adj. \u03b1 = .05/28, p < .0018; note that none of the correlations that reach conventional significance survive this correction\n\na Variance = 0 for reporting of Sample size\n\nb Variance = 0 for reporting of Central tendency/Frequency\n\n|  | CI |  |  |\n| --- | --- | --- | --- |\n| At least one non-philosopher? | Not reported | Reported | Total |\n| NO | 36 | 25 | 61 |\n| YES | 60 | 13 | 73 |\n| Total | 96 | 38 | 134 |\n\nTable 4 Frequencies for author composition by Confidence Intervals (CI) reporting\n\nthat NHST (in the form of reporting p-values) is overwhelmingly the dominant statistical analysis approach. During the period we examined, the older field of experimental psychology has gradually acknowledged the shortcomings of overreliance on p-values as a sole marker of findings' meaningfulness, and reporting complementary measures such as effect sizes and confidence intervals has become common.",
    "is_useful": true,
    "question": "What trends have emerged in the statistical analysis approaches in experimental psychology regarding the use of p-values and complementary measures?"
  },
  {
    "text": "| Not reported | Reported | Total |\n| NO | 36 | 25 | 61 |\n| YES | 60 | 13 | 73 |\n| Total | 96 | 38 | 134 |\n\nTable 4 Frequencies for author composition by Confidence Intervals (CI) reporting\n\nthat NHST (in the form of reporting p-values) is overwhelmingly the dominant statistical analysis approach. During the period we examined, the older field of experimental psychology has gradually acknowledged the shortcomings of overreliance on p-values as a sole marker of findings' meaningfulness, and reporting complementary measures such as effect sizes and confidence intervals has become common. In Experimental Philosophy, however, this is not yet the norm: only half of the papers we examined reported measures of effect size, and still fewer reported confidence intervals. (Admittedly, confidence intervals have a one-to-one relation with p-values, but they are widely viewed as being more straightforward to interpret).\n\nFurthermore, it is now accepted in the fields of experimental psychology and cognitive neuroscience that underpowered studies have, in the past, led to an over-representation of false positives in the published record; this has led to a recent emphasis on using prospective power analysis, when possible, to pre-determine sample sizes; to a lesser extent, reporting of observed power has also increased. We find no evidence of this trend in the Experimental Philosophy literature: among the studies we assessed, a very small number made any reference at all to statistical power. Finally, very few studies employed more sophisticated statistical approaches, such as Bayes factor.",
    "is_useful": true,
    "question": "What is the current trend in the reporting of statistical analyses and confidence intervals in experimental psychology compared to experimental philosophy?"
  },
  {
    "text": "In Experimental Philosophy, however, this is not yet the norm: only half of the papers we examined reported measures of effect size, and still fewer reported confidence intervals. (Admittedly, confidence intervals have a one-to-one relation with p-values, but they are widely viewed as being more straightforward to interpret).\n\nFurthermore, it is now accepted in the fields of experimental psychology and cognitive neuroscience that underpowered studies have, in the past, led to an over-representation of false positives in the published record; this has led to a recent emphasis on using prospective power analysis, when possible, to pre-determine sample sizes; to a lesser extent, reporting of observed power has also increased. We find no evidence of this trend in the Experimental Philosophy literature: among the studies we assessed, a very small number made any reference at all to statistical power. Finally, very few studies employed more sophisticated statistical approaches, such as Bayes factor.\n\nThe results reported here suggest that to date, Experimental Philosophy has adopted analytical and reporting practices that are closer to those that dominated psychology and cognitive neuroscience before the re-examination prompted by recent concerns about a replication crisis (Button et al. 2013; Open Science Collaboration 2012, 2015). In our Introduction, we reviewed surveys of the Psychology literature that spanned the years 1996 to 2013. We showed that reporting of effect sizes, for example, has increased from 26% of the articles sampled in 1996\u20132000 (Matthews et al.",
    "is_useful": true,
    "question": "What are some statistical practices in Experimental Philosophy that differ from those in experimental psychology and cognitive neuroscience?"
  },
  {
    "text": "We find no evidence of this trend in the Experimental Philosophy literature: among the studies we assessed, a very small number made any reference at all to statistical power. Finally, very few studies employed more sophisticated statistical approaches, such as Bayes factor.\n\nThe results reported here suggest that to date, Experimental Philosophy has adopted analytical and reporting practices that are closer to those that dominated psychology and cognitive neuroscience before the re-examination prompted by recent concerns about a replication crisis (Button et al. 2013; Open Science Collaboration 2012, 2015). In our Introduction, we reviewed surveys of the Psychology literature that spanned the years 1996 to 2013. We showed that reporting of effect sizes, for example, has increased from 26% of the articles sampled in 1996\u20132000 (Matthews et al. 2008) to over 90% in a survey of articles published in Canadian psychology journals in 2013 (Counsell and Harlow 2017).",
    "is_useful": true,
    "question": "What does the analysis of Experimental Philosophy literature reveal about its analytical and reporting practices compared to the trends observed in psychology and cognitive neuroscience before the replication crisis?"
  },
  {
    "text": "Finally, very few studies employed more sophisticated statistical approaches, such as Bayes factor.\n\nThe results reported here suggest that to date, Experimental Philosophy has adopted analytical and reporting practices that are closer to those that dominated psychology and cognitive neuroscience before the re-examination prompted by recent concerns about a replication crisis (Button et al. 2013; Open Science Collaboration 2012, 2015). In our Introduction, we reviewed surveys of the Psychology literature that spanned the years 1996 to 2013. We showed that reporting of effect sizes, for example, has increased from 26% of the articles sampled in 1996\u20132000 (Matthews et al. 2008) to over 90% in a survey of articles published in Canadian psychology journals in 2013 (Counsell and Harlow 2017). The turning point seems to be after 2010, as a survey of papers from 2009 to 2010 still found effect sizes were reported in only about 40% of\n\n|  | Design |  |  |\n| --- | --- | --- | --- |\n| ES | Not reported | Reported | Total |\n| Not reported | 30 | 33 | 63 |\n| Reported | 13 | 58 | 71 |\n| Total | 43 | 91 | 134 |\n\nTable 5 Frequencies for effect size (ES) by reporting of design\n\n|  | Demographics |  |  |\n| --- | --- | --- | --- |\n| ES | Not reported | Reported | Total |\n| Not reported | 27 | 36 | 63 |\n| Reported | 7 | 64 | 71 |\n| Total | 34 | 100 | 134 |\n\nTable 6 Frequencies for effect size (ES) by reporting of demographics\n\nstudies (Fritz et al.",
    "is_useful": true,
    "question": "What trends in reporting practices have been observed in Experimental Philosophy related to effect sizes over time?"
  },
  {
    "text": "2012); and a large-scale analysis of survey articles (Fritz et al. 2013) examining articles published in psychology journals between 1990 and 2010 found only 3% reported power analysis, 10% reported confidence intervals, and 38% reported effect sizes (although an upward trend across this period was noted for effect sizes). This has changed in recent years (though the process is still ongoing): Tressoldi et al. (2013), found that effect sizes and confidence intervals were reported in a majority of articles published in 2011 in both high and low impact journals (with the notable\u2014and lamentable\u2014exception of the highest-impact venues, Nature and Science), in some journals reaching 90% - the figure also found by Counsell and Harlow (2017). In light of this, our findings that only 53% of Experimental Philosophy articles in our sample reported effect sizes, and only 28% provided confidence intervals, suggest that statistical reporting practices in Experimental Philosophy seem to be lagging a few years behind those of comparable fields.\n\nThe studies we examined almost always provided information about sample size. Other important information about sample demographics and study design was less commonly (though frequently) reported. However, fewer than half of the studies directly referred to the number of participants that had been excluded from analysis. It is possible, of course, that the low proportion of reported exclusions is due to a low rate of exclusions in the studies themselves, and that all authors who excluded participants also reported this explicitly.",
    "is_useful": true,
    "question": "What trends have been observed in the reporting of statistical measures such as effect sizes and confidence intervals in scientific articles over time?"
  },
  {
    "text": "In light of this, our findings that only 53% of Experimental Philosophy articles in our sample reported effect sizes, and only 28% provided confidence intervals, suggest that statistical reporting practices in Experimental Philosophy seem to be lagging a few years behind those of comparable fields.\n\nThe studies we examined almost always provided information about sample size. Other important information about sample demographics and study design was less commonly (though frequently) reported. However, fewer than half of the studies directly referred to the number of participants that had been excluded from analysis. It is possible, of course, that the low proportion of reported exclusions is due to a low rate of exclusions in the studies themselves, and that all authors who excluded participants also reported this explicitly. However, it is noteworthy that participant exclusion is a highly-common practice in psychology and related fields; although there are often good justifications for doing so (e.g., when participants fail to engage with the task, are unable to perform it adequately, or have clear response biases), the practice has also been highlighted as an element of 'researcher degrees of freedom' (Simmons et al. 2011). Specifically, when exclusion criteria are not set a-priori (and reported as such), this leaves potential room for introduction of novel exclusion criteria after the results are known; this may, in turn, make it easier to obtain statistically-significant results\u2014\n\n|  | Participants excluded?",
    "is_useful": true,
    "question": "What concerns are raised about statistical reporting practices in Experimental Philosophy compared to comparable fields?"
  },
  {
    "text": "It is possible, of course, that the low proportion of reported exclusions is due to a low rate of exclusions in the studies themselves, and that all authors who excluded participants also reported this explicitly. However, it is noteworthy that participant exclusion is a highly-common practice in psychology and related fields; although there are often good justifications for doing so (e.g., when participants fail to engage with the task, are unable to perform it adequately, or have clear response biases), the practice has also been highlighted as an element of 'researcher degrees of freedom' (Simmons et al. 2011). Specifically, when exclusion criteria are not set a-priori (and reported as such), this leaves potential room for introduction of novel exclusion criteria after the results are known; this may, in turn, make it easier to obtain statistically-significant results\u2014\n\n|  | Participants excluded? |  |  |\n| --- | --- | --- | --- |\n| M-Turk | No | Yes | Total |\n| No | 49 | 26 | 75 |\n| Yes | 26 | 33 | 59 |\n| Total | 75 | 59 | 134 |\n\nTable 7 Frequencies for use of M-Turk by exclusion of participants\n\nand due to the human susceptibility to cognitive biases, which even those who do research on such biases are not immune to (Simmons et al. 2011), the best researchers, armed with the best of intentions, may be unaware that they are using exclusion rules they would not have invoked before the data were known.",
    "is_useful": true,
    "question": "What are the implications of participant exclusion practices in psychological research? "
  },
  {
    "text": "2011), the best researchers, armed with the best of intentions, may be unaware that they are using exclusion rules they would not have invoked before the data were known.\n\nOur current sample gives reason to believe that participant exclusion may also be common in Experimental Philosophy, due to the large variety of criteria that have been applied when such exclusions were reported. On the one hand, as mentioned above, there are often perfectly valid reasons for excluding participants. On the other hand, however, the need to exclude a substantial number of participants (in some cases, over half) should be avoided as much as possible, to prevent concerns about researcherdegrees-of-freedom (Simmons et al. 2011) and statistical artefacts (Shanks 2017) as alternative explanations for reported findings. Several of the studies we surveyed excluded a large number of participants for failing basic comprehension tests or otherwise showing that they did not follow task requirements: For example, Wilkenfeld et al. (2016) tested 142 participants but also mention that a further 188 participants were excluded for failing to consent, failing to complete the experiment, or giving an incorrect response to one of the reading or comprehension questions; Horvath and Wiegmann (2016) excluded the data of 142 (out of 284) subjects who did not complete the survey or completed it in under 1 min; Berni\u016bnas and Dranseika (2016) excluded 52 of 300 participants for failing a comprehension task; and Roberts et al.",
    "is_useful": true,
    "question": "What are some concerns related to participant exclusion in research, particularly in Experimental Philosophy?"
  },
  {
    "text": "The flipside of exclusion criteria is very strict inclusion criteria: Holtzman (2013) reported that out of 1195 participants recruited through blogs and social networks and who had completed his survey, he focused only on 234 philosophers who held a PhD or DPhil in philosophy. There is nothing wrong with conducting research on populations with specific educational or professional backgrounds; but ideally, recruitment procedures should prevent the sample from consisting mostly of participants who do not belong to the relevant population.\n\nMost of the above examples are of studies that used online platforms for data collection. Although such platforms are incredibly useful, their use may also result in the recruitment of a high number of unsuitable participants or a low level of participant engagement, which can negatively impact the quality of the data collected. This attests to the difficulties involved in carrying out research online; however, such difficulties must be mitigated through rigorous recruitment procedures and the use of comprehensible tasks. Unless the measured variables are entirely independent of the exclusion criteria (a requirement that is very hard to verify), excessive post-hoc data selection even when completely justified in light of the study's goals\u2014can lead to results that are pure artefacts resulting from regression to the mean (Shanks 2017). Finally, many of the concerns raised by data exclusion can be assuaged by adhering to two simple recommendations: Pre-registering the study before it is run, including details of its proposed exclusion criteria and analysis plans; and reporting the effect of exclusions on the results after the study is concluded. We go into further detail on both of these recommendations below.",
    "is_useful": true,
    "question": "What strategies can be employed to enhance the integrity of research data collected from online platforms?"
  },
  {
    "text": "This attests to the difficulties involved in carrying out research online; however, such difficulties must be mitigated through rigorous recruitment procedures and the use of comprehensible tasks. Unless the measured variables are entirely independent of the exclusion criteria (a requirement that is very hard to verify), excessive post-hoc data selection even when completely justified in light of the study's goals\u2014can lead to results that are pure artefacts resulting from regression to the mean (Shanks 2017). Finally, many of the concerns raised by data exclusion can be assuaged by adhering to two simple recommendations: Pre-registering the study before it is run, including details of its proposed exclusion criteria and analysis plans; and reporting the effect of exclusions on the results after the study is concluded. We go into further detail on both of these recommendations below.\n\nThe sample of studies covered by our analysis is representative of the work being published in leading philosophy journals, but is obviously not entirely comprehensive: some Experimental Philosophy articles have not been included in our sample because they were published in journals such as Episteme, an outlet that was not listed in the two rankings considered in this study. Furthermore, the sample of journals considered here is rather heterogeneous: for example, some of the journals that are classed here as philosophical, such as Review of Philosophy and Psychology, are outlets intended to attract genuinely interdisciplinary research. It should also be noted that the classification of authors as philosophers and non-philosophers is at least somewhat arbitrary. We considered the affiliation at the time of publication (usually given in the published article) but this might not fully capture the researcher's educational background.",
    "is_useful": true,
    "question": "What practices can help mitigate the difficulties in conducting online research and address concerns about data exclusion in studies?"
  },
  {
    "text": "We also note that our coding strategy (a score of B0^ for the answer Bno^, and a score of B1^ for the answer Byes^) has a limited resolution, meaning that items which varied in their degree of completeness could still be given the same score. Importantly, however, this is likely to have resulted in a more positive picture of reporting practices than the actual reality: Any mention of a relevant variable (e.g., effect size) would lead to a paper being assigned a value of 1 for that variable, even if the report itself was partial or applied inconsistently (or even incorrectly, an issue we did not delve into); a value of 0 was only assigned if the paper did not mention the variable at all. This may have somewhat inflated the number of papers coded with a value of 1 for any given variable.\n\nOn the other hand, the keyword-based search deployed here may have also occasionally missed some papers which did, in fact, report on a particular variable. In particular, in examining the reporting of study design features, we assessed whether the study was presented as Bwithin subjects^, Bbetween subjects^, Brepeated measures^ or Bindependent groups^; however, even in psychological research these labels are not universally used in reports; it is often assumed that educated readers would be able to infer such design features from the description of the study.\n\nNotably, we focus here on the type of information reported, not on reporting or analysis errors. In the field of psychology, recent studies (Veldkamp et al. 2014; Nuijten et al.",
    "is_useful": true,
    "question": "What limitations can affect the accuracy of reporting practices in open science, particularly in the context of study design features?"
  },
  {
    "text": "This may have somewhat inflated the number of papers coded with a value of 1 for any given variable.\n\nOn the other hand, the keyword-based search deployed here may have also occasionally missed some papers which did, in fact, report on a particular variable. In particular, in examining the reporting of study design features, we assessed whether the study was presented as Bwithin subjects^, Bbetween subjects^, Brepeated measures^ or Bindependent groups^; however, even in psychological research these labels are not universally used in reports; it is often assumed that educated readers would be able to infer such design features from the description of the study.\n\nNotably, we focus here on the type of information reported, not on reporting or analysis errors. In the field of psychology, recent studies (Veldkamp et al. 2014; Nuijten et al. 2016) have focused instead on the prevalence of inconsistent p-values in top psychology journals by means of an automated procedure to retrieve and check errors in the reporting of statistical results. A recent application of this type of analysis to the field of Experimental Philosophy (Colombo et al. 2018) concludes that statistical inconsistencies are not more widespread in Experimental Philosophy than in psychology\u2014meaning that when experimental philosophers use NHST, they do not tend to make consistency errors any more than psychologists do.\n\nDespite its limitations, we believe our study of current practices for reporting the design and analysis of Experimental Philosophy research offers interesting and potentially important findings. Such investigations provide insight into what researchers are doing well and what could be done to improve research and reporting practices in future studies.",
    "is_useful": true,
    "question": "What are the potential benefits of studying current practices for reporting design and analysis in research?"
  },
  {
    "text": "In the field of psychology, recent studies (Veldkamp et al. 2014; Nuijten et al. 2016) have focused instead on the prevalence of inconsistent p-values in top psychology journals by means of an automated procedure to retrieve and check errors in the reporting of statistical results. A recent application of this type of analysis to the field of Experimental Philosophy (Colombo et al. 2018) concludes that statistical inconsistencies are not more widespread in Experimental Philosophy than in psychology\u2014meaning that when experimental philosophers use NHST, they do not tend to make consistency errors any more than psychologists do.\n\nDespite its limitations, we believe our study of current practices for reporting the design and analysis of Experimental Philosophy research offers interesting and potentially important findings. Such investigations provide insight into what researchers are doing well and what could be done to improve research and reporting practices in future studies. This complements direct assessments of replicability, such as the XPhi Replicability Project, a recent large-scale effort to reproduce central Experimental Philosophy findings (Cova et al. 2018 https://osf.io/dvkpr/), which has provided encouraging data about current levels of replication in the field. We should not be complacent, though: Ensuring continued replicability requires the consistent adoption of appropriate reporting practices. We therefore end this report with a set of recommendations for authors, editors and reviewers of Experimental Philosophy papers (see Fig. 1 for a summary infographic).\n\nWe start with a general recommendation for philosophers and academic philosophy departments.",
    "is_useful": true,
    "question": "What measures can be taken to improve research and reporting practices in psychology and Experimental Philosophy?"
  },
  {
    "text": "Despite its limitations, we believe our study of current practices for reporting the design and analysis of Experimental Philosophy research offers interesting and potentially important findings. Such investigations provide insight into what researchers are doing well and what could be done to improve research and reporting practices in future studies. This complements direct assessments of replicability, such as the XPhi Replicability Project, a recent large-scale effort to reproduce central Experimental Philosophy findings (Cova et al. 2018 https://osf.io/dvkpr/), which has provided encouraging data about current levels of replication in the field. We should not be complacent, though: Ensuring continued replicability requires the consistent adoption of appropriate reporting practices. We therefore end this report with a set of recommendations for authors, editors and reviewers of Experimental Philosophy papers (see Fig. 1 for a summary infographic).\n\nWe start with a general recommendation for philosophers and academic philosophy departments. A growing number of philosophers are carrying out empirical research, and an increasing number (in sub-fields such as philosophy of mind and philosophy of neuroscience and psychology) view empirical findings as directly relevant to their conceptual analysis. If this trend is to continue, it will become essential for philosophers to acquire statistical literacy as part of their education. Statistical analyses are the lens through which present-day science looks at empirical data. Therefore, an adequate understanding of statistics\u2014including current developments and controversies in relevant fields\u2014should not be outsourced to collaborators from other fields, but rather should become as integral to a philosopher's education as courses in logic currently are.\n\nAs for authors, editors and reviewers, we strongly endorse the recommendations of Simmons et al.",
    "is_useful": true,
    "question": "What practices are suggested to enhance the reporting and replicability of research in the field of Experimental Philosophy?"
  },
  {
    "text": "We therefore end this report with a set of recommendations for authors, editors and reviewers of Experimental Philosophy papers (see Fig. 1 for a summary infographic).\n\nWe start with a general recommendation for philosophers and academic philosophy departments. A growing number of philosophers are carrying out empirical research, and an increasing number (in sub-fields such as philosophy of mind and philosophy of neuroscience and psychology) view empirical findings as directly relevant to their conceptual analysis. If this trend is to continue, it will become essential for philosophers to acquire statistical literacy as part of their education. Statistical analyses are the lens through which present-day science looks at empirical data. Therefore, an adequate understanding of statistics\u2014including current developments and controversies in relevant fields\u2014should not be outsourced to collaborators from other fields, but rather should become as integral to a philosopher's education as courses in logic currently are.\n\nAs for authors, editors and reviewers, we strongly endorse the recommendations of Simmons et al. (2011), who made a list of suggestions aimed at reducing the number of false-positive publications by putting in place checks on experimenter degrees of freedom. These recommendations were aimed at researchers in psychology, but are equally applicable to any field in which statistics are used to analyze empirical data, and particularly to fields where those data are human behaviors, beliefs and attitudes. We\n\n![](_page_18_Figure_4.jpeg)\n\nFig. 1 Recommendations for authors, editors and reviewers of Experimental Philosophy studies. This list complements the recommendations that Simmons et al. (2011) made for Psychology.",
    "is_useful": true,
    "question": "What essential skill should philosophers acquire to effectively engage with empirical research and statistical analysis in modern science?"
  },
  {
    "text": "Therefore, an adequate understanding of statistics\u2014including current developments and controversies in relevant fields\u2014should not be outsourced to collaborators from other fields, but rather should become as integral to a philosopher's education as courses in logic currently are.\n\nAs for authors, editors and reviewers, we strongly endorse the recommendations of Simmons et al. (2011), who made a list of suggestions aimed at reducing the number of false-positive publications by putting in place checks on experimenter degrees of freedom. These recommendations were aimed at researchers in psychology, but are equally applicable to any field in which statistics are used to analyze empirical data, and particularly to fields where those data are human behaviors, beliefs and attitudes. We\n\n![](_page_18_Figure_4.jpeg)\n\nFig. 1 Recommendations for authors, editors and reviewers of Experimental Philosophy studies. This list complements the recommendations that Simmons et al. (2011) made for Psychology. We repeat two of their recommendations (marked with asterisks) but endorse all of their suggestions. The present recommendations build on practices that have been adopted in recent years in other empirical fields, but have yet to become the norm in Experimental Philosophy\n\nwill not repeat those recommendations here, but our recommendations below do include a couple of them that, in light of the present findings, seem to have particular relevance to Experimental Philosophy.\n\nFor example, it seems particularly necessary for authors in Experimental Philosophy to take heed of Simmons et al.'s (2011) recommendation that BIf observations are eliminated, authors must also report what the statistical results are if those observations are included^.",
    "is_useful": true,
    "question": "Why is it important for philosophers to have a solid understanding of statistics in the context of open science?"
  },
  {
    "text": "We\n\n![](_page_18_Figure_4.jpeg)\n\nFig. 1 Recommendations for authors, editors and reviewers of Experimental Philosophy studies. This list complements the recommendations that Simmons et al. (2011) made for Psychology. We repeat two of their recommendations (marked with asterisks) but endorse all of their suggestions. The present recommendations build on practices that have been adopted in recent years in other empirical fields, but have yet to become the norm in Experimental Philosophy\n\nwill not repeat those recommendations here, but our recommendations below do include a couple of them that, in light of the present findings, seem to have particular relevance to Experimental Philosophy.\n\nFor example, it seems particularly necessary for authors in Experimental Philosophy to take heed of Simmons et al.'s (2011) recommendation that BIf observations are eliminated, authors must also report what the statistical results are if those observations are included^. Further requirements also make sense in light of the large number of exclusions in some of the studies examined here (none of which report whether and to what extent application of exclusion or inclusion criteria affected the results): reports must commit to having defined the rules for exclusion prior to conducting any analysis (including the calculation of descriptive statistics), and must provide a clear rationale for such exclusions, to prevent ad-hoc removal of participants. Furthermore, to prevent undisclosed exclusions, papers should always explicitly report whether any participants were excluded or not.\n\nMore generally, transparency can be improved by adopting pre-registration.",
    "is_useful": true,
    "question": "What recommendations can enhance transparency and data integrity in Experimental Philosophy studies?"
  },
  {
    "text": "For example, it seems particularly necessary for authors in Experimental Philosophy to take heed of Simmons et al.'s (2011) recommendation that BIf observations are eliminated, authors must also report what the statistical results are if those observations are included^. Further requirements also make sense in light of the large number of exclusions in some of the studies examined here (none of which report whether and to what extent application of exclusion or inclusion criteria affected the results): reports must commit to having defined the rules for exclusion prior to conducting any analysis (including the calculation of descriptive statistics), and must provide a clear rationale for such exclusions, to prevent ad-hoc removal of participants. Furthermore, to prevent undisclosed exclusions, papers should always explicitly report whether any participants were excluded or not.\n\nMore generally, transparency can be improved by adopting pre-registration. There is increasing support across the sciences for the idea of pre-registering studies, with initiatives such as the Preregistration Challenge (http://cos.io/prereg) offering assistance and incentives to conduct pre-registered research, and journals such as Psychological Science awarding 'badges' to papers that employ various good practices, including pre-registration. Current pre-registration platforms (e.g., the Open Science Framework, http://osf.io/; and AsPredicted, http://AsPredicted.org/) allow registration to consist simply of the basic study design, although they also enable inclusion of a detailed pre-specification of the study's procedures, expected outcomes and plan for statistical analysis (including exclusion criteria).",
    "is_useful": true,
    "question": "What practices can improve transparency in research according to the recommendations for authors in the context of open science?"
  },
  {
    "text": "Furthermore, to prevent undisclosed exclusions, papers should always explicitly report whether any participants were excluded or not.\n\nMore generally, transparency can be improved by adopting pre-registration. There is increasing support across the sciences for the idea of pre-registering studies, with initiatives such as the Preregistration Challenge (http://cos.io/prereg) offering assistance and incentives to conduct pre-registered research, and journals such as Psychological Science awarding 'badges' to papers that employ various good practices, including pre-registration. Current pre-registration platforms (e.g., the Open Science Framework, http://osf.io/; and AsPredicted, http://AsPredicted.org/) allow registration to consist simply of the basic study design, although they also enable inclusion of a detailed pre-specification of the study's procedures, expected outcomes and plan for statistical analysis (including exclusion criteria). Importantly, pre-registering the analysis plan does not preclude analyses that were not originally considered, or further analyses on subsets of the data; rather, it enables a clear and transparent distinction between confirmatory (pre-registered) and exploratory analyses, with the acknowledgment that it is often the latter kind that leads to the most interesting follow-up research.\n\nWith regard to specific analysis techniques, NHST is the main approach to statistical analysis in Experimental Philosophy (and is still the norm in Experimental Psychology too). However, experimental philosophers should take heed of the recent move in psychology toward augmenting p-values with measures of effect size and increased use of confidence intervals.",
    "is_useful": true,
    "question": "How can transparency in research be improved in the context of open science?"
  },
  {
    "text": "Importantly, pre-registering the analysis plan does not preclude analyses that were not originally considered, or further analyses on subsets of the data; rather, it enables a clear and transparent distinction between confirmatory (pre-registered) and exploratory analyses, with the acknowledgment that it is often the latter kind that leads to the most interesting follow-up research.\n\nWith regard to specific analysis techniques, NHST is the main approach to statistical analysis in Experimental Philosophy (and is still the norm in Experimental Psychology too). However, experimental philosophers should take heed of the recent move in psychology toward augmenting p-values with measures of effect size and increased use of confidence intervals. In particular, a paper's discussion and interpretation of its findings should focus on effect sizes, as they are more informative than simply reporting whether a finding was statistically significant.\n\nThe use of other statistical approaches in place of NHST (e.g., Bayesian analysis) is also on the rise in psychology and other sciences, although the use of these approaches is still controversial: Simmons et al. (2011) oppose the adoption of Bayesian statistics as a way of addressing the shortcomings of p-values, noting that such analyses are prone to arbitrary assumptions (e.g., in the choice of prior probabilities) that, along with simply adding another set of tests to choose from, increase researcher degrees of freedom; several other authors (e.g., Dienes 2011, 2014; Kruschke 2013; Rouder et al. 2009), focus instead on the usefulness of Bayesian analyses for establishing whether the evidence supports the null hypothesis.",
    "is_useful": true,
    "question": "What is the importance of pre-registering an analysis plan in the context of scientific research?"
  },
  {
    "text": "In particular, a paper's discussion and interpretation of its findings should focus on effect sizes, as they are more informative than simply reporting whether a finding was statistically significant.\n\nThe use of other statistical approaches in place of NHST (e.g., Bayesian analysis) is also on the rise in psychology and other sciences, although the use of these approaches is still controversial: Simmons et al. (2011) oppose the adoption of Bayesian statistics as a way of addressing the shortcomings of p-values, noting that such analyses are prone to arbitrary assumptions (e.g., in the choice of prior probabilities) that, along with simply adding another set of tests to choose from, increase researcher degrees of freedom; several other authors (e.g., Dienes 2011, 2014; Kruschke 2013; Rouder et al. 2009), focus instead on the usefulness of Bayesian analyses for establishing whether the evidence supports the null hypothesis. Whatever the outcome of these debates, experimental philosophers should remain up to date on the current consensus regarding best practice.\n\nAuthors should also make sure they provide all the relevant information on both the methods and results. Although the vast majority of the studies we examined reported their sample size, a much smaller number reported sample demographics that would allow an assessment of their findings' generalizability. Furthermore, many studies were vague on design and procedure details that determine whether a reader who wanted to conduct an exact replication would be able to do so. To facilitate clear and comprehensive writing, journal editors should recognize that word limits can be a serious obstacle to proper reporting of methods and results.",
    "is_useful": true,
    "question": "What should authors prioritize in their reporting to enhance the transparency and replicability of their research findings?"
  },
  {
    "text": "2009), focus instead on the usefulness of Bayesian analyses for establishing whether the evidence supports the null hypothesis. Whatever the outcome of these debates, experimental philosophers should remain up to date on the current consensus regarding best practice.\n\nAuthors should also make sure they provide all the relevant information on both the methods and results. Although the vast majority of the studies we examined reported their sample size, a much smaller number reported sample demographics that would allow an assessment of their findings' generalizability. Furthermore, many studies were vague on design and procedure details that determine whether a reader who wanted to conduct an exact replication would be able to do so. To facilitate clear and comprehensive writing, journal editors should recognize that word limits can be a serious obstacle to proper reporting of methods and results. In light of this, journals such as Psychological Science have now made clear that BThe Method and Results sections of Research Articles do not count toward the total word count limit. The aim here is to allow authors to provide clear, complete, self-contained descriptions of their studies^ (Psychological Science 2018). We suggest that editors of Philosophy journals should also consider revising their guidelines and strive to allow for sufficient level of detail in reporting.\n\nPhilosophers are not as accustomed as psychologists are to using graphs to make their point, but Experimental Philosophy authors should present their findings graphically if visualization allows for readers to better see trends and patterns (Matejka and Fitzmaurice 2017). For example, although there is some controversy about the use of bar-graphs to display results (see Bar Bar Plots Project 20175 ; Pastore et al.",
    "is_useful": true,
    "question": "What practices can be adopted to enhance transparency and reproducibility in research reporting?"
  },
  {
    "text": "In light of this, journals such as Psychological Science have now made clear that BThe Method and Results sections of Research Articles do not count toward the total word count limit. The aim here is to allow authors to provide clear, complete, self-contained descriptions of their studies^ (Psychological Science 2018). We suggest that editors of Philosophy journals should also consider revising their guidelines and strive to allow for sufficient level of detail in reporting.\n\nPhilosophers are not as accustomed as psychologists are to using graphs to make their point, but Experimental Philosophy authors should present their findings graphically if visualization allows for readers to better see trends and patterns (Matejka and Fitzmaurice 2017). For example, although there is some controversy about the use of bar-graphs to display results (see Bar Bar Plots Project 20175 ; Pastore et al. 2017), there is a consensus that bar graphs showing means are uninterpretable without including error bars representing standard errors, standard deviations, or confidence intervals; when including error bars, the measure they represent should be clearly indicated.\n\nHowever, even when graphics are helpful, authors should always provide numerical values for descriptive statistics and effect sizes as well, so that the study can be included in future replication efforts, Bayesian analyses and meta-analyses. To avoid redundancy, numerical values that are represented in graphic depictions can be given in supplementary online information , which is allowed by most journals. In cases in which journals do not allow authors to use supplementary materials , editors and publishers should consider updating their editorial policies to allow for their use.",
    "is_useful": true,
    "question": "What changes could academic journals consider to improve clarity and detail in the reporting of research findings?"
  },
  {
    "text": "For example, although there is some controversy about the use of bar-graphs to display results (see Bar Bar Plots Project 20175 ; Pastore et al. 2017), there is a consensus that bar graphs showing means are uninterpretable without including error bars representing standard errors, standard deviations, or confidence intervals; when including error bars, the measure they represent should be clearly indicated.\n\nHowever, even when graphics are helpful, authors should always provide numerical values for descriptive statistics and effect sizes as well, so that the study can be included in future replication efforts, Bayesian analyses and meta-analyses. To avoid redundancy, numerical values that are represented in graphic depictions can be given in supplementary online information , which is allowed by most journals. In cases in which journals do not allow authors to use supplementary materials , editors and publishers should consider updating their editorial policies to allow for their use.\n\nFurther, it is the role of editors and reviewers to verify that appropriate reporting practices, including those detailed above, are adhered to. In particular, editors of philosophy journals that publish experimental papers should make it a habit to go outside their usual reviewer pool and seek reviewers with the relevant methodological and statistical expertise to evaluate the empirical aspects of the work.\n\nReviewers, for their part, should focus not only on the content of the findings but also make sure to address quality of reporting, verifying the clarity and completeness of empirical methods, and the use of statistical analyses that go further than simply reporting p-values. As recommended by Simmons et al.",
    "is_useful": true,
    "question": "What practices should be followed to ensure accurate and interpretable reporting of scientific results?"
  },
  {
    "text": "To avoid redundancy, numerical values that are represented in graphic depictions can be given in supplementary online information , which is allowed by most journals. In cases in which journals do not allow authors to use supplementary materials , editors and publishers should consider updating their editorial policies to allow for their use.\n\nFurther, it is the role of editors and reviewers to verify that appropriate reporting practices, including those detailed above, are adhered to. In particular, editors of philosophy journals that publish experimental papers should make it a habit to go outside their usual reviewer pool and seek reviewers with the relevant methodological and statistical expertise to evaluate the empirical aspects of the work.\n\nReviewers, for their part, should focus not only on the content of the findings but also make sure to address quality of reporting, verifying the clarity and completeness of empirical methods, and the use of statistical analyses that go further than simply reporting p-values. As recommended by Simmons et al. (2011), reviewers should also be tolerant of imperfections in the results\u2014empirical data are messy, and an unrealistic expectation for perfectly neat stories is a strong incentive for researchers to apply socalled 'researcher degrees of freedom'. Although we have no evidence that unrealistic demands are a particular problem amongst reviewers of Experimental Philosophy studies, we do note that real data often lend themselves less comfortably to the kind of air-tight conceptual arguments that philosophers are more accustomed to.\n\nThe rapid recent growth of Experimental Philosophy suggests exciting prospects for informing philosophical arguments using empirical data.",
    "is_useful": true,
    "question": "What practices should editors and reviewers follow to ensure the quality of reporting in experimental research?"
  },
  {
    "text": "Reviewers, for their part, should focus not only on the content of the findings but also make sure to address quality of reporting, verifying the clarity and completeness of empirical methods, and the use of statistical analyses that go further than simply reporting p-values. As recommended by Simmons et al. (2011), reviewers should also be tolerant of imperfections in the results\u2014empirical data are messy, and an unrealistic expectation for perfectly neat stories is a strong incentive for researchers to apply socalled 'researcher degrees of freedom'. Although we have no evidence that unrealistic demands are a particular problem amongst reviewers of Experimental Philosophy studies, we do note that real data often lend themselves less comfortably to the kind of air-tight conceptual arguments that philosophers are more accustomed to.\n\nThe rapid recent growth of Experimental Philosophy suggests exciting prospects for informing philosophical arguments using empirical data. This burgeoning field must, however, insure itself against facing its own replication crisis in years to come by taking\n\n<sup>5</sup> https://www.kickstarter.com/projects/1474588473/barbarplots\n\nadvantage of insights reached, over the same recent period, by other fields; adopting bestpractice standards in analysis and reporting should go a long way towards this goal.\n\nAcknowledgements Andrea Polonioli acknowledges the support of the European Research Council under the ERC Consolidator Grant Agreement No. 616358 for a project called Pragmatic and Epistemic Role of Factually Erroneous Cognitions and Thoughts (PERFECT).",
    "is_useful": true,
    "question": "What practices should reviewers adopt to ensure the quality of reporting in empirical research?"
  },
  {
    "text": "Although we have no evidence that unrealistic demands are a particular problem amongst reviewers of Experimental Philosophy studies, we do note that real data often lend themselves less comfortably to the kind of air-tight conceptual arguments that philosophers are more accustomed to.\n\nThe rapid recent growth of Experimental Philosophy suggests exciting prospects for informing philosophical arguments using empirical data. This burgeoning field must, however, insure itself against facing its own replication crisis in years to come by taking\n\n<sup>5</sup> https://www.kickstarter.com/projects/1474588473/barbarplots\n\nadvantage of insights reached, over the same recent period, by other fields; adopting bestpractice standards in analysis and reporting should go a long way towards this goal.\n\nAcknowledgements Andrea Polonioli acknowledges the support of the European Research Council under the ERC Consolidator Grant Agreement No. 616358 for a project called Pragmatic and Epistemic Role of Factually Erroneous Cognitions and Thoughts (PERFECT). Mariana Vega-Mendoza was supported by the AHRC Open World Research Initiative Grant BMultilingualism: Empowering Individuals, Transforming Societies (MEITS)^ AH/N004671/1. David Carmel was supported by the European Research Council (Co-I, ERC Advanced Grant XSPECT - DLV-692739, PI: Andy Clark). The authors are very grateful for the help of the editors and reviewers in shaping the final version of the article.",
    "is_useful": true,
    "question": "What steps can the field of Experimental Philosophy take to ensure it avoids facing a replication crisis in the future?"
  },
  {
    "text": "Meta-Psychology, 2021, vol 5, MP.2020.2625, https://doi.org/10.15626/MP.2020.2625 Article type: Tutorial Published under the CC-BY4.0 license\n\nOpen data: Not applicable Open materials: Not applicable Open and reproducible analysis: Not applicable Open reviews and editorial process: Yes Preregistration: No\n\nEdited by: Erin M. Buchanan Reviewed by: Williams, M. & Dunleavy, D. Analysis reproduced by: Not applicable All supplementary files can be accessed at the OSF project page: https://doi.org/10.17605/OSF.IO/U68Y7\n\n# Preregistration of secondary data analysis: A template and tutorial\n\nOlmo R. van den Akker Department of Methodology and Statistics, Tilburg University\n\nLorne Campbell Department of Psychology, University of Western Ontario\n\nRodica Ioana Damian Department of Psychology, University of Houston\n\nAndrew N. Hall Department of Psychology, Northwestern University\n\n> Elliott Kruse EGADE Business School, Tec de Monterrey\n\nStuart J. Ritchie Social, Genetic and Developmental Psychiatry Centre, King's College\n\nAnna E. van 't Veer Methodology and Statistics Unit, Institute of Psychology, Leiden University\n\nSara J. Weston Department of Psychology, University of Oregon\n\nWilliam J. Chopik Department of Psychology, Michigan State University\n\nPamela E. Davis-Kean Department of Psychology, University of Michigan\n\n> Jessica E. Kosie Department of Psychology, Princeton University\n\nJerome Olsen Department of Applied Psychology: Work, Education and Economy, University of Vienna Max Planck Institute for Research on Collective Goods\n\nK. D. Valentine Division of General Internal Medicine, Massachusetts General Hospital\n\nMarjan Bakker Department of Methodology and Statistics, Tilburg University\n\nPreregistration has been lauded as one of the solutions to the so-called 'crisis of confidence' in the social sciences and has therefore gained popularity in recent years.",
    "is_useful": true,
    "question": "What is a key practice that has gained popularity in recent years as a solution to the crisis of confidence in the social sciences?"
  },
  {
    "text": "However, the current guidelines for preregistration have been developed primarily for studies where new data will be collected. Yet, preregistering secondary data analyses--- where new analyses are proposed for existing data---is just as important, given that researchers' hypotheses and analyses may be biased by their prior knowledge of the data. The need for proper guidance in this area is especially desirable now that data is increasingly shared publicly. In this tutorial, we present a template specifically designed for the preregistration of secondary data analyses and provide comments and a worked example that may help with using the template effectively. Through this illustration, we show that completing such a template is feasible, helps limit researcher degrees of freedom, and may make researchers more deliberate in their data selection and analysis efforts.\n\nKeywords: preregistration, secondary data analysis\n\nPreregistration has been lauded as one of the key solutions to the replication crisis in the social sciences, mainly because it has the potential to prevent p-hacking by restricting researcher degrees of freedom, but also because it improves transparency and study planning, and can reduce publication bias. However, despite its growing popularity, preregistration is still in its infancy and preregistration practices are far from optimal (Claesen, Gomes, Tuerlinckx, & Vanpaemel, 2019; Veldkamp et al., 2018). Moreover, the current guidelines for preregistration are primarily relevant for studies in which new data will be collected.",
    "is_useful": true,
    "question": "What are the benefits of preregistration in research, particularly in the context of secondary data analyses?"
  },
  {
    "text": "Through this illustration, we show that completing such a template is feasible, helps limit researcher degrees of freedom, and may make researchers more deliberate in their data selection and analysis efforts.\n\nKeywords: preregistration, secondary data analysis\n\nPreregistration has been lauded as one of the key solutions to the replication crisis in the social sciences, mainly because it has the potential to prevent p-hacking by restricting researcher degrees of freedom, but also because it improves transparency and study planning, and can reduce publication bias. However, despite its growing popularity, preregistration is still in its infancy and preregistration practices are far from optimal (Claesen, Gomes, Tuerlinckx, & Vanpaemel, 2019; Veldkamp et al., 2018). Moreover, the current guidelines for preregistration are primarily relevant for studies in which new data will be collected. In this paper, we suggest that preregistration is also attainable when testing new hypotheses with pre-existing data and provide a tutorial on how to effectively preregister such secondary data analyses.\n\nSecondary data analysis involves the analysis of existing data to investigate research questions, often in addition to the main ones for which the data were originally gathered (Grady, Cummings, & Hulley, 2013). Analyzing these datasets comes with its own challenges (Cheng & Phillips, 2014; Smith et al., 2011).",
    "is_useful": true,
    "question": "How can preregistration contribute to improving transparency and study planning in research, specifically in the context of secondary data analysis?"
  },
  {
    "text": "However, despite its growing popularity, preregistration is still in its infancy and preregistration practices are far from optimal (Claesen, Gomes, Tuerlinckx, & Vanpaemel, 2019; Veldkamp et al., 2018). Moreover, the current guidelines for preregistration are primarily relevant for studies in which new data will be collected. In this paper, we suggest that preregistration is also attainable when testing new hypotheses with pre-existing data and provide a tutorial on how to effectively preregister such secondary data analyses.\n\nSecondary data analysis involves the analysis of existing data to investigate research questions, often in addition to the main ones for which the data were originally gathered (Grady, Cummings, & Hulley, 2013). Analyzing these datasets comes with its own challenges (Cheng & Phillips, 2014; Smith et al., 2011). For instance, common secondary datasets often include many different variables from many different respondents, sometimes measured at different points in time (e.g., the World Values Survey, Inglehart et al., 2014; the Wisconsin Longitudinal Study, Herd, Carr, & Roan, 2014). This provides ample opportunity for researchers to p-hack and increases the likelihood of obtaining spurious statistically significant results (Weston, Ritchie, Rohrer, & Przybylski, 2019).\n\nIn addition, because secondary data are often extensive and difficult to collect initially, researchers frequently analyze the same dataset multiple times to answer different research questions.",
    "is_useful": true,
    "question": "What are some challenges associated with conducting secondary data analysis in research?"
  },
  {
    "text": "Analyzing these datasets comes with its own challenges (Cheng & Phillips, 2014; Smith et al., 2011). For instance, common secondary datasets often include many different variables from many different respondents, sometimes measured at different points in time (e.g., the World Values Survey, Inglehart et al., 2014; the Wisconsin Longitudinal Study, Herd, Carr, & Roan, 2014). This provides ample opportunity for researchers to p-hack and increases the likelihood of obtaining spurious statistically significant results (Weston, Ritchie, Rohrer, & Przybylski, 2019).\n\nIn addition, because secondary data are often extensive and difficult to collect initially, researchers frequently analyze the same dataset multiple times to answer different research questions. Researchers are therefore not likely to come to a dataset with completely fresh eyes, and may have insight regarding associations between at least some of the variables in the dataset. Such prior knowledge may steer the researchers toward a hypothesis that they already know is in line with the data. This practice is called HARKing (Hypothesizing After Results Are Known; Kerr, 1998) and can lead to false positive results (Rubin, 2017).",
    "is_useful": true,
    "question": "What are some challenges researchers face when analyzing secondary datasets in open science?"
  },
  {
    "text": "This provides ample opportunity for researchers to p-hack and increases the likelihood of obtaining spurious statistically significant results (Weston, Ritchie, Rohrer, & Przybylski, 2019).\n\nIn addition, because secondary data are often extensive and difficult to collect initially, researchers frequently analyze the same dataset multiple times to answer different research questions. Researchers are therefore not likely to come to a dataset with completely fresh eyes, and may have insight regarding associations between at least some of the variables in the dataset. Such prior knowledge may steer the researchers toward a hypothesis that they already know is in line with the data. This practice is called HARKing (Hypothesizing After Results Are Known; Kerr, 1998) and can lead to false positive results (Rubin, 2017). If HARKing goes undisclosed, it is not possible for third parties to evaluate whether the statistical tests for the hypotheses are well founded, as statistical hypothesis tests (e.g., null hypothesis significance tests, NHST) are only valid when the hypotheses are drawn up a priori (Wagenmakers, Wetzels, Borsboom, Van der Maas, & Kievit, 2012; but see Devezer et al., 2020).\n\nBecause secondary data analyses are particularly sensitive to data-driven researcher decisions, preregistering them is especially important. Other options exist to increase error control and illustrate sensitivity to flexibility in data analysis, however.",
    "is_useful": true,
    "question": "What practices in data analysis can lead to false positive results and challenge the validity of statistical hypothesis tests?"
  },
  {
    "text": "Such prior knowledge may steer the researchers toward a hypothesis that they already know is in line with the data. This practice is called HARKing (Hypothesizing After Results Are Known; Kerr, 1998) and can lead to false positive results (Rubin, 2017). If HARKing goes undisclosed, it is not possible for third parties to evaluate whether the statistical tests for the hypotheses are well founded, as statistical hypothesis tests (e.g., null hypothesis significance tests, NHST) are only valid when the hypotheses are drawn up a priori (Wagenmakers, Wetzels, Borsboom, Van der Maas, & Kievit, 2012; but see Devezer et al., 2020).\n\nBecause secondary data analyses are particularly sensitive to data-driven researcher decisions, preregistering them is especially important. Other options exist to increase error control and illustrate sensitivity to flexibility in data analysis, however. For example, a multiverse analysis (Steegen, Tuerlinckx, Gelman, & Vanpaemel, 2016) or specification-curve analysis (Simonsohn, Simmons, & Nelson, 2015) would be useful if researchers are unsure about which specific analysis is most suitable to test their hypothesis. In these approaches, all plausible analytic specifications are implemented to get an overall picture of the evidence without the need to choose a specific (and potentially biased) statistical analysis. This makes it impossible for researchers to cherry-pick variables or analyses based on their prior knowledge.",
    "is_useful": true,
    "question": "What practices can help ensure the validity of statistical tests in open science research?"
  },
  {
    "text": "Because secondary data analyses are particularly sensitive to data-driven researcher decisions, preregistering them is especially important. Other options exist to increase error control and illustrate sensitivity to flexibility in data analysis, however. For example, a multiverse analysis (Steegen, Tuerlinckx, Gelman, & Vanpaemel, 2016) or specification-curve analysis (Simonsohn, Simmons, & Nelson, 2015) would be useful if researchers are unsure about which specific analysis is most suitable to test their hypothesis. In these approaches, all plausible analytic specifications are implemented to get an overall picture of the evidence without the need to choose a specific (and potentially biased) statistical analysis. This makes it impossible for researchers to cherry-pick variables or analyses based on their prior knowledge. However, it would still be possible to cherry-pick the range of analyses, and it is difficult to weight the results from the different analyses in an unbiased manner. It would thus be appropriate to complement these methods with a preregistration, especially when the aim is to limit the potential for p-hacking and HARKing, for both primary and secondary data analysis.\n\nTo facilitate the preregistration of secondary data analyses, a session was organized at the Society for the Improvement of Psychological Science (SIPS, see https://improvingpsych.org) conference in 2018 with the aim of creating an expert-generated preregistration template specifically tailored to secondary data analysis.",
    "is_useful": true,
    "question": "What methods can researchers use to enhance error control and address flexibility in data analysis during secondary data analyses?"
  },
  {
    "text": "In these approaches, all plausible analytic specifications are implemented to get an overall picture of the evidence without the need to choose a specific (and potentially biased) statistical analysis. This makes it impossible for researchers to cherry-pick variables or analyses based on their prior knowledge. However, it would still be possible to cherry-pick the range of analyses, and it is difficult to weight the results from the different analyses in an unbiased manner. It would thus be appropriate to complement these methods with a preregistration, especially when the aim is to limit the potential for p-hacking and HARKing, for both primary and secondary data analysis.\n\nTo facilitate the preregistration of secondary data analyses, a session was organized at the Society for the Improvement of Psychological Science (SIPS, see https://improvingpsych.org) conference in 2018 with the aim of creating an expert-generated preregistration template specifically tailored to secondary data analysis. Providing guidance on how to preregister is vital as preregistration is hard and requires practice and effort to be effective (Nosek et al., 2019). Participants in the session were experts on or had experience with secondary data analysis, preregistration, or both, thereby providing a good mix of expertise for the task at hand. The session began with analyzing the standard OSF Preregistration template (Bowman et al., 2016) and through successive rounds of discussion and testing, participants decided whether items could be edited, omitted, or added to make the template suitable for secondary data analysis.",
    "is_useful": true,
    "question": "What approaches are recommended to enhance the integrity of secondary data analysis in open science? "
  },
  {
    "text": "To facilitate the preregistration of secondary data analyses, a session was organized at the Society for the Improvement of Psychological Science (SIPS, see https://improvingpsych.org) conference in 2018 with the aim of creating an expert-generated preregistration template specifically tailored to secondary data analysis. Providing guidance on how to preregister is vital as preregistration is hard and requires practice and effort to be effective (Nosek et al., 2019). Participants in the session were experts on or had experience with secondary data analysis, preregistration, or both, thereby providing a good mix of expertise for the task at hand. The session began with analyzing the standard OSF Preregistration template (Bowman et al., 2016) and through successive rounds of discussion and testing, participants decided whether items could be edited, omitted, or added to make the template suitable for secondary data analysis. The resulting first draft of the template was further improved in the months following the conference through a digital back and forth involving the preregistration of an actual secondary data analysis. These efforts---the generation of the template and the preregistration of an example analysis---culminated in the preregistration template presented here.\n\nSpecific templates like this can greatly facilitate preregistration as it gives the author guidance about what to include in the preregistration so that\n\n#### PREREGISTRATION OF SECONDARY DATA ANALYSIS: A TEMPLATE AND TUTORIAL\n\nall researcher degrees of freedom are covered (Veldkamp et al., 2018).",
    "is_useful": true,
    "question": "What is the purpose of creating specific templates for preregistration in secondary data analysis?"
  },
  {
    "text": "The session began with analyzing the standard OSF Preregistration template (Bowman et al., 2016) and through successive rounds of discussion and testing, participants decided whether items could be edited, omitted, or added to make the template suitable for secondary data analysis. The resulting first draft of the template was further improved in the months following the conference through a digital back and forth involving the preregistration of an actual secondary data analysis. These efforts---the generation of the template and the preregistration of an example analysis---culminated in the preregistration template presented here.\n\nSpecific templates like this can greatly facilitate preregistration as it gives the author guidance about what to include in the preregistration so that\n\n#### PREREGISTRATION OF SECONDARY DATA ANALYSIS: A TEMPLATE AND TUTORIAL\n\nall researcher degrees of freedom are covered (Veldkamp et al., 2018). As such, the template would also be well-suited as a framework for a registered report submission that focuses on secondary data. Some of the questions in the preregistration template for secondary data analysis are similar to the questions in more 'traditional' templates; others aim to solve the challenges unique to the preregistration of secondary data analysis, such as the increased need for transparency about the process leading up to the preregistration.\n\nThe template presented here is not the only preregistration template for secondary data analysis. Mertens and Krypotos (2019) simultaneously developed a template consisting of 10 questions based on the AsPredicted template (see https://aspredicted.org).",
    "is_useful": true,
    "question": "How can a specific template facilitate the preregistration process for secondary data analysis in open science?"
  },
  {
    "text": "As such, the template would also be well-suited as a framework for a registered report submission that focuses on secondary data. Some of the questions in the preregistration template for secondary data analysis are similar to the questions in more 'traditional' templates; others aim to solve the challenges unique to the preregistration of secondary data analysis, such as the increased need for transparency about the process leading up to the preregistration.\n\nThe template presented here is not the only preregistration template for secondary data analysis. Mertens and Krypotos (2019) simultaneously developed a template consisting of 10 questions based on the AsPredicted template (see https://aspredicted.org). Our template differs from that template in two ways. First, it involves 25 questions and therefore captures a wider array of researcher degrees of freedom. For example, our template includes specific questions about defining and handling outliers, and the specification of robustness checks, both of which give leeway for data-driven decisions in secondary data analyses (Weston et al., 2019). Moreover, a more comprehensive template gives researchers the option to use as many or as few of the questions as they want, in order to tailor their preregistration to specific study needs. Second, our template comes with elaborate comments and a worked example that we hope makes the preregistration of secondary data analysis more concrete. We think both these contributions are helpful to researchers looking to preregister their secondary data analysis.",
    "is_useful": true,
    "question": "What are key features of preregistration templates designed for secondary data analysis?"
  },
  {
    "text": "Our template differs from that template in two ways. First, it involves 25 questions and therefore captures a wider array of researcher degrees of freedom. For example, our template includes specific questions about defining and handling outliers, and the specification of robustness checks, both of which give leeway for data-driven decisions in secondary data analyses (Weston et al., 2019). Moreover, a more comprehensive template gives researchers the option to use as many or as few of the questions as they want, in order to tailor their preregistration to specific study needs. Second, our template comes with elaborate comments and a worked example that we hope makes the preregistration of secondary data analysis more concrete. We think both these contributions are helpful to researchers looking to preregister their secondary data analysis.\n\n# Using the template to preregister a secondary data analysis: Template questions and example answers, with guiding comments in italics\n\n#### Part 1: Study information\n\n## Question 1: Provide the working title of your study.\n\nDo religious people follow the golden rule? Assessing the link between religiosity and prosocial behavior using data from the Wisconsin Longitudinal Study.\n\nWe specifically mention the data set we are using so that readers know we are preregistering a secondary data analysis. Clarifying this from the outset is helpful because readers may look at such  preregistrations differently than they look at preregistrations of primary data analyses.\n\n### Question 2: Name the authors of this preregistration.",
    "is_useful": true,
    "question": "What are the advantages of using a comprehensive template for preregistering secondary data analyses?"
  },
  {
    "text": "Second, our template comes with elaborate comments and a worked example that we hope makes the preregistration of secondary data analysis more concrete. We think both these contributions are helpful to researchers looking to preregister their secondary data analysis.\n\n# Using the template to preregister a secondary data analysis: Template questions and example answers, with guiding comments in italics\n\n#### Part 1: Study information\n\n## Question 1: Provide the working title of your study.\n\nDo religious people follow the golden rule? Assessing the link between religiosity and prosocial behavior using data from the Wisconsin Longitudinal Study.\n\nWe specifically mention the data set we are using so that readers know we are preregistering a secondary data analysis. Clarifying this from the outset is helpful because readers may look at such  preregistrations differently than they look at preregistrations of primary data analyses.\n\n### Question 2: Name the authors of this preregistration.\n\nJosiah Carberry (JC) \u2013 ORCID iD: https://orcid.org/0000-0002-1825-0097 Pomona Sprout (PS) \u2013 Personal webpage:\n\nhttps://en.wikipedia.org/wiki/Hogwarts_staff#Pomona_Sprout\n\nWhen listing the authors, add an ORCID iD or a link to a personal webpage so that you and your co-authors can be easily identified. This is particularly important when preregistering secondary data analyses because you may have prior knowledge about the data that may influence the contents of the preregistration.",
    "is_useful": true,
    "question": "What are the benefits of using a template for preregistering secondary data analysis in research?"
  },
  {
    "text": "We specifically mention the data set we are using so that readers know we are preregistering a secondary data analysis. Clarifying this from the outset is helpful because readers may look at such  preregistrations differently than they look at preregistrations of primary data analyses.\n\n### Question 2: Name the authors of this preregistration.\n\nJosiah Carberry (JC) \u2013 ORCID iD: https://orcid.org/0000-0002-1825-0097 Pomona Sprout (PS) \u2013 Personal webpage:\n\nhttps://en.wikipedia.org/wiki/Hogwarts_staff#Pomona_Sprout\n\nWhen listing the authors, add an ORCID iD or a link to a personal webpage so that you and your co-authors can be easily identified. This is particularly important when preregistering secondary data analyses because you may have prior knowledge about the data that may influence the contents of the preregistration. If a reader has access to a personal profile that lists prior research, they can judge whether any prior knowledge of the data is plausible and whether it potentially biased the data analysis. That is, whether it introduced systematic error in the testing because researchers selected or encouraged one outcome or answer over others (Merriam-Webster, n.d.).\n\n### Question 3: List each research question included in this study.\n\nRQ1 = Are more religious people more prosocial than less religious people?\n\nRQ2 = Does the relationship between religiosity and prosociality differ for people with different religious affiliations?",
    "is_useful": true,
    "question": "Why is it important to mention the dataset being used when preregistering a secondary data analysis?"
  },
  {
    "text": "This is particularly important when preregistering secondary data analyses because you may have prior knowledge about the data that may influence the contents of the preregistration. If a reader has access to a personal profile that lists prior research, they can judge whether any prior knowledge of the data is plausible and whether it potentially biased the data analysis. That is, whether it introduced systematic error in the testing because researchers selected or encouraged one outcome or answer over others (Merriam-Webster, n.d.).\n\n### Question 3: List each research question included in this study.\n\nRQ1 = Are more religious people more prosocial than less religious people?\n\nRQ2 = Does the relationship between religiosity and prosociality differ for people with different religious affiliations?\n\nResearch questions are often used as a steppingstone for the development of specific and testable hypotheses and can therefore be phrased on a more conceptual level than hypotheses. Note that it is perfectly fine to skip the research questions and only preregister your hypotheses.\n\nQuestion 4: Please provide the hypotheses of your secondary data analysis. Make sure they are specific and testable, and make it clear what your statistical framework is (e.g., Bayesian inference, NHST). In case your hypothesis is directional, do not forget to state the direction. Please also provide a rationale for each hypothesis.\n\n\"Do to others as you would have them do to you\" (Luke 6:31). This golden rule is taught by all ma-\n\n#### VAN DEN AKKER ET AL.",
    "is_useful": true,
    "question": "Why is it important to consider prior knowledge when preregistering secondary data analyses in research?"
  },
  {
    "text": "RQ1 = Are more religious people more prosocial than less religious people?\n\nRQ2 = Does the relationship between religiosity and prosociality differ for people with different religious affiliations?\n\nResearch questions are often used as a steppingstone for the development of specific and testable hypotheses and can therefore be phrased on a more conceptual level than hypotheses. Note that it is perfectly fine to skip the research questions and only preregister your hypotheses.\n\nQuestion 4: Please provide the hypotheses of your secondary data analysis. Make sure they are specific and testable, and make it clear what your statistical framework is (e.g., Bayesian inference, NHST). In case your hypothesis is directional, do not forget to state the direction. Please also provide a rationale for each hypothesis.\n\n\"Do to others as you would have them do to you\" (Luke 6:31). This golden rule is taught by all ma-\n\n#### VAN DEN AKKER ET AL.\n\njor religions, in one way or another, to promote prosociality (Parliament of the World's Religions, 1993). Religious prosociality is the idea that religions facilitate behavior that is beneficial for others at a personal cost (Norenzayan & Shariff, 2008). The encouragement of prosocial behavior by religious teachings appears to be fruitful: a considerable amount of research shows that religion is positively related to prosocial behavior (e.g., Friedrichs, 1960; Koenig, McGue, Krueger, & Bouchard, 2007; Morgan, 1983).",
    "is_useful": true,
    "question": "What role does religiosity play in influencing prosocial behavior among individuals?"
  },
  {
    "text": "Just like in primary data analysis, a good hypothesis is specific (i.e., it includes a specific population), quantifiable, and testable. A one-sided hypothesis is suitable if theory, previous literature, or (scientific) reasoning indicates that your effect of interest is likely to be in a certain direction (e.g., A < B). Note that we provided detailed information about the theory and previous literature in our answer. This is crucial for secondary data analysis because it allows the reader to assess the thought process behind the hypotheses. Readers can then judge for themselves whether they think the hypotheses logically follow from the theory and previous literature or that they may have been tainted by the authors' prior knowledge of the data. Ideally, your preregistration already contains the framework for the introduction of the final paper. Moreover, writing up the introduction now instead of post hoc forces you to think clearly about the way you arrived at the hypotheses and may uncover flaws in your reasoning that can then be corrected before data collection begins.\n\nPart 2: Data description\n\nQuestion 5: Name and describe the dataset(s), and if applicable, the subset(s) of the data you plan to use. Useful information to include here is the type of data (e.g., cross-sectional or longitudinal), the general content of the questions, and some details about the respondents. In the case of longitudinal data, information about the survey's waves is useful as well.\n\nTo answer our research questions we will use a dataset from the Wisconsin Longitudinal Study (WLS; Herd, Carr, & Roan, 2014).",
    "is_useful": true,
    "question": "What characteristics should a good hypothesis possess in the context of secondary data analysis?"
  },
  {
    "text": "Ideally, your preregistration already contains the framework for the introduction of the final paper. Moreover, writing up the introduction now instead of post hoc forces you to think clearly about the way you arrived at the hypotheses and may uncover flaws in your reasoning that can then be corrected before data collection begins.\n\nPart 2: Data description\n\nQuestion 5: Name and describe the dataset(s), and if applicable, the subset(s) of the data you plan to use. Useful information to include here is the type of data (e.g., cross-sectional or longitudinal), the general content of the questions, and some details about the respondents. In the case of longitudinal data, information about the survey's waves is useful as well.\n\nTo answer our research questions we will use a dataset from the Wisconsin Longitudinal Study (WLS; Herd, Carr, & Roan, 2014). The WLS provides long-term data on a random sample of all the men and women who graduated from Wisconsin high schools in 1957. The WLS involves twelve waves of data. Six waves were collected from the original participants or their parents (1957, 1964, 1975, 1992, 2004, and 2011), four were collected from a selected sibling (1977, 1994, 2005, and 2011), one from the spouse of the original participant (2004), and one from the spouse of the selected sibling (2006). The questions vary across waves and are related to domains as diverse as socio-economic background, physical and mental health, and psychological makeup.",
    "is_useful": true,
    "question": "What are the benefits of preregistration and how does it influence the research process in open science?"
  },
  {
    "text": "To answer our research questions we will use a dataset from the Wisconsin Longitudinal Study (WLS; Herd, Carr, & Roan, 2014). The WLS provides long-term data on a random sample of all the men and women who graduated from Wisconsin high schools in 1957. The WLS involves twelve waves of data. Six waves were collected from the original participants or their parents (1957, 1964, 1975, 1992, 2004, and 2011), four were collected from a selected sibling (1977, 1994, 2005, and 2011), one from the spouse of the original participant (2004), and one from the spouse of the selected sibling (2006). The questions vary across waves and are related to domains as diverse as socio-economic background, physical and mental health, and psychological makeup. We will use the subset consisting of the 1957 graduates who completed the follow-up 2003- 2005 wave of the WLS dataset because it includes specific modules on religiosity and volunteering.\n\nLike the WLS data we use in our example, many large-scale datasets are outlined in detail in an accompanying paper. It is important to cite papers like this, but also to mention the most relevant information in the preregistration so that readers do not have to search for the information themselves. Sometimes information about the dataset is not readily available.",
    "is_useful": true,
    "question": "What are some challenges associated with utilizing large-scale datasets in research?"
  },
  {
    "text": "The questions vary across waves and are related to domains as diverse as socio-economic background, physical and mental health, and psychological makeup. We will use the subset consisting of the 1957 graduates who completed the follow-up 2003- 2005 wave of the WLS dataset because it includes specific modules on religiosity and volunteering.\n\nLike the WLS data we use in our example, many large-scale datasets are outlined in detail in an accompanying paper. It is important to cite papers like this, but also to mention the most relevant information in the preregistration so that readers do not have to search for the information themselves. Sometimes information about the dataset is not readily available. In those cases, be especially candid with the information you have about the dataset because the data you provide may be the only information about the data available to readers of the preregistration.\n\n# Question 6: Specify the extent to which the dataset is open or publicly available. Make note of any barriers to accessing the data, even if it is publicly available.\n\nThe dataset we will use is publicly available, but you need to formally agree to acknowledge the funding source for the Wisconsin Longitudinal Study, to cite the data release in any manuscripts,\n\n#### PREREGISTRATION OF SECONDARY DATA ANALYSIS: A TEMPLATE AND TUTORIAL\n\nworking papers, or published articles using these data, and to inform WLS about any published papers for use in the WLS bibliography and for reporting purposes. To do this you need to submit some information about yourself on the website: (https: //www.ssc.wisc.edu/wlsresearch/data/downloads).",
    "is_useful": true,
    "question": "What are the requirements for utilizing publicly available datasets in open science research?"
  },
  {
    "text": "In those cases, be especially candid with the information you have about the dataset because the data you provide may be the only information about the data available to readers of the preregistration.\n\n# Question 6: Specify the extent to which the dataset is open or publicly available. Make note of any barriers to accessing the data, even if it is publicly available.\n\nThe dataset we will use is publicly available, but you need to formally agree to acknowledge the funding source for the Wisconsin Longitudinal Study, to cite the data release in any manuscripts,\n\n#### PREREGISTRATION OF SECONDARY DATA ANALYSIS: A TEMPLATE AND TUTORIAL\n\nworking papers, or published articles using these data, and to inform WLS about any published papers for use in the WLS bibliography and for reporting purposes. To do this you need to submit some information about yourself on the website: (https: //www.ssc.wisc.edu/wlsresearch/data/downloads). You will then receive an email with a download link.\n\nIt is important to check whether the data is open or publicly available also to other researchers. For example, it could be that you have access via the organization providing the data (explain this in your answer to Q7), but that does not necessarily mean that it is publicly available to others. An example of publicly available data that is difficult to access would be data for which you need to register a profile on a website, or for which the owners of the data need to accept your request before you can have access.\n\n# Question 7: How can the data be accessed?",
    "is_useful": true,
    "question": "What considerations should researchers keep in mind regarding the accessibility and openness of a dataset before conducting a secondary data analysis?"
  },
  {
    "text": "To do this you need to submit some information about yourself on the website: (https: //www.ssc.wisc.edu/wlsresearch/data/downloads). You will then receive an email with a download link.\n\nIt is important to check whether the data is open or publicly available also to other researchers. For example, it could be that you have access via the organization providing the data (explain this in your answer to Q7), but that does not necessarily mean that it is publicly available to others. An example of publicly available data that is difficult to access would be data for which you need to register a profile on a website, or for which the owners of the data need to accept your request before you can have access.\n\n# Question 7: How can the data be accessed? Provide a persistent identifier or link if the data are available online or give a description of how you obtained the dataset.\n\nThe data can be accessed by going to the following link and searching for the variables that are specified in Q12 of this preregistration: https: //www.ssc.wisc.edu/wlsresearch/documentation/browse/?label=&variable=&wave_108=on& searchButton=Search\n\nWhen available, report the dataset's persistent identifier (e.g., a DOI) so that the data can always be retrieved from the Internet. In our example, we could only provide a link, but we added instructions for the reader to retrieve the data.",
    "is_useful": true,
    "question": "What are the important considerations for data accessibility in the context of open science?"
  },
  {
    "text": "An example of publicly available data that is difficult to access would be data for which you need to register a profile on a website, or for which the owners of the data need to accept your request before you can have access.\n\n# Question 7: How can the data be accessed? Provide a persistent identifier or link if the data are available online or give a description of how you obtained the dataset.\n\nThe data can be accessed by going to the following link and searching for the variables that are specified in Q12 of this preregistration: https: //www.ssc.wisc.edu/wlsresearch/documentation/browse/?label=&variable=&wave_108=on& searchButton=Search\n\nWhen available, report the dataset's persistent identifier (e.g., a DOI) so that the data can always be retrieved from the Internet. In our example, we could only provide a link, but we added instructions for the reader to retrieve the data. In general, try to bring the reader as close to the relevant data as possible, so instead of giving the link to the overarching website, give the link to the part of the website where the data can easily be located.\n\n### Question 8: Specify the date of download and/or access for each author.\n\nPS: Downloaded 12 February 2019; Accessed 12 February 2019.\n\nJC: Downloaded 3 January 2019 (estimated); Accessed 12 February 2019.\n\nWe will use the data accessed by JC on 12 February 2019 for our statistical analyses.",
    "is_useful": true,
    "question": "What challenges might researchers face when trying to access publicly available data?"
  },
  {
    "text": "In our example, we could only provide a link, but we added instructions for the reader to retrieve the data. In general, try to bring the reader as close to the relevant data as possible, so instead of giving the link to the overarching website, give the link to the part of the website where the data can easily be located.\n\n### Question 8: Specify the date of download and/or access for each author.\n\nPS: Downloaded 12 February 2019; Accessed 12 February 2019.\n\nJC: Downloaded 3 January 2019 (estimated); Accessed 12 February 2019.\n\nWe will use the data accessed by JC on 12 February 2019 for our statistical analyses.\n\nState here for each author when the dataset was initially downloaded (e.g., for previous analyses or merely to obtain the data) and when either metadata or the actual data (specify which) was first accessed (e.g., to identify variables of interest or to help fill out this form). Also, specify the author whose downloaded data you will use for the statistical analyses. This information is crucial in light of the reproducibility of your study because it is possible that the data has been edited since you last downloaded or accessed it. If you cannot retrieve when you downloaded or accessed the data, estimate those dates. In case you collected the data yourself to answer another research question, please state the date you first looked at the data. Finally, because not everybody will use the same date format it is important to state the date you downloaded or accessed the data unambiguously.",
    "is_useful": true,
    "question": "What practices are recommended for providing access to datasets in order to enhance reproducibility in research?"
  },
  {
    "text": "State here for each author when the dataset was initially downloaded (e.g., for previous analyses or merely to obtain the data) and when either metadata or the actual data (specify which) was first accessed (e.g., to identify variables of interest or to help fill out this form). Also, specify the author whose downloaded data you will use for the statistical analyses. This information is crucial in light of the reproducibility of your study because it is possible that the data has been edited since you last downloaded or accessed it. If you cannot retrieve when you downloaded or accessed the data, estimate those dates. In case you collected the data yourself to answer another research question, please state the date you first looked at the data. Finally, because not everybody will use the same date format it is important to state the date you downloaded or accessed the data unambiguously. For example, avoid dates like 12/02/2019 and instead use 12 February 2019 or December 2nd, 2019. \n\n# Question 9: If the data collection procedure is well documented, provide a link to that information. If the data collection procedure is not well documented, describe, to the best of your ability, how data were collected.\n\nThe WLS data was and is being collected by the University of Wisconsin Survey Center for use by the research community. The origins of the WLS can be traced back to a state-sponsored questionnaire administered during the spring of 1957 at all Wisconsin high school to students in their final year.",
    "is_useful": true,
    "question": "Why is it important to document the dates when datasets are downloaded or accessed in the context of reproducibility in research?"
  },
  {
    "text": "In case you collected the data yourself to answer another research question, please state the date you first looked at the data. Finally, because not everybody will use the same date format it is important to state the date you downloaded or accessed the data unambiguously. For example, avoid dates like 12/02/2019 and instead use 12 February 2019 or December 2nd, 2019. \n\n# Question 9: If the data collection procedure is well documented, provide a link to that information. If the data collection procedure is not well documented, describe, to the best of your ability, how data were collected.\n\nThe WLS data was and is being collected by the University of Wisconsin Survey Center for use by the research community. The origins of the WLS can be traced back to a state-sponsored questionnaire administered during the spring of 1957 at all Wisconsin high school to students in their final year. Therefore, the dataset constitutes a specific sample not necessarily representative of the United States as a whole. Most panel members were born in 1939, and the sample is broadly representative of white, non-Hispanic American men and women who completed at least a high school education. A flowchart for the data collection can be found here: https:// www.ssc.wisc.edu/wlsresearch/about/flowchart/ cor459d7.pdf\n\nWhile describing the data collection procedure, pay specific attention to the representativeness of the sample, and possible biases stemming from the data collection.",
    "is_useful": true,
    "question": "What is important to consider when documenting the data collection procedure in terms of date formats and sample representativeness?"
  },
  {
    "text": "The WLS data was and is being collected by the University of Wisconsin Survey Center for use by the research community. The origins of the WLS can be traced back to a state-sponsored questionnaire administered during the spring of 1957 at all Wisconsin high school to students in their final year. Therefore, the dataset constitutes a specific sample not necessarily representative of the United States as a whole. Most panel members were born in 1939, and the sample is broadly representative of white, non-Hispanic American men and women who completed at least a high school education. A flowchart for the data collection can be found here: https:// www.ssc.wisc.edu/wlsresearch/about/flowchart/ cor459d7.pdf\n\nWhile describing the data collection procedure, pay specific attention to the representativeness of the sample, and possible biases stemming from the data collection. For example, describe the population that was sampled from, whether the aim was to acquire a representative / regional / convenience sample, whether the data collectors were aware of this aim, the data collectors' recruitment efforts, the procedure for running participants,\n\n#### VAN DEN AKKER ET AL.\n\nwhether randomization was used, and whether participants were compensated for their time. All of this information can be used to judge whether the sample is representative of a wider population or whether the data is biased in some way, which crucially determines the conclusions that can be drawn from the results. In addition, thinking about the representativeness of a dataset is a crucial part of the planning stage of the research.",
    "is_useful": true,
    "question": "What factors should be considered to assess the representativeness and potential biases of a dataset collected for research purposes?"
  },
  {
    "text": "For example, describe the population that was sampled from, whether the aim was to acquire a representative / regional / convenience sample, whether the data collectors were aware of this aim, the data collectors' recruitment efforts, the procedure for running participants,\n\n#### VAN DEN AKKER ET AL.\n\nwhether randomization was used, and whether participants were compensated for their time. All of this information can be used to judge whether the sample is representative of a wider population or whether the data is biased in some way, which crucially determines the conclusions that can be drawn from the results. In addition, thinking about the representativeness of a dataset is a crucial part of the planning stage of the research. For example, you might come to the conclusion that the dataset at hand is not suitable after all and opt for a different dataset, thereby preventing research waste. Finally, it is good practice to describe what entity originally collected the data (e.g., your own lab, another lab, a multi-lab collaboration, a (national) survey collection organization, a private organization) because different data sources may have different purposes for collecting the data, which may also result in biased data.\n\nQuestion 10: Some studies offer codebooks to describe their data. If such a codebook is publicly available, link to it here or upload the document. If not, provide other available documentation. Also provide guidance on what parts of the codebook or other documentation are most relevant.\n\nThe codebook for the dataset we use can be found here: https://www.ssc.wisc.edu/wlsresearc h/documentation/waves/?wave=grad2k.",
    "is_useful": true,
    "question": "What factors should researchers consider to ensure that their sample is representative and to avoid potential biases in their data?"
  },
  {
    "text": "For example, you might come to the conclusion that the dataset at hand is not suitable after all and opt for a different dataset, thereby preventing research waste. Finally, it is good practice to describe what entity originally collected the data (e.g., your own lab, another lab, a multi-lab collaboration, a (national) survey collection organization, a private organization) because different data sources may have different purposes for collecting the data, which may also result in biased data.\n\nQuestion 10: Some studies offer codebooks to describe their data. If such a codebook is publicly available, link to it here or upload the document. If not, provide other available documentation. Also provide guidance on what parts of the codebook or other documentation are most relevant.\n\nThe codebook for the dataset we use can be found here: https://www.ssc.wisc.edu/wlsresearc h/documentation/waves/?wave=grad2k. We will mainly use questions from the mail survey about religion and spirituality, and the phone survey on volunteering, but will also use some questions from other modules (see the answer to Q12).\n\nAny documentation is welcome here, as readers will use this documentation to make sense of the dataset. If applicable, provide the codebook for the entire dataset but guide the reader to the relevant parts of the codebook so they do not have to search for the relevant parts extensively. Alternatively, you can create your own data dictionaries/codebooks (Arslan, 2019; Buchanan et al., 2019). If, for some reason codebook information cannot be shared publicly, provide an explanation.",
    "is_useful": true,
    "question": "What are best practices for ensuring the suitability and transparency of datasets in research?"
  },
  {
    "text": "Alternatively, you can create your own data dictionaries/codebooks (Arslan, 2019; Buchanan et al., 2019). If, for some reason codebook information cannot be shared publicly, provide an explanation. \n\n#### Part 3: Variables\n\nQuestion 11: If you are going to use any manipulated variables, identify them here. Describe the variables and the levels or treatment arms of each variable (note that this is not applicable for observational studies and meta-analyses). If you are collapsing groups across variables this should be explicitly stated, including the relevant formula. If your further analysis is contingent on a manipulation check, describe your decisions rules here.\n\n### Not applicable.\n\nManipulated variables in secondary datasets usually originate from another study investigating another research question. You may, therefore, need to adapt the manipulated variable to answer your own research question. For example, it may be necessary to relabel or even omit one of the treatment arms. Please provide a careful log of all these adaptations so that readers will have a clear grasp of the variable you will be using and how it differs from the variable in the original dataset. Any resources mentioned in the answer to Q10 may be useful here as well. \n\nQuestion 12: If you are going to use measured variables, identify them here. Describe both outcome measures as well as predictors and covariates and label them accordingly.",
    "is_useful": true,
    "question": "What guidelines should researchers follow when using manipulated and measured variables in their studies, particularly concerning data adaptation and transparency?"
  },
  {
    "text": "If you are collapsing groups across variables this should be explicitly stated, including the relevant formula. If your further analysis is contingent on a manipulation check, describe your decisions rules here.\n\n### Not applicable.\n\nManipulated variables in secondary datasets usually originate from another study investigating another research question. You may, therefore, need to adapt the manipulated variable to answer your own research question. For example, it may be necessary to relabel or even omit one of the treatment arms. Please provide a careful log of all these adaptations so that readers will have a clear grasp of the variable you will be using and how it differs from the variable in the original dataset. Any resources mentioned in the answer to Q10 may be useful here as well. \n\nQuestion 12: If you are going to use measured variables, identify them here. Describe both outcome measures as well as predictors and covariates and label them accordingly. If you are using a scale or an index, state the construct the scale/index represents, which items the scale/index will consist of, how these items will be aggregated, and whether this aggregation is based on a recommendation from the study codebook or validation research. When the aggregation of the items is based on exploratory factor analysis (EFA) or confirmatory factor analysis (CFA), also specify the relevant details (EFA: rotation, how the number of factors will be determined, how best fit will be selected, CFA: how loadings will be specified, how fit will be assessed, which residuals variance terms will be correlated). If you are using any categorical variables, state how you will code them in the statistical analyses.",
    "is_useful": true,
    "question": "What considerations should be made when adapting manipulated variables from secondary datasets for a new research question?"
  },
  {
    "text": "Question 12: If you are going to use measured variables, identify them here. Describe both outcome measures as well as predictors and covariates and label them accordingly. If you are using a scale or an index, state the construct the scale/index represents, which items the scale/index will consist of, how these items will be aggregated, and whether this aggregation is based on a recommendation from the study codebook or validation research. When the aggregation of the items is based on exploratory factor analysis (EFA) or confirmatory factor analysis (CFA), also specify the relevant details (EFA: rotation, how the number of factors will be determined, how best fit will be selected, CFA: how loadings will be specified, how fit will be assessed, which residuals variance terms will be correlated). If you are using any categorical variables, state how you will code them in the statistical analyses.\n\nReligiosity (IV): Religiosity is measured using a newly created scale with a subset of items from the Religion and Spirituality module of the 2004 mail survey (described here: https://www.ssc.wisc.edu /wlsresearch/documentation/waves/?wave=grad 2k&module=gmail_religion). The scale includes general questions about how religious/spiritual the individual is and how important religion/spirituality is to them. Importantly, the questions are not specific to a particular denomination and are on the same response scale. The specific variables are as follows:\n\n- 1. il001rer: How religious are you?",
    "is_useful": true,
    "question": "What factors should be considered when measuring variables in research, including outcome measures, predictors, and the aggregation of items in a scale?"
  },
  {
    "text": "If you are using any categorical variables, state how you will code them in the statistical analyses.\n\nReligiosity (IV): Religiosity is measured using a newly created scale with a subset of items from the Religion and Spirituality module of the 2004 mail survey (described here: https://www.ssc.wisc.edu /wlsresearch/documentation/waves/?wave=grad 2k&module=gmail_religion). The scale includes general questions about how religious/spiritual the individual is and how important religion/spirituality is to them. Importantly, the questions are not specific to a particular denomination and are on the same response scale. The specific variables are as follows:\n\n- 1. il001rer: How religious are you?\n- 2. il002rer: How spiritual are you?\n- 3. il003rer: How important is religion in your life?\n- 4. il004rer: How important is spirituality in your life?\n- 5. il005rer: How important was it, or would it have been if you had children, to send your children for religious or spiritual instruction?\n- 6. il006rer: How closely do you identify with being a member of a religious group?\n- 7. il007rer: How important is it for you to be with other people who are the same religion as you?\n- 8. il008rer: How important do you think it is for people of your religion to marry other people who are the same religion?\n- 9. il009rer: How strongly do you believe that one should stick to a particular faith?",
    "is_useful": true,
    "question": "How is religiosity measured in relation to statistical analyses involving categorical variables?"
  },
  {
    "text": "Be very specific when describing these characteristics so that readers with no knowledge of the data are able to redo your moves easily. For our WLS dataset, it is impossible to know the exact sample size without inspecting the data. If that is the case, provide an estimate of the sample size. If you provide an estimate, try to be conservative and pick the lowest sample size of the possible options. If it is impossible to provide an estimate, it is also possible to mask the data. For example, it is possible to add random noise to all values of the dependent variable. In that case, it is impossible to pick up any real effects and you are essentially blind to the data. Similarly, it is possible to blind yourself to real effects in the data by having someone relabel the treatment levels so you cannot link them to the treatment levels anymore. These\n\nand other methods of data blinding are clearly described by Dutilh, Sarafoglou, and Wagenmakers (2019).\n\nQuestion 14: What do you know about missing data in the dataset (i.e., overall missingness rate, information about differential dropout)? How will you deal with incomplete or missing data? Based on this information, provide a new expected sample size.\n\nThe WLS provides a documented set of missing codes. In Table 1 below you can find missingness information for every variable we will include in the statistical analyses. 'System missing' refers to the number of participants that did not or could not complete the questionnaire.",
    "is_useful": true,
    "question": "What practices should researchers consider when estimating sample sizes and dealing with data blinding in open science?"
  },
  {
    "text": "In that case, it is impossible to pick up any real effects and you are essentially blind to the data. Similarly, it is possible to blind yourself to real effects in the data by having someone relabel the treatment levels so you cannot link them to the treatment levels anymore. These\n\nand other methods of data blinding are clearly described by Dutilh, Sarafoglou, and Wagenmakers (2019).\n\nQuestion 14: What do you know about missing data in the dataset (i.e., overall missingness rate, information about differential dropout)? How will you deal with incomplete or missing data? Based on this information, provide a new expected sample size.\n\nThe WLS provides a documented set of missing codes. In Table 1 below you can find missingness information for every variable we will include in the statistical analyses. 'System missing' refers to the number of participants that did not or could not complete the questionnaire. 'Partial interview' refers to the number of participants that did not get that particular question because they were only partially interviewed. The rest of the codes are self-explanatory.\n\nImportantly, some respondents refused to answer the religiosity questions. These respondents apparently felt strongly about these questions, which could indicate that they are either very religious or very anti-religious. If that is the case, the respondent's propensity to respond is directly associated with their level of religiosity and that the data is missing not at random (MNAR). Because it is not possible to test the stringent assumptions of the modern techniques for handling MNAR data we will resort to simple listwise deletion.",
    "is_useful": true,
    "question": "What are some methods for managing missing data and their implications in research analysis?"
  },
  {
    "text": "Provide descriptive information, if available, on the amount of missing data for each variable you will use in the statistical analyses and discuss potential issues with the pattern of missing data for your planned analyses. Also provide a plan for how the analyses will take into account the presence of missing data. Where appropriate, provide specific details how this plan will be implemented. This can be done by specifying a step-by-step protocol for how you will impute any missing data. You could first explain how you will assess whether the data are missing at random (MAR) missing completely at random (MCAR) or missing not at random (MNAR), and then state that you will use technique X in case of MAR data, technique Y in case of MCAR data, and technique Z in case of MNAR data. For an overview of the types of missing data, and the different techniques to handle missing data, see Lang & Little (2018). Note that the missing data technique we used in our example, listwise deletion, is usually not the best way to handle missing data. We decided to use it in this example because it gave us the opportunity to illustrate how researchers can describe potential biases arising from their analysis methods in a preregistration.\n\nIf you cannot specify the exact number of missing data because the dataset does not provide that information, provide an estimate. If you provide an estimate, try to be conservative and pick the lowest sample size of the possible options. If it is impossible to provide an estimate, you could also mask the data (see Dutilh, Sarafoglou, & Wagenmakers, 2019).",
    "is_useful": true,
    "question": "What considerations should researchers take into account regarding missing data in their statistical analyses?"
  },
  {
    "text": "For an overview of the types of missing data, and the different techniques to handle missing data, see Lang & Little (2018). Note that the missing data technique we used in our example, listwise deletion, is usually not the best way to handle missing data. We decided to use it in this example because it gave us the opportunity to illustrate how researchers can describe potential biases arising from their analysis methods in a preregistration.\n\nIf you cannot specify the exact number of missing data because the dataset does not provide that information, provide an estimate. If you provide an estimate, try to be conservative and pick the lowest sample size of the possible options. If it is impossible to provide an estimate, you could also mask the data (see Dutilh, Sarafoglou, & Wagenmakers, 2019). It is good practice to state all missingness information with relation to the total sample size of the dataset.\n\n#### Table 1\n\n#### An overview of the missing values for all variables we will use in our analyses.\n\n| Variable | System missing | Don't know | Inappropriate | Refused | Not ascertained | Partial interview | Could not code | Remaining | Remaining (%) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| il001rer | 3,471 | 0 | 0 | 190 | 0 | 0 | 0 | 6,656 | 64 |\n| il002rer | 3,471 | 0 | 0 | 212 | 0 | 0 | 0 | 6,",
    "is_useful": true,
    "question": "What are the recommended practices for handling missing data in open science research?"
  },
  {
    "text": "To see how decisions about outliers can influence the results of a study, see Bakker and Wicherts (2014) and Lonsdorf et al. (2019). For more information about outliers in the context of preregistration, see Leys, Delacre, Mora, Lakens, and Ley (2019).\n\n# Question 16: Are there sampling weights available with this dataset? If so, are you using them or are you using your own sampling weights?\n\nThe WLS dataset does not include sampling weights and we will not use our own sampling weights as we do not seek to make any claims that are generalizable to the national population.\n\nBecause secondary data samples may not be entirely representative of the population you are interested in, it can be useful to incorporate sampling weights into your analysis. You should state here whether (and why) you will use sampling weights, and provide specifics on exactly how you will use them. To implement sampling weights into your analyses, we recommend using the \"survey\" package in R (Lumley, 2004).\n\n#### Part 4: Knowledge of data\n\nQuestion 17: List the publications, working papers (in preparation, unpublished, preprints), and conference presentations (talks, posters) you have worked on that are based on the dataset you will use. For each work, list the variables you analyzed, but limit yourself to variables that are relevant to the proposed analysis. If the dataset is longitudinal, also state which wave of the dataset you analyzed.\n\n Importantly, some of your team members may have used this dataset, and others may not have.",
    "is_useful": true,
    "question": "How can the decisions about outliers in a study affect the results and what is the relevance of preregistration in this context?"
  },
  {
    "text": "Because secondary data samples may not be entirely representative of the population you are interested in, it can be useful to incorporate sampling weights into your analysis. You should state here whether (and why) you will use sampling weights, and provide specifics on exactly how you will use them. To implement sampling weights into your analyses, we recommend using the \"survey\" package in R (Lumley, 2004).\n\n#### Part 4: Knowledge of data\n\nQuestion 17: List the publications, working papers (in preparation, unpublished, preprints), and conference presentations (talks, posters) you have worked on that are based on the dataset you will use. For each work, list the variables you analyzed, but limit yourself to variables that are relevant to the proposed analysis. If the dataset is longitudinal, also state which wave of the dataset you analyzed.\n\n Importantly, some of your team members may have used this dataset, and others may not have. It is therefore important to specify the previous works for every co-author separately. Also mention relevant work on this dataset by researchers you are affiliated with as their knowledge of the data may have been spilled over to you. When the provider of the data also has an overview of all the work that has been done using the dataset, link to that overview.\n\nBoth authors (PS and JC) have previously used the Graduates 2003-2005 wave to assess the link between Big Five personality traits and prosociality.",
    "is_useful": true,
    "question": "What considerations should be made when analyzing secondary data samples in open science, particularly regarding representativeness and sampling procedures?"
  },
  {
    "text": "For each work, list the variables you analyzed, but limit yourself to variables that are relevant to the proposed analysis. If the dataset is longitudinal, also state which wave of the dataset you analyzed.\n\n Importantly, some of your team members may have used this dataset, and others may not have. It is therefore important to specify the previous works for every co-author separately. Also mention relevant work on this dataset by researchers you are affiliated with as their knowledge of the data may have been spilled over to you. When the provider of the data also has an overview of all the work that has been done using the dataset, link to that overview.\n\nBoth authors (PS and JC) have previously used the Graduates 2003-2005 wave to assess the link between Big Five personality traits and prosociality. The variables we used to measure the Big Five personality traits were ih001rei (extraversion), ih009rei (agreeableness), ih017rei (conscientiousness), ih025rei (neuroticism), and ih032rei (openness). The variables we used to measure prosociality were ih013rer (\"To what extent do you agree that you see yourself as someone who is generally trusting?\"), ih015rer (\"To what extent do you agree that you see yourself as someone who is considerate to almost everyone?\"), and ih016rer (\"To what extent do you agree that you see yourself as someone who likes to cooperate with others?). We presented the results at the ARP conference in St. Louis in 2013 and we are currently finalizing a manuscript based on these results.",
    "is_useful": true,
    "question": "What are the key considerations for analyzing and presenting teamwork research in open science?"
  },
  {
    "text": "We presented the results at the ARP conference in St. Louis in 2013 and we are currently finalizing a manuscript based on these results.\n\nAdditionally, a senior graduate student in JC's lab used the Graduates 2011 wave for exploratory analyses on depression. She linked depression to alcohol use and general health indicators. She did not look at variables related to religiosity or prosociality. Her results have not yet been submitted anywhere.\n\nAn overview of all publications based on the WLS data can be found here: https://www.ssc.wisc. edu/wlsresearch/publications/pubs.php?topic= ALL.\n\nIt is important to specify the different ways you have previously used the data because this information helps you to establish any knowledge of the data you may already have. This prior knowledge will need to be provided in Q18. If available, include persistent identifiers (e.g., a DOI) to any relevant papers and presentations. \n\nUnderstandably, there is a subjectivity involved in determining what constitutes \"relevant\" work or \"relevant\" variables for the proposed analysis. We advise researchers to use their professional judgment and when in doubt always mention the work or variable so readers can assess their relevance themselves. In the worked example, the exploratory analysis by the student in JC's lab is probably not relevant, but because of the close affiliation of the student to JC, it is good to include it anyway.",
    "is_useful": true,
    "question": "What is the significance of acknowledging prior knowledge and previous analyses when working with research data in the context of open science?"
  },
  {
    "text": "edu/wlsresearch/publications/pubs.php?topic= ALL.\n\nIt is important to specify the different ways you have previously used the data because this information helps you to establish any knowledge of the data you may already have. This prior knowledge will need to be provided in Q18. If available, include persistent identifiers (e.g., a DOI) to any relevant papers and presentations. \n\nUnderstandably, there is a subjectivity involved in determining what constitutes \"relevant\" work or \"relevant\" variables for the proposed analysis. We advise researchers to use their professional judgment and when in doubt always mention the work or variable so readers can assess their relevance themselves. In the worked example, the exploratory analysis by the student in JC's lab is probably not relevant, but because of the close affiliation of the student to JC, it is good to include it anyway.\n\nListing previous works based on the data also helps to prevent a common practice identified by the American Psychological Association (2019) as unethical: the so-called \"least publishable unit\" practice (also known as \"salami-slicing\"), in which researchers publish multiple papers on closely related variables from the same dataset. Given that secondary datasets often involve many closely related variables, this is a particularly pernicious issue here.\n\nQuestion 18: What prior knowledge do you have about the dataset that may be relevant for the proposed analysis? Your prior knowledge could stem from working with the data first-hand, from reading previously published research, or from codebooks. Also provide any relevant knowledge of subsets of the data you will not be using. Provide prior knowledge for every author separately.",
    "is_useful": true,
    "question": "What is the importance of specifying previous uses of data in the context of research analysis?"
  },
  {
    "text": "In the worked example, the exploratory analysis by the student in JC's lab is probably not relevant, but because of the close affiliation of the student to JC, it is good to include it anyway.\n\nListing previous works based on the data also helps to prevent a common practice identified by the American Psychological Association (2019) as unethical: the so-called \"least publishable unit\" practice (also known as \"salami-slicing\"), in which researchers publish multiple papers on closely related variables from the same dataset. Given that secondary datasets often involve many closely related variables, this is a particularly pernicious issue here.\n\nQuestion 18: What prior knowledge do you have about the dataset that may be relevant for the proposed analysis? Your prior knowledge could stem from working with the data first-hand, from reading previously published research, or from codebooks. Also provide any relevant knowledge of subsets of the data you will not be using. Provide prior knowledge for every author separately.\n\nIn a previous study (mentioned in Q17) we used three prosociality variables (ih013rer, ih015rer, and ih016rer) that may be related to the prosociality variables we use in this study. We found that ih013rer, ih015rer, and ih016rer are positively associated with agreeableness (ih009rec). Because previous research (on other datasets) shows a positive association between agreeableness and religiosity (Saroglou, 2002) agreeableness may act as a confounding variable. To account for this we will include agreeableness in our analysis as a control variable.",
    "is_useful": true,
    "question": "What ethical considerations must researchers keep in mind when publishing findings based on secondary datasets to avoid unethical practices?"
  },
  {
    "text": "Your prior knowledge could stem from working with the data first-hand, from reading previously published research, or from codebooks. Also provide any relevant knowledge of subsets of the data you will not be using. Provide prior knowledge for every author separately.\n\nIn a previous study (mentioned in Q17) we used three prosociality variables (ih013rer, ih015rer, and ih016rer) that may be related to the prosociality variables we use in this study. We found that ih013rer, ih015rer, and ih016rer are positively associated with agreeableness (ih009rec). Because previous research (on other datasets) shows a positive association between agreeableness and religiosity (Saroglou, 2002) agreeableness may act as a confounding variable. To account for this we will include agreeableness in our analysis as a control variable. We did not find any associations between prosociality and the other Big Five variables.\n\nIt is important to denote your prior knowledge diligently because it provides information about possible biases in your statistical analysis decisions. For example, you may have learned at an academic conference or in a footnote of another paper that the correlation between two variables is high in this dataset. If you do a test of this hypothesis, you already know the test result, making the interpretation of the test invalid (Wagenmakers, et al., 2012). In cases like this, where you have direct knowledge about a hypothesized association, you should disregard doing a confirmatory analysis altogether or do one based on a different dataset.",
    "is_useful": true,
    "question": "How does prior knowledge influence the validity of statistical analysis decisions in research?"
  },
  {
    "text": "In cases like this, where you have direct knowledge about a hypothesized association, you should disregard doing a confirmatory analysis altogether or do one based on a different dataset.\n\nAny indirect knowledge about the hypothesized association does not preclude a confirmatory analysis but should be transparently reported in this section. In our example, we mentioned that we know about the positive association between agreeableness and prosociality, which may say something about the direction of our hypothesized association given the association between agreeableness and religiosity. Moreover, this prior knowledge urged us to add agreeableness as a control variable. Thus, aside from improving your preregistration, evaluating your prior knowledge of the data can also improve the analyses themselves. \n\nAll information like this that may influence the hypothesized association is relevant here. For example, restriction of range (Meade, 2010), measurement reliability (Silver, 2008), and the number of response options (Gradstein, 1986) have been shown to influence the association between two variables. You may have provided univariate information regarding these aspects in previous questions. In this section, you can write about how they may affect your hypothesized association.\n\nDo note that it is unlikely that you are able to account for all the effects of prior knowledge on your analytical decisions. For example, you may have prior knowledge that you are not consciously aware of. The best way to capture this unconscious prior knowledge is to revisit previous work, think deeply about any information that might be relevant for the\n\ncurrent project, and present it here to the best of your ability.",
    "is_useful": true,
    "question": "How can prior knowledge influence the analysis and interpretation of hypothesized associations in research?"
  },
  {
    "text": "All information like this that may influence the hypothesized association is relevant here. For example, restriction of range (Meade, 2010), measurement reliability (Silver, 2008), and the number of response options (Gradstein, 1986) have been shown to influence the association between two variables. You may have provided univariate information regarding these aspects in previous questions. In this section, you can write about how they may affect your hypothesized association.\n\nDo note that it is unlikely that you are able to account for all the effects of prior knowledge on your analytical decisions. For example, you may have prior knowledge that you are not consciously aware of. The best way to capture this unconscious prior knowledge is to revisit previous work, think deeply about any information that might be relevant for the\n\ncurrent project, and present it here to the best of your ability. This exercise helps you reflect on potential biases you may have and makes it possible for readers of the preregistration to assess whether the prior knowledge you mentioned is plausible given the list of prior work you provided in Q17.\n\nOf course, it is still possible that researchers purposefully neglect to mention prior knowledge or provide false information in a preregistration. Even though we believe that deliberate deceit like this is rare, at the end of our template we require researchers to formally \"promise\" to have truthfully filled out the template and that no other preregistration exists on the same hypotheses and data. A violation of this formal statement can be seen as misconduct, and we believe researchers are unlikely to cross that line.",
    "is_useful": true,
    "question": "What factors can influence the hypothesized association between variables in research, and how can researchers account for prior knowledge in their analytical decisions?"
  },
  {
    "text": "The best way to capture this unconscious prior knowledge is to revisit previous work, think deeply about any information that might be relevant for the\n\ncurrent project, and present it here to the best of your ability. This exercise helps you reflect on potential biases you may have and makes it possible for readers of the preregistration to assess whether the prior knowledge you mentioned is plausible given the list of prior work you provided in Q17.\n\nOf course, it is still possible that researchers purposefully neglect to mention prior knowledge or provide false information in a preregistration. Even though we believe that deliberate deceit like this is rare, at the end of our template we require researchers to formally \"promise\" to have truthfully filled out the template and that no other preregistration exists on the same hypotheses and data. A violation of this formal statement can be seen as misconduct, and we believe researchers are unlikely to cross that line. \n\n# Part 5: Analyses\n\nQuestion 19: For each hypothesis, describe the statistical model you will use to test the hypothesis. Include the type of model (e.g., ANOVA, multiple regression, SEM) and the specification of the model. Specify any interactions and post-hoc analyses and remember that any test not included here must be labeled as an exploratory test in the final paper.\n\nOur first hypothesis will be tested using three analyses since we use three variables to measure prosociality. For each, we will run a directional null hypothesis significance test to see whether a positive effect exists of religiosity on prosociality.",
    "is_useful": true,
    "question": "What is the importance of disclosing prior knowledge in research preregistration within the context of open science?"
  },
  {
    "text": "The code we will use for all these analyses can be found at https://osf.io/e3htr.\n\nThink carefully about the variety of statistical methods that are available for testing each of your hypotheses. One of the classic \"Questionable \n\nResearch Practices\" is trying multiple methods and only publishing the ones that \"work\" (i.e., that support your hypothesis). Almost every method has several options that may be more or less suited to the question you are asking. Therefore, it is crucial to specify a priori which one you are going to use and how.\n\nIf you can, include the code you will use to run your statistical analyses, as this forces you to think about your analyses in detail and makes it easy for readers to see exactly what you plan to do. Ideally, when you have loaded the data in a software program you only have to press one button to run your analyses. If including the code is impossible, describe the analyses such that you could give a positive answer to the question: \"Would a colleague who is not involved in this project be able to recreate this statistical analysis?\"\n\n# Question 20: If applicable, specify a predicted effect size or a minimum effect size of interest for all the effects tested in your statistical analyses.\n\nFor the logistic regression with 'Did the graduate do volunteer work in the last 12 months?' as the outcome variable, our minimum effect size of interest is an odds of 1.05.",
    "is_useful": true,
    "question": "What is the importance of specifying statistical methods and sharing code in research analyses?"
  },
  {
    "text": "If you can, include the code you will use to run your statistical analyses, as this forces you to think about your analyses in detail and makes it easy for readers to see exactly what you plan to do. Ideally, when you have loaded the data in a software program you only have to press one button to run your analyses. If including the code is impossible, describe the analyses such that you could give a positive answer to the question: \"Would a colleague who is not involved in this project be able to recreate this statistical analysis?\"\n\n# Question 20: If applicable, specify a predicted effect size or a minimum effect size of interest for all the effects tested in your statistical analyses.\n\nFor the logistic regression with 'Did the graduate do volunteer work in the last 12 months?' as the outcome variable, our minimum effect size of interest is an odds of 1.05. This means that a oneunit increase on the religiosity scale would be associated with a 1.05 factor change in odds of having done volunteering work in the last 12 months versus not having done so.\n\nFor the linear regressions with 'The number of graduate's volunteer activities in the last 12 months\", and \"How many hours did the graduate volunteer during a typical month in the last 12 months?' as the outcome variables, the minimum regression coefficients of interest of the religiosity variables are 0.05 and 0.5, respectively.",
    "is_useful": true,
    "question": "What is the importance of including code or detailed descriptions of statistical analyses in the context of open science?"
  },
  {
    "text": "Use the sample size after updating for missing data and outliers, and justify the assumptions and parameters used (e.g., give an explanation of why anything smaller than the smallest effect size of interest would be theoretically or practically unimportant).\n\nThe sample size after updating for missing data and outliers is 1,358 for the logistic regression with gv103re as the outcome variable, and 1,086 and 1,041 for the linear regressions with gv109re and gv111re as the outcome variables, respectively. For all three analyses this corresponds to a statistical power of approximately 1.00 when assuming our minimum effect sizes of interest. For the linear regressions we additionally assumed the variance explained by the predictor to be 0.2 and the residual variance to be 1.0 (see figure below for the full power analysis of the regression with the lowest sample size). For the logistic regression we assumed an intercept of -1.56 corresponding to a situation where half of the participants have done volunteer work in the last year (see the R-code for the full power analysis at https://osf.io/f96rn).\n\nAdvice on conducting a power analysis using G*Power can be found in Faul, Erdfelder, Buchner, and Lang (2009). Advice on conducting a power analysis using R can be found here: cran.r-project .org/web/packages/pwr/vignettes/pwr-vignette .html.",
    "is_useful": true,
    "question": "What considerations should be made when determining sample size and conducting power analysis in research? "
  },
  {
    "text": "Advice on conducting a power analysis using R can be found here: cran.r-project .org/web/packages/pwr/vignettes/pwr-vignette .html. \n\nNote that power analyses for secondary data analyses are unlike power analyses for primary data analyses because we already have a good idea about what our sample size is based on our answers to Q13, Q14, and Q15. Therefore, we are primarily interested in finding out what effect sizes we are able to find for a given power level or what our power is given our minimum effect size of interest. In our example, \n\nwe chose the second option. When presenting your power analysis be sure to state the version of G*Power, R, or any other tool you calculated power with, including any packages or add-ons, and also report or copy all the input and results of the power analysis.\n\nQuestion 22: What criteria will you use to make inferences? Describe the information you will use (e.g., specify the p-values, effect sizes, confidence intervals, Bayes factors, specific model fit indices), as well as cut-off criteria, where appropriate. Will you be using one- or two-tailed tests for each of your analyses? If you are comparing multiple conditions or testing multiple hypotheses, will you account for this, and if so, how?\n\nWe will make inferences about the association between religiosity and prosociality based on the p-values and the size of the regression coefficients of the religiosity variable in the three main regressions.",
    "is_useful": true,
    "question": "What considerations should be taken into account when conducting a power analysis for secondary data analyses?"
  },
  {
    "text": "In our example, \n\nwe chose the second option. When presenting your power analysis be sure to state the version of G*Power, R, or any other tool you calculated power with, including any packages or add-ons, and also report or copy all the input and results of the power analysis.\n\nQuestion 22: What criteria will you use to make inferences? Describe the information you will use (e.g., specify the p-values, effect sizes, confidence intervals, Bayes factors, specific model fit indices), as well as cut-off criteria, where appropriate. Will you be using one- or two-tailed tests for each of your analyses? If you are comparing multiple conditions or testing multiple hypotheses, will you account for this, and if so, how?\n\nWe will make inferences about the association between religiosity and prosociality based on the p-values and the size of the regression coefficients of the religiosity variable in the three main regressions. We will conclude that a regression analysis supports our hypothesis if both the p-value is smaller than .01 and the regression coefficient is larger than our minimum effect size of interest. We chose an alpha of .01 to account for the fact that we do a test for each of the three regressions (0.05/3, rounded down). If the conditions above hold for all three regressions, we will conclude that our hypothesis is fully supported, if they hold for one or two of the regressions, we will conclude that our hypothesis is partially supported, and if they hold for none of the regressions we will conclude that our hypothesis is not supported.",
    "is_useful": true,
    "question": "What factors should be considered when reporting the results of a power analysis in research?"
  },
  {
    "text": "If you are comparing multiple conditions or testing multiple hypotheses, will you account for this, and if so, how?\n\nWe will make inferences about the association between religiosity and prosociality based on the p-values and the size of the regression coefficients of the religiosity variable in the three main regressions. We will conclude that a regression analysis supports our hypothesis if both the p-value is smaller than .01 and the regression coefficient is larger than our minimum effect size of interest. We chose an alpha of .01 to account for the fact that we do a test for each of the three regressions (0.05/3, rounded down). If the conditions above hold for all three regressions, we will conclude that our hypothesis is fully supported, if they hold for one or two of the regressions, we will conclude that our hypothesis is partially supported, and if they hold for none of the regressions we will conclude that our hypothesis is not supported.\n\nIt is crucial to specify your inference criteria before running a statistical analysis because researchers have a tendency to move the goalposts when making inferences. For example, almost 40% of p-values between 0.05 and 0.10 are reported as \"marginally significant\", even though these values are not significant when compared to the traditional alpha level of 0.05, and the evidential value of these p-values is low (Olsson-Collentine, Van Assen, & Hartgerink, 2019).",
    "is_useful": true,
    "question": "How can researchers avoid bias in statistical analysis when testing multiple hypotheses?"
  },
  {
    "text": "You can use your prior knowledge of the dataset to set up a decision tree specifying possible problems that might arise and how you will address them in the analyses. Thinking through such a decision tree will make you less overwhelmed when something does end up going differently than expected. \n\nHowever, note that decision trees come with their own problems and can quickly become very complex. Alternatively, you might choose to select analysis methods that make assumptions that are as conservative as possible; preregister robustness analyses which test the robustness of your findings to analysis strategies that make different assumptions; and/or pre-specify a single primary analysis strategy but note that you will also report an exploratory investigation of the validity of distributional assumptions (Williams & Albers, 2019). Of course, there are pros and cons to all methods of dealing with violations, and you should choose a technique that is most appropriate for your study.\n\nQuestion 24: Provide a series of decisions about evaluating the strength, reliability, or robustness of your focal hypothesis test. This may include within-study replication attempts, additional covariates, cross-validation efforts (out-of-sample replication, split/hold-out sample), applying weights, selectively applying constraints in an SEM context (e.g., comparing model fit statistics), overfitting adjustment techniques used (e.g., regularization approaches such as ridge regression), or some other\n\n#### simulation/sampling/bootstrapping method.\n\nTo assess the sensitivity of our results to our selection criterion for outliers, we will run an additional analysis without removing any outliers.\n\nThere are many methods you can use to test the limits of your hypothesis.",
    "is_useful": true,
    "question": "What strategies can researchers employ to address potential issues in data analysis and ensure the robustness of their findings?"
  },
  {
    "text": "Of course, there are pros and cons to all methods of dealing with violations, and you should choose a technique that is most appropriate for your study.\n\nQuestion 24: Provide a series of decisions about evaluating the strength, reliability, or robustness of your focal hypothesis test. This may include within-study replication attempts, additional covariates, cross-validation efforts (out-of-sample replication, split/hold-out sample), applying weights, selectively applying constraints in an SEM context (e.g., comparing model fit statistics), overfitting adjustment techniques used (e.g., regularization approaches such as ridge regression), or some other\n\n#### simulation/sampling/bootstrapping method.\n\nTo assess the sensitivity of our results to our selection criterion for outliers, we will run an additional analysis without removing any outliers.\n\nThere are many methods you can use to test the limits of your hypothesis. The options mentioned in the question are not supposed to be exhaustive or prescriptive. We included these examples to encourage researchers to think about these methods, all of which serve the same purpose as preregistration: improving the robustness and replicability of the results.\n\nQuestion 25: If you plan to explore your dataset to look for unexpected differences or relationships, describe those tests here, or add them to the final paper under a heading that clearly differentiates this exploratory part of your study from the confirmatory part.\n\nAs an exploratory analysis, we will test the relationship between scores on the religiosity scale and prosociality after adjusting for a variety of social, educational, and cognitive covariates that are available in the dataset.",
    "is_useful": true,
    "question": "What are some of the techniques that can improve the robustness and replicability of research results in the context of open science?"
  },
  {
    "text": "To assess the sensitivity of our results to our selection criterion for outliers, we will run an additional analysis without removing any outliers.\n\nThere are many methods you can use to test the limits of your hypothesis. The options mentioned in the question are not supposed to be exhaustive or prescriptive. We included these examples to encourage researchers to think about these methods, all of which serve the same purpose as preregistration: improving the robustness and replicability of the results.\n\nQuestion 25: If you plan to explore your dataset to look for unexpected differences or relationships, describe those tests here, or add them to the final paper under a heading that clearly differentiates this exploratory part of your study from the confirmatory part.\n\nAs an exploratory analysis, we will test the relationship between scores on the religiosity scale and prosociality after adjusting for a variety of social, educational, and cognitive covariates that are available in the dataset. We have no specific hypotheses about which covariates will attenuate the religiosity-prosociality relation most substantially, but we will use this exploratory analysis to generate hypotheses to test in other, independent datasets.\n\nWhereas it is not presently the norm to preregister exploratory analyses, it is often good to be clear about which variables will be explored (if any), for example, to differentiate these from the variables for which you have specific predictions or to plan ahead about how to compute these variables.",
    "is_useful": true,
    "question": "What strategies can researchers employ to enhance the robustness and replicability of their study results?"
  },
  {
    "text": "Question 25: If you plan to explore your dataset to look for unexpected differences or relationships, describe those tests here, or add them to the final paper under a heading that clearly differentiates this exploratory part of your study from the confirmatory part.\n\nAs an exploratory analysis, we will test the relationship between scores on the religiosity scale and prosociality after adjusting for a variety of social, educational, and cognitive covariates that are available in the dataset. We have no specific hypotheses about which covariates will attenuate the religiosity-prosociality relation most substantially, but we will use this exploratory analysis to generate hypotheses to test in other, independent datasets.\n\nWhereas it is not presently the norm to preregister exploratory analyses, it is often good to be clear about which variables will be explored (if any), for example, to differentiate these from the variables for which you have specific predictions or to plan ahead about how to compute these variables.\n\n#### Part 6: Statement of integrity\n\nThe authors of this preregistration state that they filled out this preregistration to the best of their knowledge and that no other preregistration exists pertaining to the same hypotheses and dataset.\n\n# Summary\n\nIn this tutorial we presented a preregistration template for the analysis of secondary data and have provided guidance for its effective use. We are aware that the number of questions (25) in the template may be overwhelming, but it is important to note that not every question is relevant for every preregistration. Our aim was to be inclusive and cover all bases in light of the diversity of secondary data analyses.",
    "is_useful": true,
    "question": "What practices are encouraged to differentiate exploratory analyses from confirmatory analyses in research studies?"
  },
  {
    "text": "Whereas it is not presently the norm to preregister exploratory analyses, it is often good to be clear about which variables will be explored (if any), for example, to differentiate these from the variables for which you have specific predictions or to plan ahead about how to compute these variables.\n\n#### Part 6: Statement of integrity\n\nThe authors of this preregistration state that they filled out this preregistration to the best of their knowledge and that no other preregistration exists pertaining to the same hypotheses and dataset.\n\n# Summary\n\nIn this tutorial we presented a preregistration template for the analysis of secondary data and have provided guidance for its effective use. We are aware that the number of questions (25) in the template may be overwhelming, but it is important to note that not every question is relevant for every preregistration. Our aim was to be inclusive and cover all bases in light of the diversity of secondary data analyses. Even though none of the questions are mandatory, we do believe that an elaborate preregistration is preferable over a concise preregistration simply because it restricts more researcher degrees of freedom. We therefore recommend that authors answer as many questions in as much detail as possible. And, if questions are not applicable, it would be good practice to also specify why this is the case so that readers can assess your reasoning.\n\nEffectively preregistering a study is challenging and can take a lot of time but, like Nosek et al.",
    "is_useful": true,
    "question": "What are the benefits of conducting a detailed preregistration for analyses of secondary data?"
  },
  {
    "text": "# Summary\n\nIn this tutorial we presented a preregistration template for the analysis of secondary data and have provided guidance for its effective use. We are aware that the number of questions (25) in the template may be overwhelming, but it is important to note that not every question is relevant for every preregistration. Our aim was to be inclusive and cover all bases in light of the diversity of secondary data analyses. Even though none of the questions are mandatory, we do believe that an elaborate preregistration is preferable over a concise preregistration simply because it restricts more researcher degrees of freedom. We therefore recommend that authors answer as many questions in as much detail as possible. And, if questions are not applicable, it would be good practice to also specify why this is the case so that readers can assess your reasoning.\n\nEffectively preregistering a study is challenging and can take a lot of time but, like Nosek et al. (2019) and many others, we believe it can improve the interpretability, verifiability and rigor of your studies and is therefore more than worth it if you want both yourself and others to have more confidence in your research findings.\n\nThe current template is merely one building block toward a more effective preregistration infrastructure and, given the ongoing developments in this area, will be a work in progress for the foreseeable future. Any feedback is therefore greatly appreciated. Please send any feedback to the corresponding author, Olmo van den Akker (ovdakker@gmail.com).",
    "is_useful": true,
    "question": "What are the benefits of elaborate preregistration in research studies?"
  },
  {
    "text": "We therefore recommend that authors answer as many questions in as much detail as possible. And, if questions are not applicable, it would be good practice to also specify why this is the case so that readers can assess your reasoning.\n\nEffectively preregistering a study is challenging and can take a lot of time but, like Nosek et al. (2019) and many others, we believe it can improve the interpretability, verifiability and rigor of your studies and is therefore more than worth it if you want both yourself and others to have more confidence in your research findings.\n\nThe current template is merely one building block toward a more effective preregistration infrastructure and, given the ongoing developments in this area, will be a work in progress for the foreseeable future. Any feedback is therefore greatly appreciated. Please send any feedback to the corresponding author, Olmo van den Akker (ovdakker@gmail.com).\n\n#### Author Contact\n\nCorrespondence concerning this article should be addressed to Olmo van den Akker, e-mail: ovdakker@gmail.com. https://orcid.org/0000- 0002-0712-3746\n\n# Acknowledgments\n\nThe authors would like to thank the participants of the Society for the Improvement of Psychological Science (SIPS) conference in 2018 that helped to create the first draft of the preregistration template but were unable to help out with the subsequent extensions (Brian Brown, Oliver Clark, Charles Ebersole, and Courtney Soderberg).",
    "is_useful": true,
    "question": "What are some benefits of effectively preregistering a study in the context of research?"
  },
  {
    "text": "#### Author Contact\n\nCorrespondence concerning this article should be addressed to Olmo van den Akker, e-mail: ovdakker@gmail.com. https://orcid.org/0000- 0002-0712-3746\n\n# Acknowledgments\n\nThe authors would like to thank the participants of the Society for the Improvement of Psychological Science (SIPS) conference in 2018 that helped to create the first draft of the preregistration template but were unable to help out with the subsequent extensions (Brian Brown, Oliver Clark, Charles Ebersole, and Courtney Soderberg).\n\n#### Conflict of Interest and Funding\n\nThis research is based on data from the Wisconsin Longitudinal Study, funded by the National Institute on Aging (R01 AG009775; R01 AG033285), and was supported by a Consolidator Grant (IMPROVE) from the European Research Council (ERC; grant no. 726361).\n\n#### Author Contributions\n\nConceptualization: OA, SW, MB Writing (original draft): OA, LC, WC, RD, PDK, AH, JK, EK, JO, SR, KDV, AV, MB Writing (reviewing and editing): OA, SW, LC, WC, RD, PDK, AH, JK, EK, JO, SR, KDV, AV, MB Project administration: OA\n\n#### Open Science Practices\n\nThis article is a tutorial that did not include any data or conducted analyses. It was not pre-registered. Therefore, it didn't receive any badges. The entire editorial process, including the open reviews, is published in the online supplement.",
    "is_useful": true,
    "question": "What are the key practices associated with open science as mentioned in relation to the editorial process of research articles?"
  },
  {
    "text": "726361).\n\n#### Author Contributions\n\nConceptualization: OA, SW, MB Writing (original draft): OA, LC, WC, RD, PDK, AH, JK, EK, JO, SR, KDV, AV, MB Writing (reviewing and editing): OA, SW, LC, WC, RD, PDK, AH, JK, EK, JO, SR, KDV, AV, MB Project administration: OA\n\n#### Open Science Practices\n\nThis article is a tutorial that did not include any data or conducted analyses. It was not pre-registered. Therefore, it didn't receive any badges. The entire editorial process, including the open reviews, is published in the online supplement.\n\n> \n\n# References\n\n- American Psychological Association. (2019). Publication Practices & Responsible Authorship. Retrieved from https://www.apa.org/research/responsible/publication\n- Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. Advances in Methods and Practices in Psychological Science. https://doi.org/10.1177/2515245919838783\n- Bakker, M., & Wicherts, J. M. (2014). Outlier removal, sum scores, and the inflation of the type I error rate in independent samples t tests: The power of alternatives and recommendations.",
    "is_useful": true,
    "question": "What practices characterize the author's approach to open science in their work?"
  },
  {
    "text": "It was not pre-registered. Therefore, it didn't receive any badges. The entire editorial process, including the open reviews, is published in the online supplement.\n\n> \n\n# References\n\n- American Psychological Association. (2019). Publication Practices & Responsible Authorship. Retrieved from https://www.apa.org/research/responsible/publication\n- Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. Advances in Methods and Practices in Psychological Science. https://doi.org/10.1177/2515245919838783\n- Bakker, M., & Wicherts, J. M. (2014). Outlier removal, sum scores, and the inflation of the type I error rate in independent samples t tests: The power of alternatives and recommendations. Psychological Methods, 19(3), 409. https://doi.org/10.1037/met0000014\n- Bowman, S. D., DeHaven, A. C., Errington, T. M., Hardwicke, T. E., Mellor, D. T., Nosek, B. A., & Soderberg, C. K. (2016, January 1). OSF Prereg Template.\n- https://doi.org/10.31222/osf.io/epgjd Brookfield, K., Parry, J., & Bolton, V. (2018).",
    "is_useful": true,
    "question": "What are some practices in open science concerning the editorial process and documentation of reviews?"
  },
  {
    "text": "La Laguna, Tenerife, Spain\n\n6 Department of Medical and Surgical Sciences, Pharmacology Unit, Alma Mater Studiorum, University of Bologna, Bologna, Italy\n\n7 Department of Ethics, Law & Humanities, APH research institute, Amsterdam UMC-VU University, Amsterdam, The Netherlands\n\n### Correspondence\n\nMichiel J. Bakkum, Department of Internal Medicine, Section Pharmacotherapy Amsterdam UMC, Vrije Universiteit Amsterdam, De Boelelaan 1117, 1081 HV, Amsterdam, The Netherlands. Email: m.bakkum@amsterdamumc.nl\n\nFunding information European Union under Erasmus+ grant, Grant/Award Number: 2020-1-NL01-KA203-083098\n\n# Abstract\n\nAims: Medical case vignettes play a crucial role in medical education, yet they often fail to authentically represent diverse patients. Moreover, these vignettes tend to oversimplify the complex relationship between patient characteristics and medical conditions, leading to biased and potentially harmful perspectives among students. Displaying aspects of patient diversity, such as ethnicity, in written cases proves challenging. Additionally, creating these cases places a significant burden on teachers in terms of labour and time. Our objective is to explore the potential of artificial intelligence (AI)-assisted computer-generated clinical cases to expedite case creation and enhance diversity, along with AI-generated patient photographs for more lifelike portrayal.\n\nMethods: In this study, we employed ChatGPT (OpenAI, GPT 3.5) to develop diverse and inclusive medical case vignettes.",
    "is_useful": true,
    "question": "How can artificial intelligence contribute to the representation of diverse patients in medical education?"
  },
  {
    "text": "Moreover, these vignettes tend to oversimplify the complex relationship between patient characteristics and medical conditions, leading to biased and potentially harmful perspectives among students. Displaying aspects of patient diversity, such as ethnicity, in written cases proves challenging. Additionally, creating these cases places a significant burden on teachers in terms of labour and time. Our objective is to explore the potential of artificial intelligence (AI)-assisted computer-generated clinical cases to expedite case creation and enhance diversity, along with AI-generated patient photographs for more lifelike portrayal.\n\nMethods: In this study, we employed ChatGPT (OpenAI, GPT 3.5) to develop diverse and inclusive medical case vignettes. We evaluated various approaches and identified a set of eight consecutive prompts that can be readily customized to accommodate local contexts and specific assignments. To enhance visual representation, we utilized Adobe Firefly beta for image generation.\n\nResults: Using the described prompts, we consistently generated cases for various assignments, producing sets of 30 cases at a time. We ensured the inclusion of mandatory checks and formatting, completing the process within approximately 60 min per set.\n\nConclusions: Our approach significantly accelerated case creation and improved diversity, although prioritizing maximum diversity compromised representativeness to some extent. While the optimized prompts are easily reusable, the process itself demands computer skills not all educators possess. To address this, we aim to share all created patients as open educational resources, empowering educators to create cases independently.\n\nThe authors confirm that the principal investigator for this paper is Michiel J. Bakkum. Clinical responsibility is not applicable to this research.",
    "is_useful": true,
    "question": "What are the potential benefits of using artificial intelligence in the creation of medical case vignettes in the context of open science?"
  },
  {
    "text": "We evaluated various approaches and identified a set of eight consecutive prompts that can be readily customized to accommodate local contexts and specific assignments. To enhance visual representation, we utilized Adobe Firefly beta for image generation.\n\nResults: Using the described prompts, we consistently generated cases for various assignments, producing sets of 30 cases at a time. We ensured the inclusion of mandatory checks and formatting, completing the process within approximately 60 min per set.\n\nConclusions: Our approach significantly accelerated case creation and improved diversity, although prioritizing maximum diversity compromised representativeness to some extent. While the optimized prompts are easily reusable, the process itself demands computer skills not all educators possess. To address this, we aim to share all created patients as open educational resources, empowering educators to create cases independently.\n\nThe authors confirm that the principal investigator for this paper is Michiel J. Bakkum. Clinical responsibility is not applicable to this research.\n\nThis is an open access article under the terms of the Creative Commons Attribution-NonCommercial License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes.\n\n\u00a9 2023 The Authors. British Journal of Clinical Pharmacology published by John Wiley & Sons Ltd on behalf of British Pharmacological Society.\n\nKEYWORDS artificial intelligence, ChatGPT, diversity and inclusivity\n\n# 1 | INTRODUCTION\n\nMedical case vignettes are widely used for educational purposes, but they often fall short in representing the diverse patient population encountered in clinical practice.1,2 The process of clinical reasoning begins the moment healthcare professionals access a patient's electronic health record.",
    "is_useful": true,
    "question": "How can open resources enhance educational practices in medical case creation?"
  },
  {
    "text": "To address this, we aim to share all created patients as open educational resources, empowering educators to create cases independently.\n\nThe authors confirm that the principal investigator for this paper is Michiel J. Bakkum. Clinical responsibility is not applicable to this research.\n\nThis is an open access article under the terms of the Creative Commons Attribution-NonCommercial License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes.\n\n\u00a9 2023 The Authors. British Journal of Clinical Pharmacology published by John Wiley & Sons Ltd on behalf of British Pharmacological Society.\n\nKEYWORDS artificial intelligence, ChatGPT, diversity and inclusivity\n\n# 1 | INTRODUCTION\n\nMedical case vignettes are widely used for educational purposes, but they often fall short in representing the diverse patient population encountered in clinical practice.1,2 The process of clinical reasoning begins the moment healthcare professionals access a patient's electronic health record. The patient's name may lead us to make assumptions about their ethnicity, while their prior medical history provides information about their condition and healthcare utilization. Even during the short walk back from the waiting room, we may form subconscious assumptions about the patient's gender identity, mobility, health literacy and more. In the best scenarios, these assumptions can enhance patient care by allowing personalized medical approaches. However, in less favourable situations, they can contribute to implicit provider bias.3 To accurately reflect this reality and ensure that students gain experience with and an awareness of these complexities, it is vital that simulated case scenarios incorporate patients who mirror the diversity commonly encountered in real clinical settings.",
    "is_useful": true,
    "question": "What is the aim of sharing created patients as open educational resources in the context of medical education?"
  },
  {
    "text": "While it is important to educate students about relative risk factors, these oversimplified and stereotypical connections can inadvertently reinforce essentialist biases and foster the mistaken belief that these characteristics have a direct one-to-one association with disease.5,6 Such an approach can have detrimental effects on real patient care, resulting in delayed diagnosis and suboptimal treatment.7 Additionally, the persistence of race-based medicine, which considers race as an essential factor in diagnostic algorithms and clinical practice guidelines, remains prevalent. In our previous publication, a clinical pharmacology and therapeutics teachers' guide, we provided insights into effectively teaching about such guidelines without oversimplifying the relationship between race and illness.8 Awareness of these biases is crucial during the creation of cases. However, attempting to eliminate them may inadvertently introduce new biases. Achieving the delicate balance between promoting diversity and mitigating stereotyping proves challenging, as human creators are inherently constrained in their capacity to develop truly diverse cases.\n\nThe process of manually creating high-quality cases, even without attention for diversity, is a labour-intensive task. This is particularly true for (objective structured) clinical examinations, where a substantial number of cases must be generated to prevent fraudulent practices. Policies that require publishing these cases after their initial use may restrict their reusability in subsequent years.",
    "is_useful": true,
    "question": "What challenges are associated with promoting diversity in the creation of clinical cases within medical education?"
  },
  {
    "text": "In our previous publication, a clinical pharmacology and therapeutics teachers' guide, we provided insights into effectively teaching about such guidelines without oversimplifying the relationship between race and illness.8 Awareness of these biases is crucial during the creation of cases. However, attempting to eliminate them may inadvertently introduce new biases. Achieving the delicate balance between promoting diversity and mitigating stereotyping proves challenging, as human creators are inherently constrained in their capacity to develop truly diverse cases.\n\nThe process of manually creating high-quality cases, even without attention for diversity, is a labour-intensive task. This is particularly true for (objective structured) clinical examinations, where a substantial number of cases must be generated to prevent fraudulent practices. Policies that require publishing these cases after their initial use may restrict their reusability in subsequent years. During a study focused on determining the specific needs of clinical pharmacology and therapeutics teachers in terms of open educational resources (OERs), it was observed that a repository of reusable clinical case scenarios was the most frequently desired resource on the European Open Platform for Prescribing Education (EurOP2 E; http:// prescribingeducation.eu).9\n\nIntroducing computer-generated case generation with randomized patient characteristics holds promise in theoretically enhancing diversity while simultaneously simplifying the case-creation process. However, our previous attempts at computer-generated case generation highlighted certain challenges. Completely randomized cases often led to the emergence of implausible scenarios, such as ethnically inappropriate names and biologically impossible combinations of diversity (eg, a 70-year-old individual being pregnant).",
    "is_useful": true,
    "question": "What challenges are associated with creating diverse clinical case scenarios in the context of open educational resources?"
  },
  {
    "text": "This is particularly true for (objective structured) clinical examinations, where a substantial number of cases must be generated to prevent fraudulent practices. Policies that require publishing these cases after their initial use may restrict their reusability in subsequent years. During a study focused on determining the specific needs of clinical pharmacology and therapeutics teachers in terms of open educational resources (OERs), it was observed that a repository of reusable clinical case scenarios was the most frequently desired resource on the European Open Platform for Prescribing Education (EurOP2 E; http:// prescribingeducation.eu).9\n\nIntroducing computer-generated case generation with randomized patient characteristics holds promise in theoretically enhancing diversity while simultaneously simplifying the case-creation process. However, our previous attempts at computer-generated case generation highlighted certain challenges. Completely randomized cases often led to the emergence of implausible scenarios, such as ethnically inappropriate names and biologically impossible combinations of diversity (eg, a 70-year-old individual being pregnant). While introducing extreme forms of diversity can be beneficial in encouraging students to question preconceived notions, we recognized the need to strike a balance that avoids the creation of scenarios that would undermine credibility or risk unintended ridicule.",
    "is_useful": true,
    "question": "What challenges are associated with computer-generated case generation in clinical education, particularly in terms of diversity and credibility?"
  },
  {
    "text": "During a study focused on determining the specific needs of clinical pharmacology and therapeutics teachers in terms of open educational resources (OERs), it was observed that a repository of reusable clinical case scenarios was the most frequently desired resource on the European Open Platform for Prescribing Education (EurOP2 E; http:// prescribingeducation.eu).9\n\nIntroducing computer-generated case generation with randomized patient characteristics holds promise in theoretically enhancing diversity while simultaneously simplifying the case-creation process. However, our previous attempts at computer-generated case generation highlighted certain challenges. Completely randomized cases often led to the emergence of implausible scenarios, such as ethnically inappropriate names and biologically impossible combinations of diversity (eg, a 70-year-old individual being pregnant). While introducing extreme forms of diversity can be beneficial in encouraging students to question preconceived notions, we recognized the need to strike a balance that avoids the creation of scenarios that would undermine credibility or risk unintended ridicule.\n\nThe emergence of user-friendly artificial intelligence (AI), such as ChatGPT, has presented an opportunity to explore whether it can address the issues of limited diversity and stereotype-driven content found in human-generated cases, as well as the limitations of completely random computer-generated scenarios.10 This innovative approach aims to utilize an AI-powered chatbot (OpenAI ChatGPT) to randomize patient characteristics in a manner that ensures broad inclusivity and maintains ethnical and biological appropriateness. Furthermore, it seeks to convey a significant portion of this information without relying on predefined categories, while preserving nuance similar to real-life clinical scenarios.",
    "is_useful": true,
    "question": "What challenges do researchers face when using randomized case generation in clinical education?"
  },
  {
    "text": "While introducing extreme forms of diversity can be beneficial in encouraging students to question preconceived notions, we recognized the need to strike a balance that avoids the creation of scenarios that would undermine credibility or risk unintended ridicule.\n\nThe emergence of user-friendly artificial intelligence (AI), such as ChatGPT, has presented an opportunity to explore whether it can address the issues of limited diversity and stereotype-driven content found in human-generated cases, as well as the limitations of completely random computer-generated scenarios.10 This innovative approach aims to utilize an AI-powered chatbot (OpenAI ChatGPT) to randomize patient characteristics in a manner that ensures broad inclusivity and maintains ethnical and biological appropriateness. Furthermore, it seeks to convey a significant portion of this information without relying on predefined categories, while preserving nuance similar to real-life clinical scenarios. This is achieved through the integration of AI image generation techniques (Adobe Firefly). The primary objective of this approach is to create a significant quantity of authentic and diverse cases that can be reused and customized as OERs within local educational contexts. Furthermore, it strives to enhance the accessibility of case creation using AI for all medical educators, particularly those teaching clinical pharmacology and therapeutics who are connected via EurOP2 E.\n\n# 2 | METHODS\n\nTo assess the feasibility of this approach, we conducted initial tests to determine if ChatGPT could generate medical cases. Initially, we\n\n![](_page_2_Picture_2.jpeg)\n\nprompted ChatGPT to create complete medical cases using a single prompt.",
    "is_useful": true,
    "question": "How can artificial intelligence contribute to increasing diversity and inclusivity in educational content creation?"
  },
  {
    "text": "Furthermore, it seeks to convey a significant portion of this information without relying on predefined categories, while preserving nuance similar to real-life clinical scenarios. This is achieved through the integration of AI image generation techniques (Adobe Firefly). The primary objective of this approach is to create a significant quantity of authentic and diverse cases that can be reused and customized as OERs within local educational contexts. Furthermore, it strives to enhance the accessibility of case creation using AI for all medical educators, particularly those teaching clinical pharmacology and therapeutics who are connected via EurOP2 E.\n\n# 2 | METHODS\n\nTo assess the feasibility of this approach, we conducted initial tests to determine if ChatGPT could generate medical cases. Initially, we\n\n![](_page_2_Picture_2.jpeg)\n\nprompted ChatGPT to create complete medical cases using a single prompt. While ChatGPT demonstrated the ability to generate cases, the results lacked diversity and customizing the output proved challenging. We observed that longer prompts often led ChatGPT to disregard certain parts of our instructions. Based on these findings, we determined that generating cases through small, incremental portions of patient characteristics at a time yielded more favourable outcomes.\n\nWe utilized ChatGPT to generate a table of fictive patients by prompting it to first generate their names and ethnicities (to reflect the diversity of Amsterdam). Subsequently, we added additional columns, one at a time, for characteristics such as age, body mass index (BMI), lifestyle and medicine use. This method yielded improved results as it allowed us to introduce more diversity and provide specific limitations in the prompts.",
    "is_useful": true,
    "question": "What strategies are employed to enhance the generation of diverse medical cases for educational use in open science?"
  },
  {
    "text": "To capture the outputs, we recommend making ChatGPT output them as .csv files (as explained in the footnote to Table 1) and copying the responses into one spreadsheet. While transforming this spreadsheet into nicely formatted cases can be done manually, we suggest exploring the mail merge option in MS Word as a way to automate the process. By utilizing mail merge, data from the spreadsheet can be seamlessly merged into predefined templates, resulting in time and effort savings by eliminating the need for individual case formatting.\n\nWhile the authors have access to a ChatGPT Plus subscription, which proved helpful during peak usage times for nonpaying users, for the sake of repeatability, all prompts were conducted using GPT-3.5, the freely accessible version of OpenAI's ChatGPT (https://chat. openai.com/). All images were generated using Adobe Firefly's (beta) free text to image generator (https://firefly.adobe.com/).\n\n# 3 | RESULTS\n\nUsing this approach, we have successfully expedited the process of creating cases, while taking steps to improve the representation of diversity within these cases. Figure 2 presents four case vignettes about cellulitis and Supporting Information Data S1 shows the 30 unedited vignettes generated for urinary tract infections. The optimization of prompts required considerable time and effort, but having completed this process, the generation of 30 cases, including formatting, manual checks for appropriateness and other necessary tasks, could be accomplished within approximately 1 h.\n\nOur objective was to implement a trial using these cases in our educational setting, where students will evaluate and provide\n\n!",
    "is_useful": true,
    "question": "What strategies can be used to efficiently manage and format data outputs in open science research?"
  },
  {
    "text": "[](_page_3_Figure_1.jpeg)\n\n![](_page_3_Figure_2.jpeg)\n\nFIGURE 1 Overview of randomized characteristics and their interconnections. The patient characteristics generated by ChatGPT: blue, randomized characteristics displayed as text; grey, dummy variables used for influencing other characteristics; yellow, characteristics to be combined in medical alerts and disabilities.\n\nfeedback on them. This feedback will be utilized to further refine and enhance the case-generation process. We envision a lesson where each student is presented with a patient who presents in a similar manner, but with patients themselves demonstrating substantial diversity. Through team-based learning,12 students will engage in discussions to identify and analyse the distinct factors that differentiate the patients and explore how these variations impact their diagnostic and treatment approaches. This interactive approach fosters critical thinking and promotes a deeper understanding of the influence of diversity on healthcare decision-making.\n\n# 4 | DISCUSSION\n\nOur approach has successfully achieved its goal of expediting the case-creation process, enhancing diversity through computer randomization of patient characteristics. Nevertheless, it is important to acknowledge a trade-off when it comes to representativeness. In the real world, patients do not possess a randomized set of characteristics; instead, they have multiple social identities that intersect and mutually shape one another (known as intersectionality).13,14 While we believe ChatGPT to be quite capable of giving ethnically appropriate names, this was not verified through name databases. Moreover, it insufficiently takes other intersections into account.",
    "is_useful": true,
    "question": "How can computer randomization of patient characteristics enhance diversity in healthcare education?"
  },
  {
    "text": "Through team-based learning,12 students will engage in discussions to identify and analyse the distinct factors that differentiate the patients and explore how these variations impact their diagnostic and treatment approaches. This interactive approach fosters critical thinking and promotes a deeper understanding of the influence of diversity on healthcare decision-making.\n\n# 4 | DISCUSSION\n\nOur approach has successfully achieved its goal of expediting the case-creation process, enhancing diversity through computer randomization of patient characteristics. Nevertheless, it is important to acknowledge a trade-off when it comes to representativeness. In the real world, patients do not possess a randomized set of characteristics; instead, they have multiple social identities that intersect and mutually shape one another (known as intersectionality).13,14 While we believe ChatGPT to be quite capable of giving ethnically appropriate names, this was not verified through name databases. Moreover, it insufficiently takes other intersections into account. For instance, ChatGPT generated a case where a 61-year-old Moroccan patient had an Arabic first name and a Dutch last name (Mohamed van der Berg). While this combination is not impossible, it is uncommon, as most 61-year-old Moroccan males living in the Netherlands are firstgeneration migrants, and such name combinations are more likely in later generations. Another example is the case vignette of Fatima Rahman in Figure 2, who consumes an atypical amount of alcohol (1.8 units per day) considering her pregnancy. Our current approach aimed to minimize stereotypes, prioritizing diversity over representativeness.",
    "is_useful": true,
    "question": "How can team-based learning enhance understanding of diversity's impact on healthcare decision-making?"
  },
  {
    "text": "Moreover, it insufficiently takes other intersections into account. For instance, ChatGPT generated a case where a 61-year-old Moroccan patient had an Arabic first name and a Dutch last name (Mohamed van der Berg). While this combination is not impossible, it is uncommon, as most 61-year-old Moroccan males living in the Netherlands are firstgeneration migrants, and such name combinations are more likely in later generations. Another example is the case vignette of Fatima Rahman in Figure 2, who consumes an atypical amount of alcohol (1.8 units per day) considering her pregnancy. Our current approach aimed to minimize stereotypes, prioritizing diversity over representativeness. Whether this balance is optimal is a topic that should be explored in future research and subject to debate.\n\nThe same is true for the choice of what characteristics to include in the vignettes and how to display them. By incorporating a wide range of patient characteristics, students gain insights into the factors that influence their clinical decision-making, distinguishing between relevant and irrelevant information across different clinical scenarios. Case vignettes should avoid providing clues about relevance by selectively including or excluding certain details. Currently, dimensions of diversity such as religion, political beliefs and sexual preference were not encompassed in the cases. This decision was made to focus on the most commonly relevant factors in medical scenarios and to align with real-life patient care practices. For instance, parameters like BMI and smoking history hold relevance across various medical scenarios and are typically documented in medical records. Conversely, religion seldom holds relevance nor is it a routinely recorded parameter in such contexts.",
    "is_useful": true,
    "question": "How can diversity and the selection of patient characteristics affect clinical decision-making in medical education?"
  },
  {
    "text": "Whether this balance is optimal is a topic that should be explored in future research and subject to debate.\n\nThe same is true for the choice of what characteristics to include in the vignettes and how to display them. By incorporating a wide range of patient characteristics, students gain insights into the factors that influence their clinical decision-making, distinguishing between relevant and irrelevant information across different clinical scenarios. Case vignettes should avoid providing clues about relevance by selectively including or excluding certain details. Currently, dimensions of diversity such as religion, political beliefs and sexual preference were not encompassed in the cases. This decision was made to focus on the most commonly relevant factors in medical scenarios and to align with real-life patient care practices. For instance, parameters like BMI and smoking history hold relevance across various medical scenarios and are typically documented in medical records. Conversely, religion seldom holds relevance nor is it a routinely recorded parameter in such contexts. Ethnicity can be significant in relation to disease prevalence and therapeutic response.15 However, capturing ethnicity as a single categorical label poses challenges due to its multifaceted nature, encompassing concepts such as ancestry, nationality and culture.16,17 Moreover, in real clinical settings, ethnicity is seldom objectified but rather inferred from a patient's phenotype, name and cultural expressions.18,19 To reflect this reality and encourage students to recognize and challenge their assumptions, we made the decision to convey information about ethnicity (as well as [sub]culture and occupation) through a photograph of the patient, rather than a label.",
    "is_useful": true,
    "question": "What factors should be considered when creating case vignettes for medical education to enhance students' clinical decision-making?"
  },
  {
    "text": "Some of these limitations can be mitigated by including explicit specifications in the prompts or regenerating the response several times until it has the correct format and level of diversity. However, addressing other limitations necessitates manual refinement. To ensure precision and consistency, we highly recommend conducting comprehensive manual reviews of all cases to identify any potential inconsistencies.\n\nWhile our optimized prompts have significantly facilitated the process of creating cases, we acknowledge that making these cases with ChatGPT still requires a certain level of computer skills. For instance, saving ChatGPT outputs to a .csv file and creating a template for mail merge in MS Word can be challenging. Exploring the possibilities of utilizing the ChatGPT API code to streamline the process is an avenue worth considering, although it may require expertise beyond our current capabilities. As an alternative approach, we intend to make the diverse fictional individuals we generate accessible on EurOP2 E as OERs. This will allow teachers to access the patient information and manually input the cases for various medical subjects themselves. Moreover, we have plans to develop a comprehensive guide on crafting these cases detailing the challenges and opportunities, and establish a discussion board where teachers can engage in conversations about the prompts, share their experiences and collectively enhance their quality.\n\nA scientific evaluation of our efforts, including students' reactions and evaluations is not included in the current manuscript. We are committed to conducting such an evaluation and reporting the findings separately.\n\n## AUTHOR CONTRIBUTIONS\n\nMichiel J. Bakkum: Conceptualization; formal analysis; writing draft; visualization.",
    "is_useful": true,
    "question": "What strategies can be employed to improve the precision and diversity of generated case scenarios in open science projects?"
  },
  {
    "text": "Exploring the possibilities of utilizing the ChatGPT API code to streamline the process is an avenue worth considering, although it may require expertise beyond our current capabilities. As an alternative approach, we intend to make the diverse fictional individuals we generate accessible on EurOP2 E as OERs. This will allow teachers to access the patient information and manually input the cases for various medical subjects themselves. Moreover, we have plans to develop a comprehensive guide on crafting these cases detailing the challenges and opportunities, and establish a discussion board where teachers can engage in conversations about the prompts, share their experiences and collectively enhance their quality.\n\nA scientific evaluation of our efforts, including students' reactions and evaluations is not included in the current manuscript. We are committed to conducting such an evaluation and reporting the findings separately.\n\n## AUTHOR CONTRIBUTIONS\n\nMichiel J. Bakkum: Conceptualization; formal analysis; writing draft; visualization. Mari\u00eblle G. Hartjes, Joost D. Pi\u00ebt, Erik M. Donker: Conceptualization; formal analysis; writing\u2014review and editing. Robert Likic, Emilio Sanz, Fabrizio de Ponti, Petra Verdonk, Milan C. Richir, Michiel A. van Agtmael: Conceptualization; writing\u2014review and editing. Jelle Tichelaar: Conceptualization; writing\u2014review and editing; supervision.\n\n### ACKNOWLEDGEMENTS\n\nThe authors acknowledge the assistance of ChatGPT in copy-editing the text of this article.",
    "is_useful": true,
    "question": "What are some ways open educational resources (OERs) can enhance teaching in medical education?"
  },
  {
    "text": "![](_page_0_Picture_0.jpeg)\n\n![](_page_0_Picture_2.jpeg)\n\n# Are Psychology Journals Anti-replication? A Snapshot of Editorial Practices\n\n#### G. N. Martin1 * and Richard M. Clarke2\n\n1 School of Psychotherapy and Psychology, Faculty of Humanities, Arts and Social Sciences, Regent's University London, London, UK, 2 Department of Infectious Disease Epidemiology, Faculty of Epidemiology and Population Health, London School of Hygiene and Tropical Medicine, London, UK\n\nRecent research in psychology has highlighted a number of replication problems in the discipline, with publication bias \u2013 the preference for publishing original and positive results, and a resistance to publishing negative results and replications- identified as one reason for replication failure. However, little empirical research exists to demonstrate that journals explicitly refuse to publish replications. We reviewed the instructions to authors and the published aims of 1151 psychology journals and examined whether they indicated that replications were permitted and accepted. We also examined whether journal practices differed across branches of the discipline, and whether editorial practices differed between low and high impact journals. Thirty three journals (3%) stated in their aims or instructions to authors that they accepted replications. There was no difference between high and low impact journals. The implications of these findings for psychology are discussed.",
    "is_useful": true,
    "question": "What factors contribute to replication problems in psychology journals, and how do the publishing practices regarding replications differ among them?"
  },
  {
    "text": "However, little empirical research exists to demonstrate that journals explicitly refuse to publish replications. We reviewed the instructions to authors and the published aims of 1151 psychology journals and examined whether they indicated that replications were permitted and accepted. We also examined whether journal practices differed across branches of the discipline, and whether editorial practices differed between low and high impact journals. Thirty three journals (3%) stated in their aims or instructions to authors that they accepted replications. There was no difference between high and low impact journals. The implications of these findings for psychology are discussed.\n\nKeywords: replication, psychology, p-hacking, journal editorial practices, publication bias\n\n## INTRODUCTION\n\nThe recent ability, or inability, of psychology to replicate novel or well-known, classic findings in the discipline has led to the controversial, but by no means generally accepted, conclusion that psychology is undergoing a \"replication crisis\" (Pashler and Harris, 2012; Pashler and Wagenmakers, 2012; Laws, 2013; American Psychological Society, 2015; Earp and Trafimow, 2015; Maxwell et al., 2015). Reproducibility of results and the replication of findings is crucial for the development of any science as it transforms a single item of information into a coherent body of demonstrable knowledge and can establish that a reported finding is reliable, robust and consistently obtained. As Simons (2013, p. 79) notes \"direct replication by other scientists is the only way to verify the reliability of an effect.\"",
    "is_useful": true,
    "question": "What is the significance of replication in the context of scientific research, particularly in psychology?"
  },
  {
    "text": "Reproducibility of results and the replication of findings is crucial for the development of any science as it transforms a single item of information into a coherent body of demonstrable knowledge and can establish that a reported finding is reliable, robust and consistently obtained. As Simons (2013, p. 79) notes \"direct replication by other scientists is the only way to verify the reliability of an effect.\" In psychology, the reported crisis seems to be twofold: (1) the discipline has bemoaned a historical failure to publish negative results (which may arise from failed replications), and a preference for the publication of positive results, the so-called publication bias, and (2) when these replications occur, they are unlikely to support the original studies. In short, the discipline has not published enough replications and, when it does, the replications are negative.\n\nKlein et al.",
    "is_useful": true,
    "question": "Why is reproducibility and replication important in the field of science?"
  },
  {
    "text": "Reproducibility of results and the replication of findings is crucial for the development of any science as it transforms a single item of information into a coherent body of demonstrable knowledge and can establish that a reported finding is reliable, robust and consistently obtained. As Simons (2013, p. 79) notes \"direct replication by other scientists is the only way to verify the reliability of an effect.\" In psychology, the reported crisis seems to be twofold: (1) the discipline has bemoaned a historical failure to publish negative results (which may arise from failed replications), and a preference for the publication of positive results, the so-called publication bias, and (2) when these replications occur, they are unlikely to support the original studies. In short, the discipline has not published enough replications and, when it does, the replications are negative.\n\nKlein et al. (2014), for example, reporting the first of the Many Labs projects hosted by the Open Science Foundation, found a reasonably good rate of replication attempts: Of 13 replication attempts of classic and contemporary findings in social and cognitive psychology using 36 samples comprising 6344 participants, 10 were successful, 1 was weakly replicated and 2 sets of findings\n\n### Edited by:\n\nFiona Fidler, University of Melbourne, Australia\n\n#### Reviewed by:\n\nDonald Sharpe, University of Regina, Canada Marjan Bakker, Tilburg University, Netherlands\n\n*Correspondence:\n\nG. N. Martin\n\nneil.martin@regents.ac.uk\n\n#### Specialty section:\n\nThis article was submitted to Quantitative Psychology and Measurement, a section of the journal Frontiers in Psychology\n\nReceived: 22 July 2016 Accepted: 22 March 2017 Published: 11 April 2017\n\n#### Citation:\n\nMartin GN and Clarke RM (2017) Are Psychology Journals Anti-replication?",
    "is_useful": true,
    "question": "Why is reproducibility important in scientific research?"
  },
  {
    "text": "A Snapshot of Editorial Practices. Front. Psychol. 8:523. doi: 10.3389/fpsyg.2017.00523\n\nwere not. A successful replication was one that is considered to produce the same (or greater) effect in the replication as in the original. Both failures to replicate in this study involved social priming. The latest set of replication attempts by the Open Science Collaboration (2015) found that of 100 experiments in cognitive and social psychology published in the journals, Journal of Experimental Psychology: Learning, Memory and Cognition (N = 28), Psychological Science (N = 40), and Journal of Personality and Social Psychology (N = 22) in the year 2008, only 36% were successfully replicated, compared to the positive findings reported in 97% of the original studies.\n\nThe studies reported in these papers were largely direct replications. Direct replications are those which faithfully reproduce the methods and materials used in the original study and ensure that the repeated experiment follows as closely as possible the procedure, details and demands of the original research (Nosek et al., 2012). They should minimize the effect of 'moderator variables,' variables which may have been present in the replication that were not present in the original report and which are often cited by the authors of studies whose findings were not replicated as the reason for the failure to replicate. Conceptual replications are more fluid.",
    "is_useful": true,
    "question": "What percentage of experiments in cognitive and social psychology were successfully replicated according to the Open Science Collaboration's findings in 2015?"
  },
  {
    "text": "The studies reported in these papers were largely direct replications. Direct replications are those which faithfully reproduce the methods and materials used in the original study and ensure that the repeated experiment follows as closely as possible the procedure, details and demands of the original research (Nosek et al., 2012). They should minimize the effect of 'moderator variables,' variables which may have been present in the replication that were not present in the original report and which are often cited by the authors of studies whose findings were not replicated as the reason for the failure to replicate. Conceptual replications are more fluid. These repeat the original experiment but might alter the procedure or materials or participant pool or independent variable in some way in order to test the strength of the original research, and the reliability and generalisability of the original result. The argument follows that if an effect is found in situation X under condition Y, then it should also be observed in situation A under condition B.\n\nThe replication failure is not limited to psychology. Only 11% of 53 landmark preclinical cancer trials were found to replicate (Begley and Ellis, 2012) with 35% of pharmacology studies replicating (Prinz et al., 2011). Of the 49 most widely cited papers in clinical research, only 44% were replicated (Ioannidis, 2005).",
    "is_useful": true,
    "question": "What are the differences between direct replications and conceptual replications in research studies?"
  },
  {
    "text": "Conceptual replications are more fluid. These repeat the original experiment but might alter the procedure or materials or participant pool or independent variable in some way in order to test the strength of the original research, and the reliability and generalisability of the original result. The argument follows that if an effect is found in situation X under condition Y, then it should also be observed in situation A under condition B.\n\nThe replication failure is not limited to psychology. Only 11% of 53 landmark preclinical cancer trials were found to replicate (Begley and Ellis, 2012) with 35% of pharmacology studies replicating (Prinz et al., 2011). Of the 49 most widely cited papers in clinical research, only 44% were replicated (Ioannidis, 2005). Sixty per cent failed to replicate in finance (Hubbard and Vetter, 1991), 40% in advertising (Reid et al., 1981), and 54% in accounting, management, finance, economics, and marketing (Hubbard and Vetter, 1996). The situation is better in education (Makel and Plucker, 2014), human factors (Jones et al., 2010), and forecasting (Evanschitzky and Armstrong, 2010).\n\nOne ostensible reason for the current turmoil in the discipline has been psychology's publication bias (Pashler and Wagenmakers, 2012).",
    "is_useful": true,
    "question": "What challenges are associated with the replication of scientific studies across various fields? "
  },
  {
    "text": "Of the 49 most widely cited papers in clinical research, only 44% were replicated (Ioannidis, 2005). Sixty per cent failed to replicate in finance (Hubbard and Vetter, 1991), 40% in advertising (Reid et al., 1981), and 54% in accounting, management, finance, economics, and marketing (Hubbard and Vetter, 1996). The situation is better in education (Makel and Plucker, 2014), human factors (Jones et al., 2010), and forecasting (Evanschitzky and Armstrong, 2010).\n\nOne ostensible reason for the current turmoil in the discipline has been psychology's publication bias (Pashler and Wagenmakers, 2012). Publication bias, it is argued, has partly led to the current batch of failed replications because journals have historically been reluctant to publish, or authors have been reluctant to submit for publication, negative results. Single dramatic effects, therefore, may have been perpetuated and supported by the publication of positive studies while negative, unsupportive studies remained either unpublished or unsubmitted. Journals are more likely to publish studies that find statistically significant results (Schooler, 2011; Simmons et al., 2011) and null findings are less likely to be submitted or accepted for publication as a result (Rosenthal, 1979; Franco et al., 2014).",
    "is_useful": true,
    "question": "What is a significant factor contributing to failed replications in clinical research and other disciplines?"
  },
  {
    "text": "One ostensible reason for the current turmoil in the discipline has been psychology's publication bias (Pashler and Wagenmakers, 2012). Publication bias, it is argued, has partly led to the current batch of failed replications because journals have historically been reluctant to publish, or authors have been reluctant to submit for publication, negative results. Single dramatic effects, therefore, may have been perpetuated and supported by the publication of positive studies while negative, unsupportive studies remained either unpublished or unsubmitted. Journals are more likely to publish studies that find statistically significant results (Schooler, 2011; Simmons et al., 2011) and null findings are less likely to be submitted or accepted for publication as a result (Rosenthal, 1979; Franco et al., 2014).\n\nWhile publication bias is thought to be well-established, there has been no objective method of confirming whether papers have been rejected for reporting negative results, largely because these data are not publicly available. Although Fanelli (2011) found that the proportion of positive results published in peerreviewed journals increased by 22% between 1990 and 2007, it does not follow that a similar, if any, number of negative results was also submitted and rejected. Fanelli (2010) noted that 91.5% of psychology studies reported data supporting the experimental hypothesis, five times higher than that reported for rocket science. Sterling et al.'s (1995, p.",
    "is_useful": true,
    "question": "What impact does publication bias have on the availability and visibility of negative research results in the field of psychology?"
  },
  {
    "text": "While publication bias is thought to be well-established, there has been no objective method of confirming whether papers have been rejected for reporting negative results, largely because these data are not publicly available. Although Fanelli (2011) found that the proportion of positive results published in peerreviewed journals increased by 22% between 1990 and 2007, it does not follow that a similar, if any, number of negative results was also submitted and rejected. Fanelli (2010) noted that 91.5% of psychology studies reported data supporting the experimental hypothesis, five times higher than that reported for rocket science. Sterling et al.'s (1995, p. 108) study of 11 major journals concluded that these outlets continued to publish positive results and that the \"practice leading to publication bias has not changed over a period of 30 years.\" Smart (1964) found that of the five psychology journals examined, the largest percentage of non-significant results published was 17%; the lowest % of positive results was 57%. Coursol and Wagner (1986)'s selfreport survey of 1000 members of the APA's Counseling and Psychotherapy Divisions found that 66% of the authors' studies reporting positive results were published but only 22% of those reporting neutral or negative results were. While this may indicate a publication bias, the finding may be explained by an alternative reason: the negative studies may have been of poorer quality.\n\nSome journals appear to be reluctant to publish replications (whether positive or negative), and prize originality and novelty. Makel et al.",
    "is_useful": true,
    "question": "What factors contribute to publication bias in academic journals, particularly regarding the reporting of negative results?"
  },
  {
    "text": "108) study of 11 major journals concluded that these outlets continued to publish positive results and that the \"practice leading to publication bias has not changed over a period of 30 years.\" Smart (1964) found that of the five psychology journals examined, the largest percentage of non-significant results published was 17%; the lowest % of positive results was 57%. Coursol and Wagner (1986)'s selfreport survey of 1000 members of the APA's Counseling and Psychotherapy Divisions found that 66% of the authors' studies reporting positive results were published but only 22% of those reporting neutral or negative results were. While this may indicate a publication bias, the finding may be explained by an alternative reason: the negative studies may have been of poorer quality.\n\nSome journals appear to be reluctant to publish replications (whether positive or negative), and prize originality and novelty. Makel et al. (2012) study of 100 journals with the highest 5-year impact factor found that 1% of published studies were replications. A survey of 429 journal editors and editorial advisors of 19 journals in management and related social sciences, found that the percentage of papers rejected because they were replications ranged from 27 to 69% (Kerr et al., 1977).",
    "is_useful": true,
    "question": "What biases exist in the publication process of academic journals regarding positive and negative research results?"
  },
  {
    "text": "Coursol and Wagner (1986)'s selfreport survey of 1000 members of the APA's Counseling and Psychotherapy Divisions found that 66% of the authors' studies reporting positive results were published but only 22% of those reporting neutral or negative results were. While this may indicate a publication bias, the finding may be explained by an alternative reason: the negative studies may have been of poorer quality.\n\nSome journals appear to be reluctant to publish replications (whether positive or negative), and prize originality and novelty. Makel et al. (2012) study of 100 journals with the highest 5-year impact factor found that 1% of published studies were replications. A survey of 429 journal editors and editorial advisors of 19 journals in management and related social sciences, found that the percentage of papers rejected because they were replications ranged from 27 to 69% (Kerr et al., 1977). A study of 79 past and present editors of social and behavioral science journals by Neuliep and Crandall (1990) found evidence of a reluctance to publish negative results, a finding also replicated in Neuliep and Crandall's (1993a) study of social science journal reviewers- 54% preferred to publish studies with new findings. Nueliep and Crandall's data suggest that journals will only publish papers that report \"original\" (sometimes \"novel\") data, findings or studies, rather than repeat experiments.",
    "is_useful": true,
    "question": "What challenges does open science face in terms of the publication of negative or replication studies?"
  },
  {
    "text": "(2012) study of 100 journals with the highest 5-year impact factor found that 1% of published studies were replications. A survey of 429 journal editors and editorial advisors of 19 journals in management and related social sciences, found that the percentage of papers rejected because they were replications ranged from 27 to 69% (Kerr et al., 1977). A study of 79 past and present editors of social and behavioral science journals by Neuliep and Crandall (1990) found evidence of a reluctance to publish negative results, a finding also replicated in Neuliep and Crandall's (1993a) study of social science journal reviewers- 54% preferred to publish studies with new findings. Nueliep and Crandall's data suggest that journals will only publish papers that report \"original\" (sometimes \"novel\") data, findings or studies, rather than repeat experiments. Sterling (1959) found in his review of 362 psychology articles that none were replications; Bozarth and Roberts (1972) similarly found that of 1046 clinical/counseling psychology papers published between 1967 and 1970, 1% were replications. However, in their analysis of the type of papers published in the first three issues of the Journal of Personality and Social Psychology from 1993, Neuliep and Crandall (1993b) reported that 33 out of 42 of them were conceptual or direct replications.",
    "is_useful": true,
    "question": "What challenges do journals face regarding the publication of replication studies and negative results in research?"
  },
  {
    "text": "Nueliep and Crandall's data suggest that journals will only publish papers that report \"original\" (sometimes \"novel\") data, findings or studies, rather than repeat experiments. Sterling (1959) found in his review of 362 psychology articles that none were replications; Bozarth and Roberts (1972) similarly found that of 1046 clinical/counseling psychology papers published between 1967 and 1970, 1% were replications. However, in their analysis of the type of papers published in the first three issues of the Journal of Personality and Social Psychology from 1993, Neuliep and Crandall (1993b) reported that 33 out of 42 of them were conceptual or direct replications.\n\nAs with replication failure, failure to publish replications is not unique to psychology- 2% of 649 marketing journal papers published in journals between 1971 and 1975 (Brown and Coney, 1976), and 6% of 501 advertising/communication journal papers published between 1977 and 1979 (Reid et al., 1981) were replications. Hubbard and Armstrong (1994) found that none of the 835 marketing papers they reviewed were direct replications. In 18 business journals, 6.3% of papers published between 1970 and 1979 and 6.2% of those published between 1980 and 1991 were replications.",
    "is_useful": true,
    "question": "What trends have been observed regarding the publication of replication studies in various academic fields?"
  },
  {
    "text": "As with replication failure, failure to publish replications is not unique to psychology- 2% of 649 marketing journal papers published in journals between 1971 and 1975 (Brown and Coney, 1976), and 6% of 501 advertising/communication journal papers published between 1977 and 1979 (Reid et al., 1981) were replications. Hubbard and Armstrong (1994) found that none of the 835 marketing papers they reviewed were direct replications. In 18 business journals, 6.3% of papers published between 1970 and 1979 and 6.2% of those published between 1980 and 1991 were replications. In 100 education journals, 461 (of 164, 589) papers were replications and only 0.13% of these were direct replications (Makel and Plucker, 2014), eight times smaller than the percentage seen in psychology journals (Makel et al., 2012).\n\nIn forecasting research, 8.4% of papers published in two journals between 1996 and 2008 were replications (Evanschitzky and Armstrong, 2010); in marketing, 41 of 2409 (1.17%) of papers in five journals published between 1990 and 2004 were replications (Evanschitzky et al., 2007).\n\nThere is some indirect evidence, therefore, to suggest that editors and reviewers are reluctant to accept replications.",
    "is_useful": true,
    "question": "What evidence suggests a reluctance among editors and reviewers to accept replication studies in scholarly journals?"
  },
  {
    "text": "In 100 education journals, 461 (of 164, 589) papers were replications and only 0.13% of these were direct replications (Makel and Plucker, 2014), eight times smaller than the percentage seen in psychology journals (Makel et al., 2012).\n\nIn forecasting research, 8.4% of papers published in two journals between 1996 and 2008 were replications (Evanschitzky and Armstrong, 2010); in marketing, 41 of 2409 (1.17%) of papers in five journals published between 1990 and 2004 were replications (Evanschitzky et al., 2007).\n\nThere is some indirect evidence, therefore, to suggest that editors and reviewers are reluctant to accept replications. However, the evidence is primarily anecdotal, based largely on data provided by surveys of selected editorial staff and reviewers' views, and on post hoc examination of journal output where the processes leading to the output decisions are unknown. In order to provide an objective analysis of journal and editors' explicit guidance to authors regarding the value and acceptance of replication studies, we examined psychology and psychologyrelated journals' instructions to authors and journal aims and scope to determine whether (i) journals specifically accepted, discouraged or prohibited the submission of replications, (ii) acceptance of replications differed by branch of the discipline, (iii) whether journals with a high impact factor differed from those with a low impact factor.",
    "is_useful": true,
    "question": "What trends in the acceptance of replication studies in academic journals suggest editors and reviewers may be hesitant to publish them?"
  },
  {
    "text": "There is some indirect evidence, therefore, to suggest that editors and reviewers are reluctant to accept replications. However, the evidence is primarily anecdotal, based largely on data provided by surveys of selected editorial staff and reviewers' views, and on post hoc examination of journal output where the processes leading to the output decisions are unknown. In order to provide an objective analysis of journal and editors' explicit guidance to authors regarding the value and acceptance of replication studies, we examined psychology and psychologyrelated journals' instructions to authors and journal aims and scope to determine whether (i) journals specifically accepted, discouraged or prohibited the submission of replications, (ii) acceptance of replications differed by branch of the discipline, (iii) whether journals with a high impact factor differed from those with a low impact factor.\n\n### MATERIALS AND METHODS\n\nThe \"instructions to authors\" and \"aims and scope\" of peer-reviewed journals publishing psychological research were reviewed in the Summer of 2015. A list of psychology and psychology-related journals was obtained first through the selection of common publishers of psychology journals (Sage, Taylor and Francis, Elsevier, Springer, Wiley). From these, all journals within the subheading of Psychology, or using the search criteria \"Psychology,\" were selected. This sample was then cross referenced with freely available online lists of the top 100 psychology journals ranked by the journal's eigenfactor and impact factor so as to obtain top journals independently published by APA, Cambridge or others. From this initially created list of journals, all non-English language and multiple entries were removed.",
    "is_useful": true,
    "question": "What factors were examined to determine the acceptance of replication studies in psychology journals?"
  },
  {
    "text": "### MATERIALS AND METHODS\n\nThe \"instructions to authors\" and \"aims and scope\" of peer-reviewed journals publishing psychological research were reviewed in the Summer of 2015. A list of psychology and psychology-related journals was obtained first through the selection of common publishers of psychology journals (Sage, Taylor and Francis, Elsevier, Springer, Wiley). From these, all journals within the subheading of Psychology, or using the search criteria \"Psychology,\" were selected. This sample was then cross referenced with freely available online lists of the top 100 psychology journals ranked by the journal's eigenfactor and impact factor so as to obtain top journals independently published by APA, Cambridge or others. From this initially created list of journals, all non-English language and multiple entries were removed. Each journal on the list was then visited online its aims and scope section was reviewed along with any additional information that pertained to the content of articles accepted.\n\nOne thousand, one hundred and fifty one journals were identified as psychology journals (e.g., Journal of Experimental Psychology: General, Frontiers in Psychology) or psychologyrelated journals (e.g., British Journal of Learning Disabilities, Journal of Employment Counseling, American Journal of Family Therapy). The number of journals whose editorial guidelines specifically stated the acceptance of replications was calculated and compared with those which did not. The number of journals accepting replications was also calculated according to the journal's impact factor (stated on the journal's website and crosschecked through online databases) and a comparison was made between those journals rated above and below the mean impact factor.",
    "is_useful": true,
    "question": "What was the focus of the review conducted in the Summer of 2015 regarding psychology journals?"
  },
  {
    "text": "From this initially created list of journals, all non-English language and multiple entries were removed. Each journal on the list was then visited online its aims and scope section was reviewed along with any additional information that pertained to the content of articles accepted.\n\nOne thousand, one hundred and fifty one journals were identified as psychology journals (e.g., Journal of Experimental Psychology: General, Frontiers in Psychology) or psychologyrelated journals (e.g., British Journal of Learning Disabilities, Journal of Employment Counseling, American Journal of Family Therapy). The number of journals whose editorial guidelines specifically stated the acceptance of replications was calculated and compared with those which did not. The number of journals accepting replications was also calculated according to the journal's impact factor (stated on the journal's website and crosschecked through online databases) and a comparison was made between those journals rated above and below the mean impact factor. Finally, the number of journals accepting replications was calculated according to the branch of the discipline they primarily published in (e.g., general, cognitive psychology, social psychology), as seen in **Table 1**.\n\n### RESULTS\n\nOf the 1151 journals reviewed, 33 specifically stated that replications would be accepted- approximately, 3%. The mean impact factor for journals with IFs was 1.93. When we examined whether there was difference in journal practices between those with an impact factor above or below this mean, we found no difference in replication acceptance 10 [x2 (1,N = 784) = 0.55, p = 0.46, Cohen's d = 0.0529].",
    "is_useful": true,
    "question": "What percentage of psychology journals reviewed explicitly stated that they accepted replications?"
  },
  {
    "text": "Finally, the number of journals accepting replications was calculated according to the branch of the discipline they primarily published in (e.g., general, cognitive psychology, social psychology), as seen in **Table 1**.\n\n### RESULTS\n\nOf the 1151 journals reviewed, 33 specifically stated that replications would be accepted- approximately, 3%. The mean impact factor for journals with IFs was 1.93. When we examined whether there was difference in journal practices between those with an impact factor above or below this mean, we found no difference in replication acceptance 10 [x2 (1,N = 784) = 0.55, p = 0.46, Cohen's d = 0.0529].\n\nWhen the journals were examined for the specific wording they used in their aims and instructions, we were able to identify four broad types of publication: (1) Journals which stated that they accepted replications; (2) Journals which did not state they accepted replications but did not discourage replications either; (3) Journals which implicitly discouraged replications through the use of emphasis on the scientific originality of submissions, and (4) Journals which actively discouraged replications by stating explicitly that they did not accept replications for publication.\n\nThe percentage of journals in each of these categories were 3% (N = 33, category 1); 63% (N = 728, category 2); 33% (N = 379, category 3); and 1% (N = 12, category 4).",
    "is_useful": true,
    "question": "What percentage of journals specifically stated that they accepted replications according to their publication policies?"
  },
  {
    "text": "When the journals were examined for the specific wording they used in their aims and instructions, we were able to identify four broad types of publication: (1) Journals which stated that they accepted replications; (2) Journals which did not state they accepted replications but did not discourage replications either; (3) Journals which implicitly discouraged replications through the use of emphasis on the scientific originality of submissions, and (4) Journals which actively discouraged replications by stating explicitly that they did not accept replications for publication.\n\nThe percentage of journals in each of these categories were 3% (N = 33, category 1); 63% (N = 728, category 2); 33% (N = 379, category 3); and 1% (N = 12, category 4). Of the journals in category 3, 104 indicated in the first line of their aims and scope that they preferred the submission of original research.\n\nWe were able to determine the primary branch of psychology for all journals. The number of journals accepting replications, according to branch, is listed in **Table 1**. There were no significant differences between different branches of psychology in terms of their acceptance of replication [x2 (15,N = 1152) = 21.02, p = 0.14, C = 0.134]. No journal in clinical, forensic, health or evolutionary psychology explicitly accepted replications.",
    "is_useful": true,
    "question": "What are the different types of publication attitudes towards replication in academic journals?"
  },
  {
    "text": "The percentage of journals in each of these categories were 3% (N = 33, category 1); 63% (N = 728, category 2); 33% (N = 379, category 3); and 1% (N = 12, category 4). Of the journals in category 3, 104 indicated in the first line of their aims and scope that they preferred the submission of original research.\n\nWe were able to determine the primary branch of psychology for all journals. The number of journals accepting replications, according to branch, is listed in **Table 1**. There were no significant differences between different branches of psychology in terms of their acceptance of replication [x2 (15,N = 1152) = 21.02, p = 0.14, C = 0.134]. No journal in clinical, forensic, health or evolutionary psychology explicitly accepted replications.\n\n### DISCUSSION\n\nThe aim of this study was to investigate whether peerreviewed journals publishing research in psychology stated that they accepted submissions in the form of replications. Of the 1151 journals included in this study, 33 explicitly stated that replications were accepted or encouraged. There were no differences between branches of the discipline in terms of the numbers of journals accepting replications nor between journals with high or low impact factors.\n\nIt is clear that the vast majority of journals in psychology rarely ever encourage the submission of replications: only 3% do.",
    "is_useful": true,
    "question": "What percentage of psychology journals encourage the submission of replication studies?"
  },
  {
    "text": "There were no significant differences between different branches of psychology in terms of their acceptance of replication [x2 (15,N = 1152) = 21.02, p = 0.14, C = 0.134]. No journal in clinical, forensic, health or evolutionary psychology explicitly accepted replications.\n\n### DISCUSSION\n\nThe aim of this study was to investigate whether peerreviewed journals publishing research in psychology stated that they accepted submissions in the form of replications. Of the 1151 journals included in this study, 33 explicitly stated that replications were accepted or encouraged. There were no differences between branches of the discipline in terms of the numbers of journals accepting replications nor between journals with high or low impact factors.\n\nIt is clear that the vast majority of journals in psychology rarely ever encourage the submission of replications: only 3% do. A typical statement is that provided by the International Journal of Behavioral Development, for example: \"Studies whose sole purpose is to replicate well-established developmental phenomena in different countries or (sub) cultures are not typically published in the International Journal of Behavioral Development.\" This prescription is not unique to this journal.\n\nThe findings are consistent with other studies which have provided partial, anecdotal evidence that editorial practices in other journals tend to discourage the submission of replications\n\n| TABLE 1 | Acceptance of replications in psychology journals by branch of the discipline.",
    "is_useful": true,
    "question": "What is the general trend regarding the acceptance of replication studies in peer-reviewed psychology journals?"
  },
  {
    "text": "Of the 1151 journals included in this study, 33 explicitly stated that replications were accepted or encouraged. There were no differences between branches of the discipline in terms of the numbers of journals accepting replications nor between journals with high or low impact factors.\n\nIt is clear that the vast majority of journals in psychology rarely ever encourage the submission of replications: only 3% do. A typical statement is that provided by the International Journal of Behavioral Development, for example: \"Studies whose sole purpose is to replicate well-established developmental phenomena in different countries or (sub) cultures are not typically published in the International Journal of Behavioral Development.\" This prescription is not unique to this journal.\n\nThe findings are consistent with other studies which have provided partial, anecdotal evidence that editorial practices in other journals tend to discourage the submission of replications\n\n| TABLE 1 | Acceptance of replications in psychology journals by branch of the discipline. |\n| --- |\n\n| Branch | Number of journals | Number of journals with | Number of journals with | Number of journals not publicly |\n| --- | --- | --- | --- | --- |\n|  | accepting replications | an IF > mean IF which | an IF < mean which | reporting an IF and which |\n|  | (Total number of | accept replications (Total | accept replications (Total | accept replications (Total |\n|  | journals, in brackets) | number of journals with | number of journals with | number of journals not publicly |\n|  |  | IF > mean, in brackets) | IF < mean, in brackets) | reporting an IF, in brackets) |\n|",
    "is_useful": true,
    "question": "What percentage of psychology journals explicitly accept or encourage the submission of replication studies?"
  },
  {
    "text": "The findings are consistent with other studies which have provided partial, anecdotal evidence that editorial practices in other journals tend to discourage the submission of replications\n\n| TABLE 1 | Acceptance of replications in psychology journals by branch of the discipline. |\n| --- |\n\n| Branch | Number of journals | Number of journals with | Number of journals with | Number of journals not publicly |\n| --- | --- | --- | --- | --- |\n|  | accepting replications | an IF > mean IF which | an IF < mean which | reporting an IF and which |\n|  | (Total number of | accept replications (Total | accept replications (Total | accept replications (Total |\n|  | journals, in brackets) | number of journals with | number of journals with | number of journals not publicly |\n|  |  | IF > mean, in brackets) | IF < mean, in brackets) | reporting an IF, in brackets) |\n| General | 4 (103) | 2 (30) | 2 (47) | 0 (26) |\n| Cognitive | 5 (123) | 2 (39) | 3 (64) | 0 (20) |\n| Social | 4 (93) | 1 (14) | 2 (56) | 1 (23) |\n| Clinical | 0 (78) | 0 (25) | 0 (35) | 0 (18) |\n| Developmental | 2 (130) | 0 (36) | 1 (61) | 1 (33) |\n| Sports |",
    "is_useful": true,
    "question": "What is the acceptance rate of replication studies across different branches of psychology journals?"
  },
  {
    "text": "| 1 (6) |\n| Cross-cultural/Minority psychology | 2 (46) | 0 (0) | 2 (12) | 0 (34) |\n| Forensic | 0 (53) | 0 (9) | 0 (27) | 0 (17) |\n| Biological/Neuropsychology | 3 (67) | 0 (31) | 3 (24) | 0 (12) |\n| Evolutionary psychology | 0 (7) | 0 (2) | 0 (1) | 0 (4) |\n| Total | 33 (1151) | 11 (269) | 17 (515) | 5 (367) |\n\n(e.g. Kerr et al. 1977; Neuliep and Crandall, 1990, 1993a,b; Makel et al. 2012). The findings from this study extend previous work by demonstrating that journals' stated editorial practices do not explicitly encourage the submission of replications. Of course, it is possible that such editorial practices actually reflect publishers' practices and wishes, rather than those of the editors. Academic publishing is a commercial enterprise, as well as an academic one, and publishers may wish to see published in their journals findings that are unique and meaningful and which will increase the journals' attractiveness, submission rates and, therefore, sales. No study has examined publishers' views of replication and this might be an avenue of research for others to consider.",
    "is_useful": true,
    "question": "How do editorial practices in academic publishing influence the submission of replication studies? "
  },
  {
    "text": "0 (4) |\n| Total | 33 (1151) | 11 (269) | 17 (515) | 5 (367) |\n\n(e.g. Kerr et al. 1977; Neuliep and Crandall, 1990, 1993a,b; Makel et al. 2012). The findings from this study extend previous work by demonstrating that journals' stated editorial practices do not explicitly encourage the submission of replications. Of course, it is possible that such editorial practices actually reflect publishers' practices and wishes, rather than those of the editors. Academic publishing is a commercial enterprise, as well as an academic one, and publishers may wish to see published in their journals findings that are unique and meaningful and which will increase the journals' attractiveness, submission rates and, therefore, sales. No study has examined publishers' views of replication and this might be an avenue of research for others to consider.\n\nOur data also reflect the aims and scope of journal editorial policies which explicitly mention replications and encourage the submission of papers which constitute replications. The conclusion cannot be drawn that 1, 218 journals do not or would not accept replications. However, it is noteworthy that, despite the establishment of the Open Science Framework in 2011, the number of special issues of journals in psychology dedicated to replications in recent years, and the controversy over psychology's \"replication crisis,\" that so few journals in Psychology explicitly encourage the submission of replications and that 379 journals emphasize the originality of the research in their aims and scope.",
    "is_useful": true,
    "question": "How do journal editorial practices affect the submission and acceptance of replication research in academic publishing?"
  },
  {
    "text": "No study has examined publishers' views of replication and this might be an avenue of research for others to consider.\n\nOur data also reflect the aims and scope of journal editorial policies which explicitly mention replications and encourage the submission of papers which constitute replications. The conclusion cannot be drawn that 1, 218 journals do not or would not accept replications. However, it is noteworthy that, despite the establishment of the Open Science Framework in 2011, the number of special issues of journals in psychology dedicated to replications in recent years, and the controversy over psychology's \"replication crisis,\" that so few journals in Psychology explicitly encourage the submission of replications and that 379 journals emphasize the originality of the research in their aims and scope. If it is agreed that that science proceeds by self-correction and that it can only progress by demonstrating that its effects and phenomena are valid and can be reliably produced, it is surprising that so few journals appear to reflect or embody this agreement explicitly in their editorial practices. An analysis of other disciplines' practices is underway to determine whether the data are reflective only of editorial practices in psychology or are common in other sciences. It is also worth acknowledging that this study was conducted in the Summer of 2015. A repeat survey of the same journals (and any new journals) in 2017 might be informative.\n\nThe issue of failing to accept replications is also tied to the issue of publication bias- the publication of positive results and the discouragement of the publication of negative results: arguably, the Scylla and Charybdis of psychological science in the early 21st century.",
    "is_useful": true,
    "question": "What are the challenges regarding the acceptance of replication studies in scientific publishing, particularly in the field of psychology?"
  },
  {
    "text": "If it is agreed that that science proceeds by self-correction and that it can only progress by demonstrating that its effects and phenomena are valid and can be reliably produced, it is surprising that so few journals appear to reflect or embody this agreement explicitly in their editorial practices. An analysis of other disciplines' practices is underway to determine whether the data are reflective only of editorial practices in psychology or are common in other sciences. It is also worth acknowledging that this study was conducted in the Summer of 2015. A repeat survey of the same journals (and any new journals) in 2017 might be informative.\n\nThe issue of failing to accept replications is also tied to the issue of publication bias- the publication of positive results and the discouragement of the publication of negative results: arguably, the Scylla and Charybdis of psychological science in the early 21st century. As Smart (1964, p. 232) stated, \"withholding negative results from publication has a repressive effect on scientific development.\"\n\nHowever, we would suggest that there are reasonably straightforward solutions to this problem and a number of specific solutions were suggested by Begley and Ellis (2012) in their discussion of replication failure in preclinical cancer trials. For example, Begley and Ellis (2012) recommend that there should be more opportunities to present negative results. It should be an \"expectation\" that negative results should be presented in conferences and in publications and that \"investigators should be required to report all findings regardless of outcome\" Begley and Ellis (2012, p.",
    "is_useful": true,
    "question": "What challenges does scientific research face regarding the publication of negative results and replication, and what solutions have been proposed to address these issues?"
  },
  {
    "text": "The issue of failing to accept replications is also tied to the issue of publication bias- the publication of positive results and the discouragement of the publication of negative results: arguably, the Scylla and Charybdis of psychological science in the early 21st century. As Smart (1964, p. 232) stated, \"withholding negative results from publication has a repressive effect on scientific development.\"\n\nHowever, we would suggest that there are reasonably straightforward solutions to this problem and a number of specific solutions were suggested by Begley and Ellis (2012) in their discussion of replication failure in preclinical cancer trials. For example, Begley and Ellis (2012) recommend that there should be more opportunities to present negative results. It should be an \"expectation\" that negative results should be presented in conferences and in publications and that \"investigators should be required to report all findings regardless of outcome\" Begley and Ellis (2012, p. 533), a suggestion which has given rise to the encouragement of pre-registered reports whereby researchers state in advance the full details of their method (including conditions and numbers of participants required) and planned statistical analysis and do not deviate from this plan. They argue that funding agencies must agree that negative results can be just as informative and as valuable as positive results.\n\nAnother solution would be for any author who submits a statistically significant empirical study for publication also conducts and submits along with the original study at least one replication, whether this replication is statistically significant or\n\nnot. Such reports might even be pre-registered.",
    "is_useful": true,
    "question": "What are some proposed solutions to the issue of publication bias and replication failure in scientific research?"
  },
  {
    "text": "For example, Begley and Ellis (2012) recommend that there should be more opportunities to present negative results. It should be an \"expectation\" that negative results should be presented in conferences and in publications and that \"investigators should be required to report all findings regardless of outcome\" Begley and Ellis (2012, p. 533), a suggestion which has given rise to the encouragement of pre-registered reports whereby researchers state in advance the full details of their method (including conditions and numbers of participants required) and planned statistical analysis and do not deviate from this plan. They argue that funding agencies must agree that negative results can be just as informative and as valuable as positive results.\n\nAnother solution would be for any author who submits a statistically significant empirical study for publication also conducts and submits along with the original study at least one replication, whether this replication is statistically significant or\n\nnot. Such reports might even be pre-registered. It is arguable that some journals such as Journal of Personality and Social Psychology and some from the JEP stable have historically published protracted series of studies within the same paper. This is true, although these portfolio of studies either conceptually replicate the results of the main study or develop studies which expand on the results of the original study. Our proposal is that each second study should be a direct replication with a different sample which meets the recruitment criteria of the original study.",
    "is_useful": true,
    "question": "What are some recommended practices for reporting negative results in scientific research?"
  },
  {
    "text": "They argue that funding agencies must agree that negative results can be just as informative and as valuable as positive results.\n\nAnother solution would be for any author who submits a statistically significant empirical study for publication also conducts and submits along with the original study at least one replication, whether this replication is statistically significant or\n\nnot. Such reports might even be pre-registered. It is arguable that some journals such as Journal of Personality and Social Psychology and some from the JEP stable have historically published protracted series of studies within the same paper. This is true, although these portfolio of studies either conceptually replicate the results of the main study or develop studies which expand on the results of the original study. Our proposal is that each second study should be a direct replication with a different sample which meets the recruitment criteria of the original study. Of course, this proposal is also open to the valid criticism that an internal (if direct) replication attempt will lead to a positive result because the literature notes that replications are more successful if conducted by the same team as the original study. However, this would be one step toward (i) ensuring that the importance of replication is made salient in the actions and plans of researchers and (ii) providing some sort of reliability check on the original study.\n\nWe might also suggest a more radical and straightforward solution. We would recommend that all journals in psychology\n\n### REFERENCES\n\n- American Psychological Society (2015). Replication in psychological science. Psychol. Sci. 26, 1827\u20131832.",
    "is_useful": true,
    "question": "What are some proposed solutions to enhance the reliability and value of research findings in scientific studies?"
  },
  {
    "text": "Our proposal is that each second study should be a direct replication with a different sample which meets the recruitment criteria of the original study. Of course, this proposal is also open to the valid criticism that an internal (if direct) replication attempt will lead to a positive result because the literature notes that replications are more successful if conducted by the same team as the original study. However, this would be one step toward (i) ensuring that the importance of replication is made salient in the actions and plans of researchers and (ii) providing some sort of reliability check on the original study.\n\nWe might also suggest a more radical and straightforward solution. We would recommend that all journals in psychology\n\n### REFERENCES\n\n- American Psychological Society (2015). Replication in psychological science. Psychol. Sci. 26, 1827\u20131832. doi: 10.1177/0956797615616374\n- Begley, C. G., and Ellis, L. M. (2012). Improve standards for preclinical cancer research. Nature 483, 531\u2013533. doi: 10.1038/483531a\n- Bozarth, J. D., and Roberts, R. R. (1972). Signifying significant significance. Am. Psychol. 27, 774\u2013775. doi: 10.1037/h0038034\n- Brown, S. W., and Coney, K. A. (1976). Building a replication tradition in marketing.",
    "is_useful": true,
    "question": "What proposal is suggested for improving the reliability of research findings in psychological science?"
  },
  {
    "text": "Econ. 30, 70\u201381.\n- Hubbard, R., and Vetter, D. E. (1996). An empirical comparison of published replication research in accounting, economics, finance, management, and marketing. J. Bus. Res. 35, 153\u2013164. doi: 10.1016/0148-2963(95) 00084-4\n- Ioannidis, J. P. A. (2005). Contradicted and initially stronger effects in highly cited clinical research. JAMA 294, 218\u2013228. doi: 10.1001/jama.294.2.218\n- Jones, K. S., Derby, P. L., and Schmidlin, E. A. (2010). An investigation of the prevalence of replication research in human factors. Hum. Factors 52, 586\u2013595. doi: 10.1177/0018720810384394\n\n(1) explicitly state that they accept the submission of replications and (2) explicitly state that they accept replications which report negative results. That is, these two recommendations should be embedded in the aims and the instructions to authors of all psychology journals. If such recommendations were to become the norm and encourage and make common the practice of publishing replicated or negative work, psychology could demonstrably put its house in order. Where psychology leads, other disciplines could follow.",
    "is_useful": true,
    "question": "What are the suggested practices for psychology journals to improve the field's research integrity?"
  },
  {
    "text": "JAMA 294, 218\u2013228. doi: 10.1001/jama.294.2.218\n- Jones, K. S., Derby, P. L., and Schmidlin, E. A. (2010). An investigation of the prevalence of replication research in human factors. Hum. Factors 52, 586\u2013595. doi: 10.1177/0018720810384394\n\n(1) explicitly state that they accept the submission of replications and (2) explicitly state that they accept replications which report negative results. That is, these two recommendations should be embedded in the aims and the instructions to authors of all psychology journals. If such recommendations were to become the norm and encourage and make common the practice of publishing replicated or negative work, psychology could demonstrably put its house in order. Where psychology leads, other disciplines could follow.\n\n### AUTHOR CONTRIBUTIONS\n\nGNM devised the original idea and method; RC conducted the research and completed the analysis; GNM wrote the first draft of the paper; RC revised this draft; both authors contributed to revisions. Both consider themselves full authors of the paper, and the order of authorship reflects the input provided.\n\n- Kerr, S., Tolliver, J., and Petree, D. (1977). Manuscript characteristics which influence acceptance for management and social science journals. Acad. Manag. J. 20, 132\u2013141.",
    "is_useful": true,
    "question": "What recommendations could help increase the prevalence of publishing replication studies and negative results in psychology journals?"
  },
  {
    "text": "That is, these two recommendations should be embedded in the aims and the instructions to authors of all psychology journals. If such recommendations were to become the norm and encourage and make common the practice of publishing replicated or negative work, psychology could demonstrably put its house in order. Where psychology leads, other disciplines could follow.\n\n### AUTHOR CONTRIBUTIONS\n\nGNM devised the original idea and method; RC conducted the research and completed the analysis; GNM wrote the first draft of the paper; RC revised this draft; both authors contributed to revisions. Both consider themselves full authors of the paper, and the order of authorship reflects the input provided.\n\n- Kerr, S., Tolliver, J., and Petree, D. (1977). Manuscript characteristics which influence acceptance for management and social science journals. Acad. Manag. J. 20, 132\u2013141. doi: 10.2307/255467\n- Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B. Jr., Bahn\u00edk, \u0160., Bernstein, M. J., et al. (2014). Investigating variation in replicability. Soc. Psychol. 45, 142\u2013152. doi: 10.1027/1864-9335/a000178\n- Laws, K. R. (2013). Negativland-a home for all findings in psychology. BMC Psychol.",
    "is_useful": true,
    "question": "What practices could enhance the reliability and transparency of research in psychology and potentially influence other disciplines?"
  },
  {
    "text": "Contents lists available at ScienceDirect\n\n![](_page_0_Picture_2.jpeg)\n\nJournal of Economic Psychology\n\njournal homepage: www.elsevier.com/locate/joep\n\n![](_page_0_Picture_5.jpeg)\n\nT\n\n# Preregistration and reproducibility\u2606\n\n# Eirik Str\u00f8mland\n\n*Department of Economics, University of Bergen, Hermann Fossgate 6, 5007 Bergen, Norway*\n\n#### ABSTRACT\n\nMany view preregistration as a promising way to improve research credibility. However, scholars have argued that using pre-analysis plans in Experimental Economics has limited benefits. This paper argues that preregistration of studies is likely to improve research credibility. I show that in a setting with selective reporting and low statistical power, effect sizes are highly inflated, and this translates into low reproducibility. Preregistering the original studies could avoid such inflation of effect sizes\u2014through increasing the share of \"frequentist\" researchers\u2014and would lead to more credible power analyses for replication studies. Numerical applications of the model indicate that the inflation bias could be very large in practice, and available empirical evidence is in line with the central assumptions of the model.\n\n# **1. Introduction**\n\nFrequentist statistics rely on the assumption that the analysis of an experiment is independent of how the results turn out. If we regard the estimates as a random draw from some fixed population model, the parameter estimates and p-values will, on average, be correct (Neyman, 1937).",
    "is_useful": true,
    "question": "What method is suggested as a way to enhance the credibility of research and address issues of inflated effect sizes in studies?"
  },
  {
    "text": "This paper argues that preregistration of studies is likely to improve research credibility. I show that in a setting with selective reporting and low statistical power, effect sizes are highly inflated, and this translates into low reproducibility. Preregistering the original studies could avoid such inflation of effect sizes\u2014through increasing the share of \"frequentist\" researchers\u2014and would lead to more credible power analyses for replication studies. Numerical applications of the model indicate that the inflation bias could be very large in practice, and available empirical evidence is in line with the central assumptions of the model.\n\n# **1. Introduction**\n\nFrequentist statistics rely on the assumption that the analysis of an experiment is independent of how the results turn out. If we regard the estimates as a random draw from some fixed population model, the parameter estimates and p-values will, on average, be correct (Neyman, 1937). In contrast, if the data analysis is contingent on the data, the p-values and parameter estimates are hard to interpret (Gelman & Loken, 2014; Simmons, Nelson, & Simonsohn, 2011). A way to ensure that the frequentist assumption holds is to tie our hands in advance \u2014 preregister the choices to be made before the data have been seen.\n\nResearchers have claimed that preregistering studies is a major step toward greater research credibility (Munaf\u00f2 et al., 2017; Nosek, Ebersole, DeHaven, & Mellor, 2018). However, the literature offers little justification for the use of preregistration in Experimental Economics.",
    "is_useful": true,
    "question": "How can preregistration of studies enhance research credibility in the context of frequentist statistics?"
  },
  {
    "text": "If we regard the estimates as a random draw from some fixed population model, the parameter estimates and p-values will, on average, be correct (Neyman, 1937). In contrast, if the data analysis is contingent on the data, the p-values and parameter estimates are hard to interpret (Gelman & Loken, 2014; Simmons, Nelson, & Simonsohn, 2011). A way to ensure that the frequentist assumption holds is to tie our hands in advance \u2014 preregister the choices to be made before the data have been seen.\n\nResearchers have claimed that preregistering studies is a major step toward greater research credibility (Munaf\u00f2 et al., 2017; Nosek, Ebersole, DeHaven, & Mellor, 2018). However, the literature offers little justification for the use of preregistration in Experimental Economics. Based on a paper by Brodeur, L\u00e9, Sangnier, and Zylberberg (2016), several prominent researchers have claimed that Experimental Economics seems to suffer less from p-hacking \u2013 selective reporting of statistically significant results \u2013 than other applied fields in Economics (Camerer et al., 2016; Coffman & Niederle, 2015; Maniadis, Tufano, & List, 2017). If researchers do not selectively report statistically significant findings, there is no reason for advocating for policies that will force them to tie their hands in advance of the data analysis.",
    "is_useful": true,
    "question": "What is the significance of preregistration in the context of research credibility and statistical analysis?"
  },
  {
    "text": "Researchers have claimed that preregistering studies is a major step toward greater research credibility (Munaf\u00f2 et al., 2017; Nosek, Ebersole, DeHaven, & Mellor, 2018). However, the literature offers little justification for the use of preregistration in Experimental Economics. Based on a paper by Brodeur, L\u00e9, Sangnier, and Zylberberg (2016), several prominent researchers have claimed that Experimental Economics seems to suffer less from p-hacking \u2013 selective reporting of statistically significant results \u2013 than other applied fields in Economics (Camerer et al., 2016; Coffman & Niederle, 2015; Maniadis, Tufano, & List, 2017). If researchers do not selectively report statistically significant findings, there is no reason for advocating for policies that will force them to tie their hands in advance of the data analysis. Moreover, Coffman and Niederle (2015) make a theoretical argument that preregistration of studies offers limited benefits, except for conducting replication studies. They argue that preregistration would have a small effect on the conditional probability that published significant findings are true, and that scientific beliefs typically rapidly converge to the truth even in absence of preregistration.\n\nThis paper offers a theoretical justification for preregistration in Experimental Economics and argues that available empirical evidence supports this theoretical account.",
    "is_useful": true,
    "question": "What are the arguments for and against the use of preregistration in Experimental Economics?"
  },
  {
    "text": "If researchers do not selectively report statistically significant findings, there is no reason for advocating for policies that will force them to tie their hands in advance of the data analysis. Moreover, Coffman and Niederle (2015) make a theoretical argument that preregistration of studies offers limited benefits, except for conducting replication studies. They argue that preregistration would have a small effect on the conditional probability that published significant findings are true, and that scientific beliefs typically rapidly converge to the truth even in absence of preregistration.\n\nThis paper offers a theoretical justification for preregistration in Experimental Economics and argues that available empirical evidence supports this theoretical account. I present a simple model in which the research community is populated by agents who may report their results unconditionally or conditionally on statistical significance and show that greater reliance on preregistration improves the estimation of the effect sizes through increasing the share of \"frequentist\" researchers. As replicators are likely to estimate statistical power based on the published effect sizes, preregistration is therefore also expected to improve reproducibility rates. Numerical illustrations show that the bias from effect size inflation may be very large in practice.\n\nThe model assumes that researchers access many possible tests of the same hypothesis, that some fraction in the population selectively report their main findings and that the statistical power is low on average. Empirical evidence seems to support these assumptions. First, a\n\n*E-mail address:* Eirik.Stromland@uib.no.",
    "is_useful": true,
    "question": "What are the theoretical justifications and potential benefits of preregistration in research, particularly in Experimental Economics?"
  },
  {
    "text": "This paper offers a theoretical justification for preregistration in Experimental Economics and argues that available empirical evidence supports this theoretical account. I present a simple model in which the research community is populated by agents who may report their results unconditionally or conditionally on statistical significance and show that greater reliance on preregistration improves the estimation of the effect sizes through increasing the share of \"frequentist\" researchers. As replicators are likely to estimate statistical power based on the published effect sizes, preregistration is therefore also expected to improve reproducibility rates. Numerical illustrations show that the bias from effect size inflation may be very large in practice.\n\nThe model assumes that researchers access many possible tests of the same hypothesis, that some fraction in the population selectively report their main findings and that the statistical power is low on average. Empirical evidence seems to support these assumptions. First, a\n\n*E-mail address:* Eirik.Stromland@uib.no.\n\nhttps://doi.org/10.1016/j.joep.2019.01.006\n\nReceived 24 May 2018; Received in revised form 24 January 2019; Accepted 25 January 2019\n\nAvailable online 06 February 2019\n\n0167-4870/ \u00a9 2019 The Author. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/BY/4.0/).",
    "is_useful": true,
    "question": "What is the role of preregistration in improving effect size estimation and reproducibility in experimental research?"
  },
  {
    "text": "Numerical illustrations show that the bias from effect size inflation may be very large in practice.\n\nThe model assumes that researchers access many possible tests of the same hypothesis, that some fraction in the population selectively report their main findings and that the statistical power is low on average. Empirical evidence seems to support these assumptions. First, a\n\n*E-mail address:* Eirik.Stromland@uib.no.\n\nhttps://doi.org/10.1016/j.joep.2019.01.006\n\nReceived 24 May 2018; Received in revised form 24 January 2019; Accepted 25 January 2019\n\nAvailable online 06 February 2019\n\n0167-4870/ \u00a9 2019 The Author. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/BY/4.0/).\n\n<sup>\u2606</sup> I am grateful to Anna Dreber Almenberg, Rune Jansen Hagen, Amanda Kvarven, Bj\u00f8rn Sandvik, Nina Serdarevic, Erik \u00d8. S\u00f8rensen, Sigve Tj\u00f8tta, Fabio Tufano and two anonymous referees for helpful comments and discussions.\n\nre-analysis of Brodeur et al. (2016) suggests similar patterns of p-hacking in Experimental Economics as in other applied fields.",
    "is_useful": true,
    "question": "What issues related to publication bias and statistical power are highlighted in the context of open science?"
  },
  {
    "text": "Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/BY/4.0/).\n\n<sup>\u2606</sup> I am grateful to Anna Dreber Almenberg, Rune Jansen Hagen, Amanda Kvarven, Bj\u00f8rn Sandvik, Nina Serdarevic, Erik \u00d8. S\u00f8rensen, Sigve Tj\u00f8tta, Fabio Tufano and two anonymous referees for helpful comments and discussions.\n\nre-analysis of Brodeur et al. (2016) suggests similar patterns of p-hacking in Experimental Economics as in other applied fields. Second, recent attempts to quantify statistical power in Economics (Ioannidis, Stanley, & Doucouliagos, 2017; Zhang & Ortmann, 2013) suggest that studies tend to be underpowered, and attempts to estimate the \"inflation bias,\" the extent to which published effects are overestimated, show that the published effects are inflated by a factor of two or more (Ioannidis et al., 2017). The latter estimate is very close to the \"worst-case\" theoretical magnitude of the inflation bias suggested in the numerical applications of the model.\n\nThis paper contributes to a small but growing number of studies in the intersection between \"meta-research\" \u2013 the field of research that studies the institutions and practices of scientific research (Ioannidis, 2018) \u2013 and economic theory (Gall, Ioannidis, & Maniadis, 2017; Gall & Maniadis, 2019; Maniadis et al., 2017).",
    "is_useful": true,
    "question": "What issues related to statistical power and publication biases are discussed in the context of economic research?"
  },
  {
    "text": "The latter estimate is very close to the \"worst-case\" theoretical magnitude of the inflation bias suggested in the numerical applications of the model.\n\nThis paper contributes to a small but growing number of studies in the intersection between \"meta-research\" \u2013 the field of research that studies the institutions and practices of scientific research (Ioannidis, 2018) \u2013 and economic theory (Gall, Ioannidis, & Maniadis, 2017; Gall & Maniadis, 2019; Maniadis et al., 2017). Specifically, this paper is an application of Ioannidis's (2008) and Gelman and Carlin's (2014) ideas to Economics. These papers showed that even if the null hypothesis is false, selective reporting and low statistical power together lead to highly exaggerated published effect sizes that, in turn, lower reproducibility. Specifically, the contribution of this paper is to put these ideas into a simple economic model and show that a move toward preregistration within the context of the model may improve reproducibility through a shift to an equilibrium with a higher share of \"frequentist\" researchers.\n\nAlthough there are several applications based on Ioannidis (2005) in Economics (Coffman & Niederle, 2015; Maniadis et al., 2014, 2017), there are, to my knowledge, no applications of the framework presented in Ioannidis (2008). This distinction is important in practice.",
    "is_useful": true,
    "question": "What is one proposed method to improve reproducibility in scientific research according to recent studies in meta-research and economic theory?"
  },
  {
    "text": "Specifically, this paper is an application of Ioannidis's (2008) and Gelman and Carlin's (2014) ideas to Economics. These papers showed that even if the null hypothesis is false, selective reporting and low statistical power together lead to highly exaggerated published effect sizes that, in turn, lower reproducibility. Specifically, the contribution of this paper is to put these ideas into a simple economic model and show that a move toward preregistration within the context of the model may improve reproducibility through a shift to an equilibrium with a higher share of \"frequentist\" researchers.\n\nAlthough there are several applications based on Ioannidis (2005) in Economics (Coffman & Niederle, 2015; Maniadis et al., 2014, 2017), there are, to my knowledge, no applications of the framework presented in Ioannidis (2008). This distinction is important in practice. Coffman and Niederle (2015) follow Ioannidis (2005) model setup, where hypotheses are modeled as either \"true\" or \"false,\" and conclude that preregistering studies offers little benefit to experimental economists because it has little effect on the probability that a significant finding is true, and beliefs tend to converge to the truth even in absence of preregistration of studies. In contrast, I adopt the focus on effect sizes taken by Ioannidis (2008) and reach the opposite conclusion.",
    "is_useful": true,
    "question": "How can preregistration contribute to improving reproducibility in economic research?"
  },
  {
    "text": "Although there are several applications based on Ioannidis (2005) in Economics (Coffman & Niederle, 2015; Maniadis et al., 2014, 2017), there are, to my knowledge, no applications of the framework presented in Ioannidis (2008). This distinction is important in practice. Coffman and Niederle (2015) follow Ioannidis (2005) model setup, where hypotheses are modeled as either \"true\" or \"false,\" and conclude that preregistering studies offers little benefit to experimental economists because it has little effect on the probability that a significant finding is true, and beliefs tend to converge to the truth even in absence of preregistration of studies. In contrast, I adopt the focus on effect sizes taken by Ioannidis (2008) and reach the opposite conclusion. A move toward more preregistration will, in general, improve the credibility and reproducibility of published results through reducing effect size inflation and improve power estimates based on published effect sizes. Importantly, Bayesian updating in this setting rapidly leads to convergence on a true belief that the null hypothesis is false both when effects are inflated and unbiased \u2014 showing that correct belief formation is not necessarily an argument against preregistration of studies. Thus, this paper is in line with the recommendation not to always treat research evidence as \"true\" or \"false\"(McShane & Gal, 2017): A finding may be \"true\" but still misleading.\n\nThis paper proceeds as follows.",
    "is_useful": true,
    "question": "What impact does preregistration have on the credibility and reproducibility of research findings in experimental economics?"
  },
  {
    "text": "In contrast, I adopt the focus on effect sizes taken by Ioannidis (2008) and reach the opposite conclusion. A move toward more preregistration will, in general, improve the credibility and reproducibility of published results through reducing effect size inflation and improve power estimates based on published effect sizes. Importantly, Bayesian updating in this setting rapidly leads to convergence on a true belief that the null hypothesis is false both when effects are inflated and unbiased \u2014 showing that correct belief formation is not necessarily an argument against preregistration of studies. Thus, this paper is in line with the recommendation not to always treat research evidence as \"true\" or \"false\"(McShane & Gal, 2017): A finding may be \"true\" but still misleading.\n\nThis paper proceeds as follows. Section 2 presents a simple economic model of the reporting process of main experimental findings and shows that given this model setup, greater reliance on preregistration would improve the quality of our estimates of treatment effects and improve reproducibility through more accurate power estimates. Using numerical illustrations for a specific assumed probability distribution, I show that the bias in the absence of preregistration may be very large in practice and illustrate how belief formation is affected in different scenarios. In Section 3 I discuss how the key assumptions of the model fit with available empirical evidence. Finally, Section 4 concludes.\n\n#### **2. Theoretical framework**\n\n# *2.1. Inflation of effect sizes*\n\nSuppose researchers want to infer whether an effect exists ( 0 ) or not ( 0) = and aim to estimate the size of the true effect.",
    "is_useful": true,
    "question": "How does preregistration influence the credibility and reproducibility of published research results?"
  },
  {
    "text": "This paper proceeds as follows. Section 2 presents a simple economic model of the reporting process of main experimental findings and shows that given this model setup, greater reliance on preregistration would improve the quality of our estimates of treatment effects and improve reproducibility through more accurate power estimates. Using numerical illustrations for a specific assumed probability distribution, I show that the bias in the absence of preregistration may be very large in practice and illustrate how belief formation is affected in different scenarios. In Section 3 I discuss how the key assumptions of the model fit with available empirical evidence. Finally, Section 4 concludes.\n\n#### **2. Theoretical framework**\n\n# *2.1. Inflation of effect sizes*\n\nSuppose researchers want to infer whether an effect exists ( 0 ) or not ( 0) = and aim to estimate the size of the true effect. Each researcher accesses multiple possible choices of test results of the point null hypothesis that = 0 and parameter estimates from some set and decides which of these possible choices to report as her main finding. The set can be considered a set of realized draws from some underlying \"latent\" joint distribution of estimates and test results. We denote by f ( , | ) t the latent joint distribution of the test results and estimates with correlation , conditional on the true value , which is normalized to be positive. The estimate may be thought of as measuring some treatment effect of interest in a randomized experiment, which ensures that the unconditional expectation of is equal to the true value .",
    "is_useful": true,
    "question": "How can preregistration impact the quality of treatment effect estimates and reproducibility in research?"
  },
  {
    "text": "Each researcher accesses multiple possible choices of test results of the point null hypothesis that = 0 and parameter estimates from some set and decides which of these possible choices to report as her main finding. The set can be considered a set of realized draws from some underlying \"latent\" joint distribution of estimates and test results. We denote by f ( , | ) t the latent joint distribution of the test results and estimates with correlation , conditional on the true value , which is normalized to be positive. The estimate may be thought of as measuring some treatment effect of interest in a randomized experiment, which ensures that the unconditional expectation of is equal to the true value . Denote by V the number of available tests for which | | t t > \u00af k (the test is statistically significant) and denote by *Pr t t* (| | < \u00af k ) the type II error probability for test k . For K independent tests, the probability of at least one correct rejection is\n\n$$\\Pr(V\\geq1|\\ \\theta^{\\circ})=1-\\prod_{k=1}^{K}\\Pr(\\{k_{k}\\ |\\ <t\\ |\\ \\theta^{\\circ}\\},\\tag{1}$$\n\nwhere < = Pr(| | t t\u00af ) k K k 1 is the joint probability of only making type II errors, which converges to zero as K . 1 We will assume K is sufficiently large so that Pr( 1| ) 1.",
    "is_useful": true,
    "question": "What factors influence the reporting of main findings from test results in research regarding the null hypothesis?"
  },
  {
    "text": "The expected value of the effect across researchers who report unconditionally is then E( k) = whereas the expected value of the effect across researchers who report conditionally on the test passing the threshold of statistical significance is written as : E( = > |t t\u00af ) T k k . The assumptions behind the selective reporting strategy conform to standard models of publication selection that assume selective reporting follows a one-sided form of \"incidental truncation\" where the reported effect is selected subject to the value of a test statistic passing some threshold in a desirable direction (Stanley & Doucouliagos, 2013).\n\nI assume that the expected utility gain from choosing a strategy of selective reporting, relative to choosing an unconditional reporting rule, is u b = . Here, b + [b b \u00af , \u00af] is a fixed material benefit parameter, and + [ \u00af , \u00af] is the subjective cost of the selective reporting strategy, with corresponding cumulative distribution function G( ) . The term b may be regarded as a fixed incentive parameter that captures the fact that behaving according to a selective reporting rule may benefit researchers in material terms through the increased chance of a good publication, while is the subjective cost component that captures that some researchers may experience a cost from deviating from an unconditional reporting rule.3 While b in reality will be a function of a vector of policy variables, *b b b* : [ D \u00af , \u00af] , we will for notational convenience treat it as a parameter that is set exogenously by an agent outside of the model.",
    "is_useful": true,
    "question": "What are the expected values of effect reported by researchers under unconditional and conditional reporting strategies, and what assumptions underpin the strategy of selective reporting?"
  },
  {
    "text": "For this reason, I choose the neutral term \"frequentist\" for a researcher who reports unconditionally and I choose not to put a particular label on researchers who report conditionally on statistical significance.\n\nIn the setup above, researchers may choose a strategy of selective reporting and gain b or choose to report their main result unconditionally on the observed data and gain zero. Thus, researchers will choose to be \"frequentist\" if > b . We denote by the subjective cost parameter for the marginal agent who is indifferent between the two strategies ( b = ) so that in equilibrium the share of frequentist researchers is determined by the complementary cumulative distribution function G\u00af ( ) Pr( = > ). A fraction G\u00af ( ) chooses to report unconditionally, and a fraction 1 G\u00af ( ) chooses a strategy of selective reporting. Note that an increase in b\u2014increasing the incentives to choose a strategy of selective reporting \u2014lowers the share of agents choosing to engage in \"frequentist\" reporting of results ( < 0 G b \u00af ( ) ).\n\nThe mean effect reported in the literature is a weighted sum of the expected estimates reported by the two types of researchers. We can express the mean effect as follows, where G\u00af ( ) is the share of \"frequentist\" researchers:\n\n$$\\bar{\\theta}=\\bar{G}(\\dot{\\omega})\\theta^{*}+[1-\\bar{G}(\\dot{\\omega})]\\theta^{T}.$$\n\nGiven the simple model outlined here, we may immediately establish an important result.\n\n**Proposition 1.** Suppose > 0 *.",
    "is_useful": true,
    "question": "What factors influence researchers' choices between frequentist and selective reporting strategies in their statistical analysis?"
  },
  {
    "text": "As G\u00af0 ( ) G\u00af1 ( ) 0 < *by assumption*, *the difference* ( ) 1 0 *is strictly negative*. *Thus*, *the inflation rate is strictly lower for* G\u00af ( ) > G\u00af 1 0 ( ).\n\nPreregistration policies could be seen as inducing an exogenous decrease in b, the expected benefit of selective reporting, and therefore a corresponding increase in G\u00af ( ) , improving parameter estimation. One example of such a preregistration policy is for a journal to adopt the format of a \"registered replication report\" where the journal commits to publishing based on a pre-analysis plan and reviewers make sure that this plan is followed (Simons, Holcombe, & Spellman, 2014).4 Such a policy would reduce b through lowering the set of journals that publish results based on a strategy of selective reporting, and would increase G\u00af ( ) as the critical would now be lower for an agent to prefer unconditional reporting over a strategy of selective reporting.\n\n<sup>3</sup> The model abstracts from strategic interaction among agents involved in the publication process, which would be a more realistic way of modeling the reporting process but is beyond the scope of the present paper.\n\n<sup>4</sup> An alternative model setup would be to assume that is endogenous; for instance, the aversion to selective reporting may be driven by social norms in the profession. In that case, one could obtain a move toward more preregistration through means other than changing the material incentives associated with selective reporting.",
    "is_useful": true,
    "question": "How can preregistration policies in scientific research affect selective reporting and parameter estimation?"
  },
  {
    "text": "<sup>3</sup> The model abstracts from strategic interaction among agents involved in the publication process, which would be a more realistic way of modeling the reporting process but is beyond the scope of the present paper.\n\n<sup>4</sup> An alternative model setup would be to assume that is endogenous; for instance, the aversion to selective reporting may be driven by social norms in the profession. In that case, one could obtain a move toward more preregistration through means other than changing the material incentives associated with selective reporting.\n\nAs the inflation bias is zero only for G\u00af ( ) 1 = , it also follows directly from the above that replicators who estimate the statistical power based on previously reported effect sizes will, on average, condition on a false assumption that the underlying effect is larger than the true effect . Therefore, replicators will in general tend to overestimate the statistical power of the replication study.5\n\nProposition 1 ensures that preregistration policies that reduce the incentives towards selective reporting lead to an improvement in estimation of effect sizes. However, this is not in itself informative about the size of the inflation bias. To illustrate how big the inflation bias may be in practice, I impose some specific distributional assumptions on the latent density f ( , | ) t and show the inflation bias numerically for different assumptions in the share of frequentist researchers in the population. I assume that each estimate is normally distributed; thus, N( , ) N k 2 2 where is a known standard deviation common to an experimental treatment and a control group, and is the true treatment effect in the study population.",
    "is_useful": true,
    "question": "How can preregistration policies impact the estimation of effect sizes in scientific research?"
  },
  {
    "text": "Therefore, replicators will in general tend to overestimate the statistical power of the replication study.5\n\nProposition 1 ensures that preregistration policies that reduce the incentives towards selective reporting lead to an improvement in estimation of effect sizes. However, this is not in itself informative about the size of the inflation bias. To illustrate how big the inflation bias may be in practice, I impose some specific distributional assumptions on the latent density f ( , | ) t and show the inflation bias numerically for different assumptions in the share of frequentist researchers in the population. I assume that each estimate is normally distributed; thus, N( , ) N k 2 2 where is a known standard deviation common to an experimental treatment and a control group, and is the true treatment effect in the study population. I assume a sample size of N = 100 in each treatment group in some initial study; thus, the total sample size is 200, which gives a statistical power of 20.5% for a true effect size of 8 percentage points ( =0.08) and a common standard deviation of = 0.5 assumed known to all researchers. In this setup, the formula for an incidentally truncated random variable yields T = + N 2 ( ) 1 ( ) 2 where (.) and (.) are the probability density function (PDF) and the cumulative distribution function (CDF) of the standard normal distribution, and is the value of the Z-statistic at which truncation occurs (Greene, 2012, p. 913).",
    "is_useful": true,
    "question": "How do preregistration policies impact the estimation of effect sizes in replication studies?"
  },
  {
    "text": "913). Inserting a truncation point of = [1.96 0.08/SE( )] for the Z-statistic that would make the estimate statistically significant I obtain = 0.1783. T 6\n\nI also display the results from hypothetical power calculations carried out by replicators who draw upon previously published results when determining their sample size. As the standard deviation is assumed common to the control group and the treatment group and known, I assume that the replicators estimate their necessary total sample size based on a z-test using the formula N = ( ) 2.8 \u00af 2 (e.g. Gelman & Hill, 2007, p. 443). The true required sample size is, however, calculated by inserting the true effect in the above formula. Table 1 displays the mean reported effect for different shares of frequentist researchers, the sample size estimated to yield 80% power of a replication and the true power given this estimated sample size.\n\nTable 1 shows that in a low-powered setting, the inflation bias is generally sizeable unless the share of frequentist researchers is very high. In the worst-case scenario, the reported effect will be, on average, 2.2 times as high as the true effect, and researchers who estimate the power of a replication study based on this effect will set a sample size that in reality gives them only 24% power.",
    "is_useful": true,
    "question": "What factors influence the estimation of sample size and power in replication studies in the context of open science?"
  },
  {
    "text": "Gelman & Hill, 2007, p. 443). The true required sample size is, however, calculated by inserting the true effect in the above formula. Table 1 displays the mean reported effect for different shares of frequentist researchers, the sample size estimated to yield 80% power of a replication and the true power given this estimated sample size.\n\nTable 1 shows that in a low-powered setting, the inflation bias is generally sizeable unless the share of frequentist researchers is very high. In the worst-case scenario, the reported effect will be, on average, 2.2 times as high as the true effect, and researchers who estimate the power of a replication study based on this effect will set a sample size that in reality gives them only 24% power. Thus, only 24% of replications would actually succeed in obtaining a statistically significant effect in the direction of the original study \u2013 one of the key measures of reproducibility used in recent replication studies (Camerer et al., 2016, 2018). Even when the share of frequentist reporting is high ( G\u00af ( ) 0.6 = ), the mean effect is still 1.5 times higher than the true effect, and the power is estimated to 80% with a sample size of 552 subjects while the true power is just 46.8%. Thus, in this low-powered setting a move toward full formal preregistration would lead to major improvement in the expected reproducibility rate.\n\nThe results shown in Table 1 depend on the assumptions made about statistical power.",
    "is_useful": true,
    "question": "What is the impact of low statistical power on the reproducibility of research findings?"
  },
  {
    "text": "Thus, only 24% of replications would actually succeed in obtaining a statistically significant effect in the direction of the original study \u2013 one of the key measures of reproducibility used in recent replication studies (Camerer et al., 2016, 2018). Even when the share of frequentist reporting is high ( G\u00af ( ) 0.6 = ), the mean effect is still 1.5 times higher than the true effect, and the power is estimated to 80% with a sample size of 552 subjects while the true power is just 46.8%. Thus, in this low-powered setting a move toward full formal preregistration would lead to major improvement in the expected reproducibility rate.\n\nThe results shown in Table 1 depend on the assumptions made about statistical power. If the statistical power in the original study is high (80%), then the inflation bias will generally be very small even in the worst-case scenario with only selective reporting. The reason is that for a constant and conventional test statistics (e.g. the t-test), an increase in sample size leads to higher values of the test statistic for lower values of the estimate. The truncated sampling distribution would then approach the \"frequentist\" sampling distribution as N , and even replicators who base their power estimates on selectively reported estimates would obtain quite accurate power estimates. Table A.1 in Appendix A illustrates how the conclusions in Table 1 change when we instead consider a scenario with statistical power equal to 80%; in this high-powered setting, there is little to gain from promoting preregistration policies as the scope for improvement of effect size estimation is small.",
    "is_useful": true,
    "question": "What is the estimated effect of implementing full formal preregistration on the reproducibility rate in low-powered research settings?"
  },
  {
    "text": "The results shown in Table 1 depend on the assumptions made about statistical power. If the statistical power in the original study is high (80%), then the inflation bias will generally be very small even in the worst-case scenario with only selective reporting. The reason is that for a constant and conventional test statistics (e.g. the t-test), an increase in sample size leads to higher values of the test statistic for lower values of the estimate. The truncated sampling distribution would then approach the \"frequentist\" sampling distribution as N , and even replicators who base their power estimates on selectively reported estimates would obtain quite accurate power estimates. Table A.1 in Appendix A illustrates how the conclusions in Table 1 change when we instead consider a scenario with statistical power equal to 80%; in this high-powered setting, there is little to gain from promoting preregistration policies as the scope for improvement of effect size estimation is small. However, it can be shown that as long as the statistical power is below about 50%, the sampling distribution of only statistically significant effects will be different enough from the unconditional sampling distribution that the difference between the two distributions will be large in practice (Gelman & Carlin, 2014).\n\n# *2.2. Belief formation*\n\nA possible argument against preregistration is that Bayesian updating rapidly leads to a true belief that 0 even in its absence (Coffman & Niederle, 2015). However, beliefs will gradually converge to a true belief on the hypothesis that 0 regardless of whether effect sizes are unbiased or inflated.",
    "is_useful": true,
    "question": "How does statistical power influence the accuracy of effect size estimation in research studies?"
  },
  {
    "text": "Table A.1 in Appendix A illustrates how the conclusions in Table 1 change when we instead consider a scenario with statistical power equal to 80%; in this high-powered setting, there is little to gain from promoting preregistration policies as the scope for improvement of effect size estimation is small. However, it can be shown that as long as the statistical power is below about 50%, the sampling distribution of only statistically significant effects will be different enough from the unconditional sampling distribution that the difference between the two distributions will be large in practice (Gelman & Carlin, 2014).\n\n# *2.2. Belief formation*\n\nA possible argument against preregistration is that Bayesian updating rapidly leads to a true belief that 0 even in its absence (Coffman & Niederle, 2015). However, beliefs will gradually converge to a true belief on the hypothesis that 0 regardless of whether effect sizes are unbiased or inflated. Thus, it does not follow from the fact that beliefs adjust quickly to the truth that replications can substitute for preregistration. To see this, I outline a simple extension of the model of belief formation considered in Coffman and Niederle (2015). The model is intended to describe a setting where no researchers preregister their studies, but they engage in conceptual replications of an initial study, maintaining their researcher degrees of freedom. Other researchers observe the results from each of these studies and update their beliefs using Bayesian updating.",
    "is_useful": true,
    "question": "What is the relationship between statistical power, effect size estimation, and preregistration policies in the context of open science?"
  },
  {
    "text": "# *2.2. Belief formation*\n\nA possible argument against preregistration is that Bayesian updating rapidly leads to a true belief that 0 even in its absence (Coffman & Niederle, 2015). However, beliefs will gradually converge to a true belief on the hypothesis that 0 regardless of whether effect sizes are unbiased or inflated. Thus, it does not follow from the fact that beliefs adjust quickly to the truth that replications can substitute for preregistration. To see this, I outline a simple extension of the model of belief formation considered in Coffman and Niederle (2015). The model is intended to describe a setting where no researchers preregister their studies, but they engage in conceptual replications of an initial study, maintaining their researcher degrees of freedom. Other researchers observe the results from each of these studies and update their beliefs using Bayesian updating. The model may not be accurate as a descriptive model of belief formation, but serves as a useful benchmark for showing that correct belief formation on a binary hypothesis may\n\n<sup>5</sup> This assumes that replicators estimate power based on the assumption that the underlying latent distribution that the data are drawn from belongs to a family of probability distributions satisfying the monotone likelihood ratio property, in which case the statistical power function will be increasing in the effect size. This will be the case for common parametric tests such as the t-test or the chi-squared test, but may not hold if replicators plan to use a non-parametric test and specifically try to estimate the power of that test.",
    "is_useful": true,
    "question": "What are the implications of belief formation in the context of preregistration and Bayesian updating for scientific research?"
  },
  {
    "text": "The model is intended to describe a setting where no researchers preregister their studies, but they engage in conceptual replications of an initial study, maintaining their researcher degrees of freedom. Other researchers observe the results from each of these studies and update their beliefs using Bayesian updating. The model may not be accurate as a descriptive model of belief formation, but serves as a useful benchmark for showing that correct belief formation on a binary hypothesis may\n\n<sup>5</sup> This assumes that replicators estimate power based on the assumption that the underlying latent distribution that the data are drawn from belongs to a family of probability distributions satisfying the monotone likelihood ratio property, in which case the statistical power function will be increasing in the effect size. This will be the case for common parametric tests such as the t-test or the chi-squared test, but may not hold if replicators plan to use a non-parametric test and specifically try to estimate the power of that test.\n\n<sup>6</sup> The effect size needs to be 1.96 standard errors away from zero to be statistically significant. As the estimate is assumed to be a mean treatment effect and the standard deviation is assumed to be known and common to both treatment groups, the standard error of the estimated effect is equal to N 2 2 , where N is the sample size in each group. In our context, the effect size needs to be at least 0.1386 to be statistically significant. Using the cumulative normal distribution, the probability of an estimate higher than 0.1386 is 20.5% for a true effect size = 0.08.",
    "is_useful": true,
    "question": "How does the model propose belief formation occurs among researchers who do not preregister their studies?"
  },
  {
    "text": "The results are displayed in Fig. 1.\n\nFig. 1 shows that for the scenario with full selective reporting of statistically significant findings ( G\u00af ( ) 0) = , beliefs converge rapidly to the true belief that 0 . Only three significant tests for the hypothesis are necessary for a posterior belief of 1 that the association under testing is true. Convergence to this belief is somewhat slower with full frequentist reporting ( G\u00af ( ) 1) = as the perceived power in this scenario will be equal to true power. Thus, one would expect belief formation to be slower in a scenario where all researchers preregistered their experiments.\n\nThis result shows that beliefs converge quickly on the truth even in a setting where the findings are severely inflated by selective reporting. Actually, beliefs converge faster the more severe selective reporting there is. Thus, the rapid convergence of beliefs does not tell us anything about the merits of preregistration: Beliefs will converge on the belief that 0 in the presence and absence of effect inflation, but the reproducibility rates in these two scenarios will be very different.7\n\n## **3. Discussion of assumptions**\n\nThe model and numerical illustrations make three crucial assumptions for effect size inflation to be large. First, it is assumed that there are sufficient researcher degrees of freedom such that researchers are always able to report their main estimate conditional on it being statistically significant. Second, it is assumed that individual tests are underpowered to detect the underlying effect . Finally, it is assumed that there is a sizeable fraction of researchers who actually engage in selective reporting of their main findings.",
    "is_useful": true,
    "question": "How does selective reporting of statistically significant findings affect beliefs about the truth of research findings?"
  },
  {
    "text": "This result shows that beliefs converge quickly on the truth even in a setting where the findings are severely inflated by selective reporting. Actually, beliefs converge faster the more severe selective reporting there is. Thus, the rapid convergence of beliefs does not tell us anything about the merits of preregistration: Beliefs will converge on the belief that 0 in the presence and absence of effect inflation, but the reproducibility rates in these two scenarios will be very different.7\n\n## **3. Discussion of assumptions**\n\nThe model and numerical illustrations make three crucial assumptions for effect size inflation to be large. First, it is assumed that there are sufficient researcher degrees of freedom such that researchers are always able to report their main estimate conditional on it being statistically significant. Second, it is assumed that individual tests are underpowered to detect the underlying effect . Finally, it is assumed that there is a sizeable fraction of researchers who actually engage in selective reporting of their main findings. These researchers preferentially report results that pass the threshold associated with statistical significance.\n\nThe assumption that researchers access enough possible tests so that at least one will be statistically significant may seem strong, but even in a simple economic experiment there is a large number of ways to test the null hypothesis of interest. For only 20 binary choices, there are more than a million ( 220) ways to analyze the same data (Ioannidis, 2018).",
    "is_useful": true,
    "question": "How does selective reporting affect the convergence of beliefs in scientific findings?"
  },
  {
    "text": "Discussion of assumptions**\n\nThe model and numerical illustrations make three crucial assumptions for effect size inflation to be large. First, it is assumed that there are sufficient researcher degrees of freedom such that researchers are always able to report their main estimate conditional on it being statistically significant. Second, it is assumed that individual tests are underpowered to detect the underlying effect . Finally, it is assumed that there is a sizeable fraction of researchers who actually engage in selective reporting of their main findings. These researchers preferentially report results that pass the threshold associated with statistical significance.\n\nThe assumption that researchers access enough possible tests so that at least one will be statistically significant may seem strong, but even in a simple economic experiment there is a large number of ways to test the null hypothesis of interest. For only 20 binary choices, there are more than a million ( 220) ways to analyze the same data (Ioannidis, 2018). The typical economic experiment involves many subjective choices: which treatment to focus on, whether to perform a parametric or non-parametric test, and if so, which one (e.g., linear or non-linear regression model, Mann-Whitney test, or robust rank-order test). The researcher also chooses what outcome variable(s) to focus on and whether to focus on the main sample or a subgroup (and if so, which subgroup), which control variables to add in a regression, and what to do with the standard errors.",
    "is_useful": true,
    "question": "What assumptions are necessary for significant effect size inflation in research outcomes?"
  },
  {
    "text": "These researchers preferentially report results that pass the threshold associated with statistical significance.\n\nThe assumption that researchers access enough possible tests so that at least one will be statistically significant may seem strong, but even in a simple economic experiment there is a large number of ways to test the null hypothesis of interest. For only 20 binary choices, there are more than a million ( 220) ways to analyze the same data (Ioannidis, 2018). The typical economic experiment involves many subjective choices: which treatment to focus on, whether to perform a parametric or non-parametric test, and if so, which one (e.g., linear or non-linear regression model, Mann-Whitney test, or robust rank-order test). The researcher also chooses what outcome variable(s) to focus on and whether to focus on the main sample or a subgroup (and if so, which subgroup), which control variables to add in a regression, and what to do with the standard errors. There may exist a reasonable justification for each of the possible choices, so that researchers do not need to consciously \"manipulate\" the data to end up reporting their main estimate conditional on statistical significance (Gelman & Loken, 2014).\n\n<sup>7</sup> In Fig. 1, it is assumed that the power is 20% for individual tests. In a scenario where true power was 80%, convergence to the belief that 0 would be very fast even in the scenario with full preregistration.\n\n![](_page_5_Figure_2.jpeg)\n\n**Fig. 1.** Belief updating for different possible shares of frequentist researchers.",
    "is_useful": true,
    "question": "What factors influence the way researchers choose to analyze data resulting in the reporting of statistically significant outcomes?"
  },
  {
    "text": "The researcher also chooses what outcome variable(s) to focus on and whether to focus on the main sample or a subgroup (and if so, which subgroup), which control variables to add in a regression, and what to do with the standard errors. There may exist a reasonable justification for each of the possible choices, so that researchers do not need to consciously \"manipulate\" the data to end up reporting their main estimate conditional on statistical significance (Gelman & Loken, 2014).\n\n<sup>7</sup> In Fig. 1, it is assumed that the power is 20% for individual tests. In a scenario where true power was 80%, convergence to the belief that 0 would be very fast even in the scenario with full preregistration.\n\n![](_page_5_Figure_2.jpeg)\n\n**Fig. 1.** Belief updating for different possible shares of frequentist researchers. These illustrations are based on Eq. (2) and the assumption that statistical power of the individual tests is equal to 20%. The y-axis measures the posterior belief attached to the hypothesis that 0 in period p, ( 0|| | t t\u00af ) p k .\n\nFor the assumption of statistical power, a recent study by Ioannidis et al. (2017) uses meta-analytic methods to estimate the median power in economic research. They find that the median power in Economics is only 18% and argue that this constitutes an empirical upper bound on true median power as publication bias will lead to overestimation of true effects and therefore, overestimation of power.",
    "is_useful": true,
    "question": "What factors do researchers consider when selecting outcome variables and control variables in their studies?"
  },
  {
    "text": "![](_page_5_Figure_2.jpeg)\n\n**Fig. 1.** Belief updating for different possible shares of frequentist researchers. These illustrations are based on Eq. (2) and the assumption that statistical power of the individual tests is equal to 20%. The y-axis measures the posterior belief attached to the hypothesis that 0 in period p, ( 0|| | t t\u00af ) p k .\n\nFor the assumption of statistical power, a recent study by Ioannidis et al. (2017) uses meta-analytic methods to estimate the median power in economic research. They find that the median power in Economics is only 18% and argue that this constitutes an empirical upper bound on true median power as publication bias will lead to overestimation of true effects and therefore, overestimation of power. Comparing high-powered and low-powered studies, Ioannidis et al. also suggest that estimates in Economics tend to be overestimated by a factor of two or more, which closely corresponds to the degree of overestimation in Table 1 in this paper for the scenario with full selective reporting. In addition, an unpublished study by Zhang and Ortmann (2013) estimates median power only for Dictator games and finds the median power is less than 25%. These power estimates are very close to the assumption made in the numerical applications of the model illustrated in Fig. 2. 8\n\nThe assumption that researchers in Experimental Economics preferentially report statistically significant results is potentially more problematic.",
    "is_useful": true,
    "question": "What are the findings regarding the statistical power of research in Economics and its implications for publication bias?"
  },
  {
    "text": "They find that the median power in Economics is only 18% and argue that this constitutes an empirical upper bound on true median power as publication bias will lead to overestimation of true effects and therefore, overestimation of power. Comparing high-powered and low-powered studies, Ioannidis et al. also suggest that estimates in Economics tend to be overestimated by a factor of two or more, which closely corresponds to the degree of overestimation in Table 1 in this paper for the scenario with full selective reporting. In addition, an unpublished study by Zhang and Ortmann (2013) estimates median power only for Dictator games and finds the median power is less than 25%. These power estimates are very close to the assumption made in the numerical applications of the model illustrated in Fig. 2. 8\n\nThe assumption that researchers in Experimental Economics preferentially report statistically significant results is potentially more problematic. Several papers (Camerer et al., 2016; Coffman & Niederle, 2015; Maniadis et al., 2017) cite Brodeur et al. (2016)\u2014a study of 50,000 hypothesis tests published in the *American Economic Review*, the *Journal of Political Economy*, and the *Quarterly Journal of Econ*omics in the time period 2005\u20132011\u2014as evidence that Experimental Economics does not suffer from \"phacking\" because there is little evidence for jumps in the distribution of p-values for randomized trials around the cutoff associated with statistical significance.",
    "is_useful": true,
    "question": "What are the implications of publication bias on power estimates in Economics research?"
  },
  {
    "text": "Thus, in terms of the model considered in this paper, we may already be in an equilibrium with close to all researchers being \"frequentist\" even in the absence of formal preregistration. However, the lack of p-hacking for all randomized trials does not in itself constitute evidence against the hypothesis of p-hacking in Experimental Economics as the Brodeur et al. (2016) analysis does not distinguish between lab experiments and field experiments. Therefore, I re-analyze the data splitting the Brodeur et al. data into lab experiments and field experiments. Fig. 2 displays the results.\n\nThe formal accounting method proposed by Brodeur et al. (2016) cannot be applied separately to the experimental data. Thus, we cannot formally test for p-hacking for these data. However, visual inspection suggests that p-hacking is prevalent in lab experiments, whereas this \"eyeballing\" test suggests that field experiments drive the overall smoothness of the experimental data reported in the final published version.9 This evidence points to the need for systematic data collection of additional experimental papers in\n\n<sup>8</sup> Similar estimates are reported in other fields, e.g. Neuroscience (Button et al., 2013).",
    "is_useful": true,
    "question": "What is the significance of distinguishing between lab experiments and field experiments in the context of p-hacking in research? "
  },
  {
    "text": "(2016) analysis does not distinguish between lab experiments and field experiments. Therefore, I re-analyze the data splitting the Brodeur et al. data into lab experiments and field experiments. Fig. 2 displays the results.\n\nThe formal accounting method proposed by Brodeur et al. (2016) cannot be applied separately to the experimental data. Thus, we cannot formally test for p-hacking for these data. However, visual inspection suggests that p-hacking is prevalent in lab experiments, whereas this \"eyeballing\" test suggests that field experiments drive the overall smoothness of the experimental data reported in the final published version.9 This evidence points to the need for systematic data collection of additional experimental papers in\n\n<sup>8</sup> Similar estimates are reported in other fields, e.g. Neuroscience (Button et al., 2013).\n\n<sup>9</sup> A previous working paper version of the paper (Brodeur, L\u00e9, Sangnier, & Zylberberg, 2013) was also divided into lab experiments and RCTs, but this division seems to have been missed in the subsequent discussion of the paper, perhaps because this split is not included in the final version (Brodeur et al., 2016).\n\n![](_page_6_Figure_2.jpeg)\n\n**Fig. 2.** Distribution of (de-rounded) z-statistics reproduced from Brodeur et al. (2016), by data type. The lines are kernel density estimates.\n\nEconomics to test more formally for excess statistical significance.",
    "is_useful": true,
    "question": "What challenges are associated with the formal testing of p-hacking in experimental data, particularly between lab and field experiments?"
  },
  {
    "text": "Neuroscience (Button et al., 2013).\n\n<sup>9</sup> A previous working paper version of the paper (Brodeur, L\u00e9, Sangnier, & Zylberberg, 2013) was also divided into lab experiments and RCTs, but this division seems to have been missed in the subsequent discussion of the paper, perhaps because this split is not included in the final version (Brodeur et al., 2016).\n\n![](_page_6_Figure_2.jpeg)\n\n**Fig. 2.** Distribution of (de-rounded) z-statistics reproduced from Brodeur et al. (2016), by data type. The lines are kernel density estimates.\n\nEconomics to test more formally for excess statistical significance. However, we should at least be skeptical of claims that Experimental Economics suffers less from selective reporting than other fields in Economics.\n\n#### **4. Concluding remarks**\n\nScholars have argued in the literature that preregistration is likely to have limited value in Experimental Economics (Coffman & Niederle, 2015). However, this paper has shown that under a plausible set of assumptions about the reporting process in experimental research, correct beliefs that the association is true may coincide with low reproducibility due to inflated effect sizes in the published literature. Preregistration policies, e.g. allowing for a publication format where the journal commits to publishing a paper based on an ex-ante analysis plan (e.g.",
    "is_useful": true,
    "question": "What challenges does the field of Experimental Economics face regarding reproducibility and the potential for inflated effect sizes in published literature?"
  },
  {
    "text": "2.** Distribution of (de-rounded) z-statistics reproduced from Brodeur et al. (2016), by data type. The lines are kernel density estimates.\n\nEconomics to test more formally for excess statistical significance. However, we should at least be skeptical of claims that Experimental Economics suffers less from selective reporting than other fields in Economics.\n\n#### **4. Concluding remarks**\n\nScholars have argued in the literature that preregistration is likely to have limited value in Experimental Economics (Coffman & Niederle, 2015). However, this paper has shown that under a plausible set of assumptions about the reporting process in experimental research, correct beliefs that the association is true may coincide with low reproducibility due to inflated effect sizes in the published literature. Preregistration policies, e.g. allowing for a publication format where the journal commits to publishing a paper based on an ex-ante analysis plan (e.g. Simons et al., 2014), will reduce the incentives to selectively report significant findings through restricting the set of possible target journals for such a reporting strategy. The reduction in incentives will lead to a larger fraction of \"frequentist\" researchers and improved effect sizes, leading replication researchers to more accurately estimate statistical power.\n\nOne interpretation of the model and available empirical evidence is that experimental economists should aim to put in place incentives for formally preregistering studies.",
    "is_useful": true,
    "question": "What are the potential benefits of preregistration policies in experimental research?"
  },
  {
    "text": "However, this paper has shown that under a plausible set of assumptions about the reporting process in experimental research, correct beliefs that the association is true may coincide with low reproducibility due to inflated effect sizes in the published literature. Preregistration policies, e.g. allowing for a publication format where the journal commits to publishing a paper based on an ex-ante analysis plan (e.g. Simons et al., 2014), will reduce the incentives to selectively report significant findings through restricting the set of possible target journals for such a reporting strategy. The reduction in incentives will lead to a larger fraction of \"frequentist\" researchers and improved effect sizes, leading replication researchers to more accurately estimate statistical power.\n\nOne interpretation of the model and available empirical evidence is that experimental economists should aim to put in place incentives for formally preregistering studies. However, the numerical illustrations of the model also show that in a research environment with only high-powered studies, preregistration would have a small influence on the inflation bias as even findings reported conditional on statistical significance would be close to the true effect. Thus, an alternative solution to recommending preregistration is for experimental researchers to aim to increase statistical power. A problem with this alternative solution is that we cannot test whether such an attempt will succeed. As power depends on the true effect, which is always unknown, there will always be a risk of overoptimistic power calculations. However, both policies that aim to increase statistical power and policies promoting preregistration are expected to have a positive effect on the reproducibility of experimental research.",
    "is_useful": true,
    "question": "What are some proposed solutions to improve reproducibility in experimental research?"
  },
  {
    "text": "One interpretation of the model and available empirical evidence is that experimental economists should aim to put in place incentives for formally preregistering studies. However, the numerical illustrations of the model also show that in a research environment with only high-powered studies, preregistration would have a small influence on the inflation bias as even findings reported conditional on statistical significance would be close to the true effect. Thus, an alternative solution to recommending preregistration is for experimental researchers to aim to increase statistical power. A problem with this alternative solution is that we cannot test whether such an attempt will succeed. As power depends on the true effect, which is always unknown, there will always be a risk of overoptimistic power calculations. However, both policies that aim to increase statistical power and policies promoting preregistration are expected to have a positive effect on the reproducibility of experimental research.\n\nA clear drawback of the simple model considered in the present paper is that it only allows for two possible choices \u2013 unconditional reporting, or reporting results conditionally on obtaining statistical significance. In reality, it is possible that a preregistration policy that reduces the incentives to engage in selective reporting will just lead agents towards other and more serious forms of undesirable behavior, such as fabricating data. A more sophisticated, game-theoretic model of the publication process by Gall and Maniadis (2019) studies these kinds of possible strategic spillovers between choices following a transparency policy in the context of false positive findings. They show that across several model specifications, provided the psychological cost associated with severe misconduct is sufficiently high, policies that aim to reduce mild misconduct will lead to a reduction in overall misconduct.",
    "is_useful": true,
    "question": "What are the expected effects of policies that promote preregistration and increase statistical power on the reproducibility of experimental research?"
  },
  {
    "text": "However, both policies that aim to increase statistical power and policies promoting preregistration are expected to have a positive effect on the reproducibility of experimental research.\n\nA clear drawback of the simple model considered in the present paper is that it only allows for two possible choices \u2013 unconditional reporting, or reporting results conditionally on obtaining statistical significance. In reality, it is possible that a preregistration policy that reduces the incentives to engage in selective reporting will just lead agents towards other and more serious forms of undesirable behavior, such as fabricating data. A more sophisticated, game-theoretic model of the publication process by Gall and Maniadis (2019) studies these kinds of possible strategic spillovers between choices following a transparency policy in the context of false positive findings. They show that across several model specifications, provided the psychological cost associated with severe misconduct is sufficiently high, policies that aim to reduce mild misconduct will lead to a reduction in overall misconduct. Moreover, removing \"mild misconduct\" from agents' choice sets tends to shift agents' behavior towards no misconduct rather than\n\nsevere misconduct. Extending the model of effect size inflation considered in the present paper to a richer strategic setting such as the one explored in Gall and Maniadis (2019) is a promising venue for future work.\n\nAnother drawback of the model is that it does not explicitly distinguish between preregistration policies per se and policies that require authors to be more transparent in their reporting of the analyses they have done along with the published paper (Simmons et al., 2011).",
    "is_useful": true,
    "question": "What are the expected effects of policies that promote preregistration and increase statistical power on the reproducibility of experimental research?"
  },
  {
    "text": "They show that across several model specifications, provided the psychological cost associated with severe misconduct is sufficiently high, policies that aim to reduce mild misconduct will lead to a reduction in overall misconduct. Moreover, removing \"mild misconduct\" from agents' choice sets tends to shift agents' behavior towards no misconduct rather than\n\nsevere misconduct. Extending the model of effect size inflation considered in the present paper to a richer strategic setting such as the one explored in Gall and Maniadis (2019) is a promising venue for future work.\n\nAnother drawback of the model is that it does not explicitly distinguish between preregistration policies per se and policies that require authors to be more transparent in their reporting of the analyses they have done along with the published paper (Simmons et al., 2011). Like a preregistration policy, such a \"transparency policy\" could also be viewed as reducing the expected benefits of selective reporting and thereby increase the share of frequentist researchers. In the model, these two policies could be interpreted as two possible means of obtaining the same end. A preregistration policy could be seen as an \"ex ante\" transparency policy, aiming to ensure that the analysis plan is actually adhered to ex post. Future work \u2013 theoretical and empirical \u2013 should aim to discuss the relative costs and benefits of ex ante versus ex post transparency policies in improving effect size estimation.\n\nFinally, it could be argued that the numerical applications of the model in Table 1 implicitly assume that replicators are somewhat \"na\u00efve,\" in the sense that they systematically base their power estimates on previously published effect sizes without taking into account the possibility for inflated effects.",
    "is_useful": true,
    "question": "What are the implications of addressing mild misconduct on overall misconduct in research settings?"
  },
  {
    "text": "Like a preregistration policy, such a \"transparency policy\" could also be viewed as reducing the expected benefits of selective reporting and thereby increase the share of frequentist researchers. In the model, these two policies could be interpreted as two possible means of obtaining the same end. A preregistration policy could be seen as an \"ex ante\" transparency policy, aiming to ensure that the analysis plan is actually adhered to ex post. Future work \u2013 theoretical and empirical \u2013 should aim to discuss the relative costs and benefits of ex ante versus ex post transparency policies in improving effect size estimation.\n\nFinally, it could be argued that the numerical applications of the model in Table 1 implicitly assume that replicators are somewhat \"na\u00efve,\" in the sense that they systematically base their power estimates on previously published effect sizes without taking into account the possibility for inflated effects. This is apparently at odds with evidence from expert surveys and prediction markets showing that researchers are surprisingly accurate in judging which results will replicate or not (Camerer et al., 2016, 2018; Dreber et al., 2015; Forsell et al., 2018). However, systematic overestimation of power may be entirely consistent with perfectly informed replicators who behave rationally according to incentives. To see this, suppose that to get a replication study published a replication team needs to demonstrate that the replication study has at least 80% power to detect the original effect and that beyond this point, increasing the sample size has no effect on the probability of getting the replication study published.",
    "is_useful": true,
    "question": "What are the potential impacts of implementing transparency policies such as preregistration on research practices and effect size estimation?"
  },
  {
    "text": "Finally, it could be argued that the numerical applications of the model in Table 1 implicitly assume that replicators are somewhat \"na\u00efve,\" in the sense that they systematically base their power estimates on previously published effect sizes without taking into account the possibility for inflated effects. This is apparently at odds with evidence from expert surveys and prediction markets showing that researchers are surprisingly accurate in judging which results will replicate or not (Camerer et al., 2016, 2018; Dreber et al., 2015; Forsell et al., 2018). However, systematic overestimation of power may be entirely consistent with perfectly informed replicators who behave rationally according to incentives. To see this, suppose that to get a replication study published a replication team needs to demonstrate that the replication study has at least 80% power to detect the original effect and that beyond this point, increasing the sample size has no effect on the probability of getting the replication study published. Then the optimal sample size rule for replicators will be the minimal sample size that is sufficient to achieve 80% power to detect the original effect, as further increases in power are likely to be very costly.\n\nAlthough empirical evidence is in line with the suggestion that preregistration potentially offers a large reduction in the inflation bias, there is not enough direct evidence to draw strong conclusions. This suggests two important implications for future empirical research. First, researchers should aim to inquire further into the extent of selective reporting in Experimental Economics, perhaps by collecting more data from strong field journals that publish lab experiments and testing for excess significance in these journals.",
    "is_useful": true,
    "question": "What are the implications of preregistration on inflation bias in research studies?"
  },
  {
    "text": "To see this, suppose that to get a replication study published a replication team needs to demonstrate that the replication study has at least 80% power to detect the original effect and that beyond this point, increasing the sample size has no effect on the probability of getting the replication study published. Then the optimal sample size rule for replicators will be the minimal sample size that is sufficient to achieve 80% power to detect the original effect, as further increases in power are likely to be very costly.\n\nAlthough empirical evidence is in line with the suggestion that preregistration potentially offers a large reduction in the inflation bias, there is not enough direct evidence to draw strong conclusions. This suggests two important implications for future empirical research. First, researchers should aim to inquire further into the extent of selective reporting in Experimental Economics, perhaps by collecting more data from strong field journals that publish lab experiments and testing for excess significance in these journals. Second, researchers should aim to estimate statistical power specifically for Experimental Economics, as existing estimates may not be representative of power for laboratory experiments and the three journals reported in Brodeur et al. (2016) constitute only a small fraction of all lab experiments. More evidence along these lines would help us make a better informed decision about whether to recommend preregistration of all studies.\n\n### **Appendix A**\n\nSee Table A1.\n\n# **Table A1**\n\nNumerical application of the model assuming a normal distribution for the estimates and 80% power in the original study to detect the true effect.",
    "is_useful": true,
    "question": "What are the key implications for future empirical research in Experimental Economics related to replication studies and preregistration?"
  },
  {
    "text": "This suggests two important implications for future empirical research. First, researchers should aim to inquire further into the extent of selective reporting in Experimental Economics, perhaps by collecting more data from strong field journals that publish lab experiments and testing for excess significance in these journals. Second, researchers should aim to estimate statistical power specifically for Experimental Economics, as existing estimates may not be representative of power for laboratory experiments and the three journals reported in Brodeur et al. (2016) constitute only a small fraction of all lab experiments. More evidence along these lines would help us make a better informed decision about whether to recommend preregistration of all studies.\n\n### **Appendix A**\n\nSee Table A1.\n\n# **Table A1**\n\nNumerical application of the model assuming a normal distribution for the estimates and 80% power in the original study to detect the true effect.\n\n| G\u00af ( ) |  | \u00af | Estimated total N to achieve 80% power | Actual power |\n| --- | --- | --- | --- | --- |\n| 0 | 0.08 | 0.09 | 968 | 70.16% |\n| 0.2 | 0.08 | 0.088 | 1014 | 72.16% |\n| 0.4 | 0.08 | 0.086 | 1060 | 74.04% |\n| 0.6 | 0.08 | 0.084 | 1112 | 76.05% |\n| 0.8 | 0.08 | 0.082 | 1166 | 78% |\n| 1 | 0.08 | 0.08 | 1226 | 80% |\n\nNote: G\u00af ( ) is the share of frequentist researchers, is the true effect and \u00af is the mean reported effect.",
    "is_useful": true,
    "question": "What considerations should researchers take into account for improving empirical research in Experimental Economics?"
  },
  {
    "text": "![](_page_0_Picture_0.jpeg)\n\nGaletzka, C. (2019). A Short Introduction to the Reproducibility Debate in Psychology. Journal of European Psychology Students, 10(1), 16\u201325. Doi:10.5334/jeps.469\n\n## LITERATURE REVIEW\n\n# A Short Introduction to the Reproducibility Debate in Psychology\n\nCedric Galetzka1\n\nReproducibility is considered a defining feature of science: Trust in scientific discovery and progress are argued to depend on the ability to reproduce previous results. However, recent large-scale replication studies have spurred debate on the reproducibility of scientific findings and suggested that psychology is facing a crisis. The reproducibility of results has been related to current publication practices, which favor sensational and statistically significant results over replication studies. In turn, this skewed incentive system may encourage researchers to engage in questionable research practices, thereby distorting the psychological literature. Important findings and criticisms, as well as potential measures to improve the reproducibility of results, such as preregistered reports, replication studies, and open science, are discussed.\n\nKeywords: reproducibility, replication, publication bias, questionable research practices, preregistration\n\n#### Introduction\n\nMore than 50 years ago, Rosenthal and Jacobson (1968) published the first study on the Pygmalion effect, showing that teachers' positive expectations of students' performance result in an increase in students' IQ points compared to students for whom no additional expectations were formed.",
    "is_useful": true,
    "question": "What are some measures suggested to improve the reproducibility of results in scientific research?"
  },
  {
    "text": "However, recent large-scale replication studies have spurred debate on the reproducibility of scientific findings and suggested that psychology is facing a crisis. The reproducibility of results has been related to current publication practices, which favor sensational and statistically significant results over replication studies. In turn, this skewed incentive system may encourage researchers to engage in questionable research practices, thereby distorting the psychological literature. Important findings and criticisms, as well as potential measures to improve the reproducibility of results, such as preregistered reports, replication studies, and open science, are discussed.\n\nKeywords: reproducibility, replication, publication bias, questionable research practices, preregistration\n\n#### Introduction\n\nMore than 50 years ago, Rosenthal and Jacobson (1968) published the first study on the Pygmalion effect, showing that teachers' positive expectations of students' performance result in an increase in students' IQ points compared to students for whom no additional expectations were formed. The Pygmalion effect inspired a wide range of research and has been frequently cited (5469 times according to Google Scholar on February 8, 2019; Calin-Jageman, 2018; Rosenthal & Jacobson, 1968). Yet, subsequent replication studies did not provide consistent confirmatory evidence, giving rise to debates on methodological design, flexibility in data analysis, and the value of replication studies in science (Calin-Jageman, 2018; Spitz, 1999).",
    "is_useful": true,
    "question": "What measures are suggested to improve the reproducibility of scientific results in psychology?"
  },
  {
    "text": "Keywords: reproducibility, replication, publication bias, questionable research practices, preregistration\n\n#### Introduction\n\nMore than 50 years ago, Rosenthal and Jacobson (1968) published the first study on the Pygmalion effect, showing that teachers' positive expectations of students' performance result in an increase in students' IQ points compared to students for whom no additional expectations were formed. The Pygmalion effect inspired a wide range of research and has been frequently cited (5469 times according to Google Scholar on February 8, 2019; Calin-Jageman, 2018; Rosenthal & Jacobson, 1968). Yet, subsequent replication studies did not provide consistent confirmatory evidence, giving rise to debates on methodological design, flexibility in data analysis, and the value of replication studies in science (Calin-Jageman, 2018; Spitz, 1999). Indeed, a\n\n1 Department of Psychology, University of Potsdam, DEU\n\nCorresponding author: Cedric Galetzka (cedric.galetzka@gmail.com)\n\nreview of 35 years of research on the impact of teachers' expectations on intelligence indicated that the original effect is rather negligible, but that context is a powerful mediator (Jussim & Harber, 2005).",
    "is_useful": true,
    "question": "What are some challenges and considerations in the context of scientific studies related to the reproducibility and validity of research findings?"
  },
  {
    "text": "The Pygmalion effect inspired a wide range of research and has been frequently cited (5469 times according to Google Scholar on February 8, 2019; Calin-Jageman, 2018; Rosenthal & Jacobson, 1968). Yet, subsequent replication studies did not provide consistent confirmatory evidence, giving rise to debates on methodological design, flexibility in data analysis, and the value of replication studies in science (Calin-Jageman, 2018; Spitz, 1999). Indeed, a\n\n1 Department of Psychology, University of Potsdam, DEU\n\nCorresponding author: Cedric Galetzka (cedric.galetzka@gmail.com)\n\nreview of 35 years of research on the impact of teachers' expectations on intelligence indicated that the original effect is rather negligible, but that context is a powerful mediator (Jussim & Harber, 2005). Nowadays, large-scale replication studies have raised concerns regarding the reproducibility of research in psychology, and issues resembling the controversy over the Pygmalion effect have been widely discussed (Baker, 2016; Calin-Jageman, 2018; Fanelli, 2018; Open Science Collaboration, 2015). This review first introduces the concept of reproducibility and highlights findings that have led to a debate on reproducibility in psychology. In addition, likely causes of non-reproducible research and proposed solutions are discussed.\n\n#### What Does Reproducibility Imply?",
    "is_useful": true,
    "question": "What are the implications of reproducibility in psychological research?"
  },
  {
    "text": "Nowadays, large-scale replication studies have raised concerns regarding the reproducibility of research in psychology, and issues resembling the controversy over the Pygmalion effect have been widely discussed (Baker, 2016; Calin-Jageman, 2018; Fanelli, 2018; Open Science Collaboration, 2015). This review first introduces the concept of reproducibility and highlights findings that have led to a debate on reproducibility in psychology. In addition, likely causes of non-reproducible research and proposed solutions are discussed.\n\n#### What Does Reproducibility Imply?\n\nReproducibility was first emphasized by Boyle (1660) who conducted elaborate air-pump experiments on how to generate a vacuum. Contemporary philosophers contested the idea of empty space, leading Boyle (1660) to formulate detailed descriptions of his methods for others to replicate (Shapin & Schaffer, 1985): \"I thought it necessary to deliver things circumstantially, that the Person I addressed them to \u2026 be able to repeat such unusual Experiments\" (p.5). Following the same procedures ensures yielding similar results if relationships are deterministic, but is not guaranteed for systems involving randomness or unknown variables (Goodman, Fanelli, & Ioannidis, 2016; Zwaan, Etz, Lucas, & Donnellan, 2018). For instance, the number of action potentials in response to a certain stimulus is observed to vary from trial to trial due to uncontrollable cognitive and physiological processes and is therefore expressed probabilistically as a firing rate (Dayan & Abbott, 2005).",
    "is_useful": true,
    "question": "What are the main concerns and discussions surrounding reproducibility in research, particularly in the field of psychology?"
  },
  {
    "text": "Contemporary philosophers contested the idea of empty space, leading Boyle (1660) to formulate detailed descriptions of his methods for others to replicate (Shapin & Schaffer, 1985): \"I thought it necessary to deliver things circumstantially, that the Person I addressed them to \u2026 be able to repeat such unusual Experiments\" (p.5). Following the same procedures ensures yielding similar results if relationships are deterministic, but is not guaranteed for systems involving randomness or unknown variables (Goodman, Fanelli, & Ioannidis, 2016; Zwaan, Etz, Lucas, & Donnellan, 2018). For instance, the number of action potentials in response to a certain stimulus is observed to vary from trial to trial due to uncontrollable cognitive and physiological processes and is therefore expressed probabilistically as a firing rate (Dayan & Abbott, 2005). In such cases, closely reproducing the procedures and design of an experiment (i.e., conducting a direct replication) may not reproduce the original findings, which led Goodman and colleagues (2016) to distinguish between methods and results reproducibility. In addition, the degree to which researchers agree on 2009).\n\ninterpretations about research findings is referred to by Goodman and colleagues (2016) as inferential reproducibility; varied conclusions might arise after re-analyzing existing results using a different statistical approach or conducting replication studies that employ different methodological designs (i.e., conceptual replications; Crandall & Sherman, 2016; Goodman et al., 2016).",
    "is_useful": true,
    "question": "What are the key concepts and challenges associated with reproducibility in scientific research?"
  },
  {
    "text": "For instance, the number of action potentials in response to a certain stimulus is observed to vary from trial to trial due to uncontrollable cognitive and physiological processes and is therefore expressed probabilistically as a firing rate (Dayan & Abbott, 2005). In such cases, closely reproducing the procedures and design of an experiment (i.e., conducting a direct replication) may not reproduce the original findings, which led Goodman and colleagues (2016) to distinguish between methods and results reproducibility. In addition, the degree to which researchers agree on 2009).\n\ninterpretations about research findings is referred to by Goodman and colleagues (2016) as inferential reproducibility; varied conclusions might arise after re-analyzing existing results using a different statistical approach or conducting replication studies that employ different methodological designs (i.e., conceptual replications; Crandall & Sherman, 2016; Goodman et al., 2016). Throughout the remainder of this review, these distinctions of reproducibility will be adopted.\n\nReporting the design of a study and the data analysis enables independent researchers to verify experimental findings. However, the degree of transparency needed to achieve methods reproducibility depends on the effect in question (Goodman et al., 2016); documenting word frequency, for instance, may be relevant for a study's manipulation but not for participant instructions. Further, reproducing a result via direct replications is argued to be necessary for a finding to be considered reliable (Popper, 1959). However, does a single replication that does not indicate an effect refute previous findings?",
    "is_useful": true,
    "question": "What are the different types of reproducibility in research, and why is transparency in reporting research design and data analysis important for validating experimental findings?"
  },
  {
    "text": "Throughout the remainder of this review, these distinctions of reproducibility will be adopted.\n\nReporting the design of a study and the data analysis enables independent researchers to verify experimental findings. However, the degree of transparency needed to achieve methods reproducibility depends on the effect in question (Goodman et al., 2016); documenting word frequency, for instance, may be relevant for a study's manipulation but not for participant instructions. Further, reproducing a result via direct replications is argued to be necessary for a finding to be considered reliable (Popper, 1959). However, does a single replication that does not indicate an effect refute previous findings? Replications necessarily vary along some dimension (e.g., time), which makes it possible for discrepant results to be attributed to random error or unknown variables (Crandall & Sherman, 2016). For example, some effects may be mediated by experience in implementing a certain experiment (Collins, 1975; Earp & Trafimow, 2015). In this case, direct replications may only falsify a finding under specific circumstances (e.g., no expert knowledge available), and these insights can be used to gain a better understanding of an effect's implicit assumptions (Earp & Trafimow, 2015). Additionally, conceptual replications may uncover a theory's boundary conditions by answering whether similar inferences can be drawn across contexts and experimental designs (Crandall & Sherman, 2016).",
    "is_useful": true,
    "question": "What role does transparency in research design and data analysis play in the reproducibility of scientific findings?"
  },
  {
    "text": "However, does a single replication that does not indicate an effect refute previous findings? Replications necessarily vary along some dimension (e.g., time), which makes it possible for discrepant results to be attributed to random error or unknown variables (Crandall & Sherman, 2016). For example, some effects may be mediated by experience in implementing a certain experiment (Collins, 1975; Earp & Trafimow, 2015). In this case, direct replications may only falsify a finding under specific circumstances (e.g., no expert knowledge available), and these insights can be used to gain a better understanding of an effect's implicit assumptions (Earp & Trafimow, 2015). Additionally, conceptual replications may uncover a theory's boundary conditions by answering whether similar inferences can be drawn across contexts and experimental designs (Crandall & Sherman, 2016). In an ideal scenario, conducting replication studies ensures science to remain self-correcting and theories to become increasingly comprehensive and is therefore considered a hallmark of scientific method (Braude, 1979; Ioannidis, 2012; Schmidt,\n\n#### The Current Debate in Psychology\n\nThe reproducibility debate in psychology originated from reports that results cannot be replicated (Baker, 2016). For example, independent research teams of the Open Science Collaboration (2015) conducted direct replications of 100 studies in the areas of social and cognitive psychology.",
    "is_useful": true,
    "question": "How do replication studies contribute to the self-correcting nature of science and the comprehensiveness of theories?"
  },
  {
    "text": "Additionally, conceptual replications may uncover a theory's boundary conditions by answering whether similar inferences can be drawn across contexts and experimental designs (Crandall & Sherman, 2016). In an ideal scenario, conducting replication studies ensures science to remain self-correcting and theories to become increasingly comprehensive and is therefore considered a hallmark of scientific method (Braude, 1979; Ioannidis, 2012; Schmidt,\n\n#### The Current Debate in Psychology\n\nThe reproducibility debate in psychology originated from reports that results cannot be replicated (Baker, 2016). For example, independent research teams of the Open Science Collaboration (2015) conducted direct replications of 100 studies in the areas of social and cognitive psychology. The authors reported statistically significant findings (p-values smaller than .05) in the same direction as the original studies in 36 percent of replications (Open Science Collaboration, 2015). Likewise, Camerer and colleagues (2018) examined psychological studies that were published in Nature and Science and found converging evidence for 61 percent of replicated studies while effect sizes were on average half as large as previously reported. Furthermore, a multilab effort by Wagenmakers and collaborators (2016) found null results for the facial feedback hypothesis by Strack, Martin, and Stepper (1988) across 17 independent replications. Similar findings were obtained for the ego depletion effect (Hagger et al., 2016) and the blocking effect in conditioning (Maes et al., 2016).",
    "is_useful": true,
    "question": "What role do replication studies play in maintaining the robustness of scientific theories and practices within the field of psychology?"
  },
  {
    "text": "The authors reported statistically significant findings (p-values smaller than .05) in the same direction as the original studies in 36 percent of replications (Open Science Collaboration, 2015). Likewise, Camerer and colleagues (2018) examined psychological studies that were published in Nature and Science and found converging evidence for 61 percent of replicated studies while effect sizes were on average half as large as previously reported. Furthermore, a multilab effort by Wagenmakers and collaborators (2016) found null results for the facial feedback hypothesis by Strack, Martin, and Stepper (1988) across 17 independent replications. Similar findings were obtained for the ego depletion effect (Hagger et al., 2016) and the blocking effect in conditioning (Maes et al., 2016). These findings suggest that psychology faces a crisis since many effects are unreliable and may be due to random error (i.e., false-positive results; Ioannidis, 2005). In addition, these effects may have inspired new studies that rest on faulty assumptions, thereby leading to a profusion of resources and undermining trust in psychological research (Begley & Ioannidis, 2015).\n\nThis view is complicated by the fact that there is no generally accepted statistical definition for results reproducibility (Zwaan et al., 2018). For example, the Open Science Collaboration (2015) employed null hypothesis significance testing, effect size comparisons, and investigators' subjective estimates to examine whether a finding was replicated.",
    "is_useful": true,
    "question": "What challenges does psychology face regarding the reliability and reproducibility of research findings?"
  },
  {
    "text": "Similar findings were obtained for the ego depletion effect (Hagger et al., 2016) and the blocking effect in conditioning (Maes et al., 2016). These findings suggest that psychology faces a crisis since many effects are unreliable and may be due to random error (i.e., false-positive results; Ioannidis, 2005). In addition, these effects may have inspired new studies that rest on faulty assumptions, thereby leading to a profusion of resources and undermining trust in psychological research (Begley & Ioannidis, 2015).\n\nThis view is complicated by the fact that there is no generally accepted statistical definition for results reproducibility (Zwaan et al., 2018). For example, the Open Science Collaboration (2015) employed null hypothesis significance testing, effect size comparisons, and investigators' subjective estimates to examine whether a finding was replicated. Moreover, these methods might provide biased estimates of results reproducibility; a replication's effect size may be similarly strong as in the original study, but the effect might be considered statistically non-significant because of differences in sample size (Goodman et al., 2016). In addition, it is unclear how many statistically non-significant replications are needed to falsify a result (Earp & Trafimow, 2015), which is why Goodman and colleagues (2016) argued to pool the results of replication studies to analyze their cumulative strength of evidence.",
    "is_useful": true,
    "question": "What challenges does psychological research face regarding the reproducibility of results?"
  },
  {
    "text": "This view is complicated by the fact that there is no generally accepted statistical definition for results reproducibility (Zwaan et al., 2018). For example, the Open Science Collaboration (2015) employed null hypothesis significance testing, effect size comparisons, and investigators' subjective estimates to examine whether a finding was replicated. Moreover, these methods might provide biased estimates of results reproducibility; a replication's effect size may be similarly strong as in the original study, but the effect might be considered statistically non-significant because of differences in sample size (Goodman et al., 2016). In addition, it is unclear how many statistically non-significant replications are needed to falsify a result (Earp & Trafimow, 2015), which is why Goodman and colleagues (2016) argued to pool the results of replication studies to analyze their cumulative strength of evidence. Relevant to this argument, the \"Many Labs\" project by Klein and colleagues (2014) reported the pooled results for 36 replications of 13 effects from the psychological literature and showed that 11 out of 13 effects were replicated. Besides, Gilbert, King, Pettigrew, and Wilson (2016) noted that if Klein and colleagues (2014) had conducted significance tests for each replication and reported their results like the Open Science Collaboration (2015), the rate of \"successful\" replications would have decreased from 85% to 34%. Therefore, critics contend that psychology faces a crisis and argue that certain decision criteria may underestimate results reproducibility (Fanelli, 2018).",
    "is_useful": true,
    "question": "What challenges exist in defining and assessing results reproducibility in psychological research?"
  },
  {
    "text": "Relevant to this argument, the \"Many Labs\" project by Klein and colleagues (2014) reported the pooled results for 36 replications of 13 effects from the psychological literature and showed that 11 out of 13 effects were replicated. Besides, Gilbert, King, Pettigrew, and Wilson (2016) noted that if Klein and colleagues (2014) had conducted significance tests for each replication and reported their results like the Open Science Collaboration (2015), the rate of \"successful\" replications would have decreased from 85% to 34%. Therefore, critics contend that psychology faces a crisis and argue that certain decision criteria may underestimate results reproducibility (Fanelli, 2018).\n\nGilbert and colleagues (2016) also pointed toward methodological differences between direct replication studies of the Open Science Collaboration (2015) and the original experiments. For example, one study that examined attitudes toward African Americans was replicated with Italian and not American participants (Gilbert et al., 2016; Payne, Burkley, & Stokes, 2008). Comparing the replication studies that were endorsed by the original authors to those that were not, Gilbert and colleagues (2016) found endorsed replications to be more likely to reproduce an effect (59.7% vs. 15.4%). Conversely, a direct replication may not disprove a finding across contexts (Earp & Trafimow, 2015).",
    "is_useful": true,
    "question": "What challenges does the field of psychology face regarding the reproducibility of research results?"
  },
  {
    "text": "Therefore, critics contend that psychology faces a crisis and argue that certain decision criteria may underestimate results reproducibility (Fanelli, 2018).\n\nGilbert and colleagues (2016) also pointed toward methodological differences between direct replication studies of the Open Science Collaboration (2015) and the original experiments. For example, one study that examined attitudes toward African Americans was replicated with Italian and not American participants (Gilbert et al., 2016; Payne, Burkley, & Stokes, 2008). Comparing the replication studies that were endorsed by the original authors to those that were not, Gilbert and colleagues (2016) found endorsed replications to be more likely to reproduce an effect (59.7% vs. 15.4%). Conversely, a direct replication may not disprove a finding across contexts (Earp & Trafimow, 2015). For example, Iso-Ahola (2017) argued that several identical replication studies on the ego-depletion effect by Hagger and colleagues (2016) may not falsify the phenomenon, since ego-depletion has been observed across a variety of experimental designs. For these reasons, opponents of the crisis movement argue to shift the focus from results to inferential reproducibility in order to develop a more nuanced understanding of psychological effects (Drummond, 2018; Goodman et al., 2016; Zwaan et al., 2018).",
    "is_useful": true,
    "question": "What are the main concerns critics have regarding the reproducibility of results in psychology?"
  },
  {
    "text": "Comparing the replication studies that were endorsed by the original authors to those that were not, Gilbert and colleagues (2016) found endorsed replications to be more likely to reproduce an effect (59.7% vs. 15.4%). Conversely, a direct replication may not disprove a finding across contexts (Earp & Trafimow, 2015). For example, Iso-Ahola (2017) argued that several identical replication studies on the ego-depletion effect by Hagger and colleagues (2016) may not falsify the phenomenon, since ego-depletion has been observed across a variety of experimental designs. For these reasons, opponents of the crisis movement argue to shift the focus from results to inferential reproducibility in order to develop a more nuanced understanding of psychological effects (Drummond, 2018; Goodman et al., 2016; Zwaan et al., 2018).\n\n#### Causes of Non-Reproducible Results\n\nWhile the degree to which findings in psychology can be reproduced is debated, a large number of studies have investigated possible causes of nonreproducible results. The following section outlines mechanisms by which current publication practices may affect related scientific practice.\n\nPublication Bias. In a recent Nature survey, more than 60% of researchers mentioned the burden to publish as one of the main reasons for non-reproducible research (Baker, 2016).",
    "is_useful": true,
    "question": "What are some of the challenges associated with reproducibility in scientific research?"
  },
  {
    "text": "For these reasons, opponents of the crisis movement argue to shift the focus from results to inferential reproducibility in order to develop a more nuanced understanding of psychological effects (Drummond, 2018; Goodman et al., 2016; Zwaan et al., 2018).\n\n#### Causes of Non-Reproducible Results\n\nWhile the degree to which findings in psychology can be reproduced is debated, a large number of studies have investigated possible causes of nonreproducible results. The following section outlines mechanisms by which current publication practices may affect related scientific practice.\n\nPublication Bias. In a recent Nature survey, more than 60% of researchers mentioned the burden to publish as one of the main reasons for non-reproducible research (Baker, 2016). The number of publications and citations are commonly used to assess the productivity of researchers, institutions, and countries, and impact career development, such as prospects of promotion or tenure (Garfield, 1955; Moher et al., 2018). Crucially, statistically significant findings are more likely to be published than null results (Begg & Berlin, 1988; Rosenthal, 1979). For example, Fanelli (2012) observed that the frequency of statistically significant findings in psychology increased from around 70 to more than 90 percent between 1990 and 2007. Conversely, this pattern may discourage researchers from attempting to publish statistically non-significant results, which has been termed the \"file-drawer effect\" (Rosenthal, 1979).",
    "is_useful": true,
    "question": "What are some of the causes of non-reproducible results in psychological research, as identified by recent studies?"
  },
  {
    "text": "The number of publications and citations are commonly used to assess the productivity of researchers, institutions, and countries, and impact career development, such as prospects of promotion or tenure (Garfield, 1955; Moher et al., 2018). Crucially, statistically significant findings are more likely to be published than null results (Begg & Berlin, 1988; Rosenthal, 1979). For example, Fanelli (2012) observed that the frequency of statistically significant findings in psychology increased from around 70 to more than 90 percent between 1990 and 2007. Conversely, this pattern may discourage researchers from attempting to publish statistically non-significant results, which has been termed the \"file-drawer effect\" (Rosenthal, 1979). In addition, publication bias is accompanied by a focus on novelty, which makes replication studies increasingly irrelevant for career advancement (Franco, Malhotra & Simonovits, 2014). This skewed incentive structure (i.e., rewarding novel and positive results over negative findings and replications) may be an obstacle to self-correction in psychology (Ioannidis, 2012; Young, Ioannidis, & Al-Ubaydli, 2008).\n\nStudy Design and Power. Incentivizing publication of statistically significant findings has repercussions on results reproducibility from a statistical perspective (Gelman & Carlin, 2014).",
    "is_useful": true,
    "question": "What are some consequences of publication bias related to the focus on statistically significant findings in research?"
  },
  {
    "text": "Conversely, this pattern may discourage researchers from attempting to publish statistically non-significant results, which has been termed the \"file-drawer effect\" (Rosenthal, 1979). In addition, publication bias is accompanied by a focus on novelty, which makes replication studies increasingly irrelevant for career advancement (Franco, Malhotra & Simonovits, 2014). This skewed incentive structure (i.e., rewarding novel and positive results over negative findings and replications) may be an obstacle to self-correction in psychology (Ioannidis, 2012; Young, Ioannidis, & Al-Ubaydli, 2008).\n\nStudy Design and Power. Incentivizing publication of statistically significant findings has repercussions on results reproducibility from a statistical perspective (Gelman & Carlin, 2014). Psychology experiments are often underpowered, that is, the probability of discovering true effects is decreased due to small effects and sample sizes (Bakker, van Dijk, & Wicherts, 2012; Cohen, 1990; Smaldino & McElreath, 2016). Low power means that effects that are discovered are less likely to be true (Ioannidis, 2005) while true effects are more likely to be inflated or found in the wrong direction (type M[agnitude] and type S[ign] error; Gelman & Carlin, 2014).",
    "is_useful": true,
    "question": "What are the potential consequences of publication bias on the reproducibility of research findings in psychology?"
  },
  {
    "text": "Study Design and Power. Incentivizing publication of statistically significant findings has repercussions on results reproducibility from a statistical perspective (Gelman & Carlin, 2014). Psychology experiments are often underpowered, that is, the probability of discovering true effects is decreased due to small effects and sample sizes (Bakker, van Dijk, & Wicherts, 2012; Cohen, 1990; Smaldino & McElreath, 2016). Low power means that effects that are discovered are less likely to be true (Ioannidis, 2005) while true effects are more likely to be inflated or found in the wrong direction (type M[agnitude] and type S[ign] error; Gelman & Carlin, 2014). The latter occurs with small-sample studies when a significance threshold is only passed by an exaggerated estimate of the true effect, which will be less likely to replicate in the future (Button et al., 2013). Goodman (2018) hypothesized that the study by the Open Science Collaboration (2015) hinted at the presence of type M errors in the psychological literature: Results of the original studies clustered below the .05 significance threshold, whereas the p-values of the high-powered replications were more broadly distributed and therefore potentially regressed to the true effect estimate (Goodman, 2018; Open Science Collaboration, 2015). Significantly, Smaldino and McElreath (2016) argued that running small-sample studies may be an adaptive response to publication bias since it allows to obtain statistically significant results at low costs.",
    "is_useful": true,
    "question": "How does low statistical power in psychological experiments affect the reliability of research findings?"
  },
  {
    "text": "The latter occurs with small-sample studies when a significance threshold is only passed by an exaggerated estimate of the true effect, which will be less likely to replicate in the future (Button et al., 2013). Goodman (2018) hypothesized that the study by the Open Science Collaboration (2015) hinted at the presence of type M errors in the psychological literature: Results of the original studies clustered below the .05 significance threshold, whereas the p-values of the high-powered replications were more broadly distributed and therefore potentially regressed to the true effect estimate (Goodman, 2018; Open Science Collaboration, 2015). Significantly, Smaldino and McElreath (2016) argued that running small-sample studies may be an adaptive response to publication bias since it allows to obtain statistically significant results at low costs.\n\nQuestionable Research Practices. Flexibility in data analysis can be utilized to increase the probability of detecting statistically significant results (Ioannidis, 2005). For example, Simmons, Nelson, and Simonsohn (2011) demonstrated that researcher degrees of freedom, or \"p-hacking\", during statistical analysis (e.g., excluding experimental conditions, adding participants, or including covariates) can raise the likelihood of finding p-values below .05 by more\n\nthan 60%. In addition, selective reporting of results may have further implications for conclusions drawn from meta-analyses (Hutton & Williamson, 2000).",
    "is_useful": true,
    "question": "What challenges does open science face regarding the reliability of small-sample studies and statistical practices?"
  },
  {
    "text": "Significantly, Smaldino and McElreath (2016) argued that running small-sample studies may be an adaptive response to publication bias since it allows to obtain statistically significant results at low costs.\n\nQuestionable Research Practices. Flexibility in data analysis can be utilized to increase the probability of detecting statistically significant results (Ioannidis, 2005). For example, Simmons, Nelson, and Simonsohn (2011) demonstrated that researcher degrees of freedom, or \"p-hacking\", during statistical analysis (e.g., excluding experimental conditions, adding participants, or including covariates) can raise the likelihood of finding p-values below .05 by more\n\nthan 60%. In addition, selective reporting of results may have further implications for conclusions drawn from meta-analyses (Hutton & Williamson, 2000). These questionable research practices (QRPs) may inflate the number of false-positives and diminish results reproducibility, but are currently a gray area in scientific conduct (John, Loewenstein, & Prelec, 2012; Wicherts et al., 2016). Importantly, methods that produce statistically significant and non-reproducible results may appear to be appropriate choices rather than intentional attempts of distorting research findings in order to achieve publication (e.g., hindsight and confirmation bias; Munaf\u00f2 et al., 2017).\n\nHARKing. Current publication practices emphasize consistency between results and hypotheses, a requirement that has been communicated to aspiring and early-career social scientists (Bem, 1987; Bishop, 2017; Kerr, 1998).",
    "is_useful": true,
    "question": "What are some concerns related to questionable research practices in the context of scientific publication and reproducibility?"
  },
  {
    "text": "In addition, selective reporting of results may have further implications for conclusions drawn from meta-analyses (Hutton & Williamson, 2000). These questionable research practices (QRPs) may inflate the number of false-positives and diminish results reproducibility, but are currently a gray area in scientific conduct (John, Loewenstein, & Prelec, 2012; Wicherts et al., 2016). Importantly, methods that produce statistically significant and non-reproducible results may appear to be appropriate choices rather than intentional attempts of distorting research findings in order to achieve publication (e.g., hindsight and confirmation bias; Munaf\u00f2 et al., 2017).\n\nHARKing. Current publication practices emphasize consistency between results and hypotheses, a requirement that has been communicated to aspiring and early-career social scientists (Bem, 1987; Bishop, 2017; Kerr, 1998). This rule of thumb may lead researchers to hypothesize after results are known (HARKing), that is, presenting post hoc hypotheses as a priori to confirm findings from statistical analyses (Kerr, 1998). HARKing may introduce bias since post hoc hypotheses might be implausible; thorough literature research should narrow down on the most plausible hypotheses prior to data collection (Kerr, 1998). Therefore, HARKing promotes the development of narrow theories that are potentially based on false-positives (Kerr, 1998).",
    "is_useful": true,
    "question": "What are some implications of questionable research practices on the reproducibility of scientific results?"
  },
  {
    "text": "HARKing. Current publication practices emphasize consistency between results and hypotheses, a requirement that has been communicated to aspiring and early-career social scientists (Bem, 1987; Bishop, 2017; Kerr, 1998). This rule of thumb may lead researchers to hypothesize after results are known (HARKing), that is, presenting post hoc hypotheses as a priori to confirm findings from statistical analyses (Kerr, 1998). HARKing may introduce bias since post hoc hypotheses might be implausible; thorough literature research should narrow down on the most plausible hypotheses prior to data collection (Kerr, 1998). Therefore, HARKing promotes the development of narrow theories that are potentially based on false-positives (Kerr, 1998). Murphy and Aguinis (2017) observed that selective reporting of hypotheses can lead to considerable bias in the literature, especially when population parameters are subtle. Importantly, this discussion is not intended to discount exploratory hypothesis testing; progress in science is also based on hypotheses that gathered support after experimentation but seemed implausible at first (Franklin, 2005). However, if post hoc insights are not distinguished from a priori hypotheses, HARKing may lead to hypotheses that are less likely to be reproduced (Hollenbeck & Wright, 2017; Kerr, 1998).\n\nPrevalence of Questionable Research Practices.",
    "is_useful": true,
    "question": "What potential issues can arise from current publication practices that emphasize consistency between hypotheses and research results in scientific research?"
  },
  {
    "text": "HARKing may introduce bias since post hoc hypotheses might be implausible; thorough literature research should narrow down on the most plausible hypotheses prior to data collection (Kerr, 1998). Therefore, HARKing promotes the development of narrow theories that are potentially based on false-positives (Kerr, 1998). Murphy and Aguinis (2017) observed that selective reporting of hypotheses can lead to considerable bias in the literature, especially when population parameters are subtle. Importantly, this discussion is not intended to discount exploratory hypothesis testing; progress in science is also based on hypotheses that gathered support after experimentation but seemed implausible at first (Franklin, 2005). However, if post hoc insights are not distinguished from a priori hypotheses, HARKing may lead to hypotheses that are less likely to be reproduced (Hollenbeck & Wright, 2017; Kerr, 1998).\n\nPrevalence of Questionable Research Practices. Publication bias, the file-drawer effect, and low power are longstanding issues in psychology, raising questions about the prevalence of QRPs (Cohen,\n\n1990; Greenwald, 1975; Rosenthal, 1979). John and colleagues (2012) questioned more than 2000 psychologists and estimated that over half had selectively reported dependent variables or results, rounded off p-values, collected or excluded data after testing for significance, and HARKed hypotheses at least once during their career. However, Fiedler and Schwarz (2016) reported that the actual frequencies of QRPs are much lower.",
    "is_useful": true,
    "question": "What are some of the consequences of practices like HARKing and selective reporting in research?"
  },
  {
    "text": "However, if post hoc insights are not distinguished from a priori hypotheses, HARKing may lead to hypotheses that are less likely to be reproduced (Hollenbeck & Wright, 2017; Kerr, 1998).\n\nPrevalence of Questionable Research Practices. Publication bias, the file-drawer effect, and low power are longstanding issues in psychology, raising questions about the prevalence of QRPs (Cohen,\n\n1990; Greenwald, 1975; Rosenthal, 1979). John and colleagues (2012) questioned more than 2000 psychologists and estimated that over half had selectively reported dependent variables or results, rounded off p-values, collected or excluded data after testing for significance, and HARKed hypotheses at least once during their career. However, Fiedler and Schwarz (2016) reported that the actual frequencies of QRPs are much lower. In addition, the prevalence of QRPs as well as the number of article retractions per journal have not been observed to increase (Fanelli, 2018). Furthermore, Head, Holman, Lanfear, Kahn, and Jennions (2015) examined distributions of p-values (i.e., p-curves) across research fields and observed that psychology showed the highest amount of pvalues that clustered around the .05 significance threshold. This pattern may indicate p-hacking since p-values are more likely to be close to .05 when, for example, data was collected until results reached statistical significance (Simonsohn, Nelson, & Simmons, 2014).",
    "is_useful": true,
    "question": "What are some concerns regarding questionable research practices in psychology? "
  },
  {
    "text": "However, Fiedler and Schwarz (2016) reported that the actual frequencies of QRPs are much lower. In addition, the prevalence of QRPs as well as the number of article retractions per journal have not been observed to increase (Fanelli, 2018). Furthermore, Head, Holman, Lanfear, Kahn, and Jennions (2015) examined distributions of p-values (i.e., p-curves) across research fields and observed that psychology showed the highest amount of pvalues that clustered around the .05 significance threshold. This pattern may indicate p-hacking since p-values are more likely to be close to .05 when, for example, data was collected until results reached statistical significance (Simonsohn, Nelson, & Simmons, 2014). However, the authors also obtained significant evidential value, indicating that the effects in question were likely to be true positives (Head et al., 2015). Thus, while QRPs appear to be present in psychological research, it is unclear to which degree the literature is biased (Fanelli, 2018; Murphy & Aguinis, 2017).\n\n#### Measures to Improve the Reproducibility of Results\n\nPublication bias and its possible repercussions on research practices are well documented, yet their precise impact on results reproducibility remain controversial (Ioannidis, 2005; Fanelli, 2018). The final section addresses proposed actions to enhance results reproducibility.\n\nPreregistration.",
    "is_useful": true,
    "question": "What measures are suggested to enhance the reproducibility of research results in the context of open science?"
  },
  {
    "text": "This pattern may indicate p-hacking since p-values are more likely to be close to .05 when, for example, data was collected until results reached statistical significance (Simonsohn, Nelson, & Simmons, 2014). However, the authors also obtained significant evidential value, indicating that the effects in question were likely to be true positives (Head et al., 2015). Thus, while QRPs appear to be present in psychological research, it is unclear to which degree the literature is biased (Fanelli, 2018; Murphy & Aguinis, 2017).\n\n#### Measures to Improve the Reproducibility of Results\n\nPublication bias and its possible repercussions on research practices are well documented, yet their precise impact on results reproducibility remain controversial (Ioannidis, 2005; Fanelli, 2018). The final section addresses proposed actions to enhance results reproducibility.\n\nPreregistration. Researchers have argued for the adoption of preregistered reports in the publication process, which entails the inclusion of an additional stage of peer-review prior to data collection (Nosek & Lakens, 2014). This step is argued to ensure that provisional acceptance of a manuscript will depend on the overall quality and relevance of a study (Ioannidis et al., 2014). This would eliminate incentives for publishing statistically significant results and engaging in QRPs and p-hacking (Ioannidis et al., 2014; Nosek & Lakens, 2014).",
    "is_useful": true,
    "question": "What measures can be implemented to enhance the reproducibility of results in research?"
  },
  {
    "text": "#### Measures to Improve the Reproducibility of Results\n\nPublication bias and its possible repercussions on research practices are well documented, yet their precise impact on results reproducibility remain controversial (Ioannidis, 2005; Fanelli, 2018). The final section addresses proposed actions to enhance results reproducibility.\n\nPreregistration. Researchers have argued for the adoption of preregistered reports in the publication process, which entails the inclusion of an additional stage of peer-review prior to data collection (Nosek & Lakens, 2014). This step is argued to ensure that provisional acceptance of a manuscript will depend on the overall quality and relevance of a study (Ioannidis et al., 2014). This would eliminate incentives for publishing statistically significant results and engaging in QRPs and p-hacking (Ioannidis et al., 2014; Nosek & Lakens, 2014). In addition, methodological flaws may be addressed before studies are conducted (Nosek & Lakens, 2014). Distinguishing post hoc insights from the initial predictions also reduces hindsight bias, overconfidence in one's own hypotheses, and HARKing (Nosek, Ebersole, DeHaven, & Mellor, 2018). Importantly, preregistration does not prohibit exploratory hypothesis testing but requires those results to be labeled as exploratory in the final publication (Nosek & Lakens, 2014).",
    "is_useful": true,
    "question": "What measures can be taken to improve the reproducibility of research results?"
  },
  {
    "text": "This step is argued to ensure that provisional acceptance of a manuscript will depend on the overall quality and relevance of a study (Ioannidis et al., 2014). This would eliminate incentives for publishing statistically significant results and engaging in QRPs and p-hacking (Ioannidis et al., 2014; Nosek & Lakens, 2014). In addition, methodological flaws may be addressed before studies are conducted (Nosek & Lakens, 2014). Distinguishing post hoc insights from the initial predictions also reduces hindsight bias, overconfidence in one's own hypotheses, and HARKing (Nosek, Ebersole, DeHaven, & Mellor, 2018). Importantly, preregistration does not prohibit exploratory hypothesis testing but requires those results to be labeled as exploratory in the final publication (Nosek & Lakens, 2014). Publishing preregistered studies is increasingly being incentivized: Examples include the \"Registered Replication Reports\" program by the Association for Psychological Science and the introduction of preregistration badges by the Center of Open Science (Association for Psychological Science, 2018; Kidwell et al., 2016). A range of journals, including Cortex and all journals by The British Psychological Society, have opened up the possibility for researchers to preregister studies (Chambers, 2013; The British Psychological Society, 2018).\n\nReplication Studies.",
    "is_useful": true,
    "question": "What practices are encouraged to improve the reliability and integrity of research findings in the context of open science?"
  },
  {
    "text": "Importantly, preregistration does not prohibit exploratory hypothesis testing but requires those results to be labeled as exploratory in the final publication (Nosek & Lakens, 2014). Publishing preregistered studies is increasingly being incentivized: Examples include the \"Registered Replication Reports\" program by the Association for Psychological Science and the introduction of preregistration badges by the Center of Open Science (Association for Psychological Science, 2018; Kidwell et al., 2016). A range of journals, including Cortex and all journals by The British Psychological Society, have opened up the possibility for researchers to preregister studies (Chambers, 2013; The British Psychological Society, 2018).\n\nReplication Studies. To improve results and inferential reproducibility, Zwaan and colleagues (2018) made the case for replication studies to become a common part of the research process. For instance, Gernsbacher (2018) proposed the inclusion of incremental replications in each study, that is, additional experiments that replicate the main findings and incrementally test the conditions in which they occur. However, if replication studies are to become frequent tools, it has to become a norm that inconclusive results from publication studies are not perceived as damaging to researchers' careers (Pennycook, 2018). Pennycook (2018) further notes that failures to replicate findings do not necessarily imply the use of QRPs or flawed methodological designs.",
    "is_useful": true,
    "question": "What practices are encouraged in open science to enhance the reproducibility and transparency of research findings?"
  },
  {
    "text": "Replication Studies. To improve results and inferential reproducibility, Zwaan and colleagues (2018) made the case for replication studies to become a common part of the research process. For instance, Gernsbacher (2018) proposed the inclusion of incremental replications in each study, that is, additional experiments that replicate the main findings and incrementally test the conditions in which they occur. However, if replication studies are to become frequent tools, it has to become a norm that inconclusive results from publication studies are not perceived as damaging to researchers' careers (Pennycook, 2018). Pennycook (2018) further notes that failures to replicate findings do not necessarily imply the use of QRPs or flawed methodological designs. To correct this perception, replication studies could be implemented in the training of social scientists, requiring, for instance, the implementation of replication studies during PhD programs (Kochari & Ostarek, 2018; Zwaan et al., 2018). However, direct replications may be difficult to implement for certain types of research (e.g., longitudinal studies), and critics have argued replication studies to only provide minimal value as long as publication bias distorts the literature (Coyne, 2016; Schmidt & Oh, 2016; for a detailed discussion, see Zwaan et al., 2018).\n\nOpen Science. Scientists are also increasingly endorsing open science practices to enhance methods and results reproducibility (Nosek et al., 2015; Open Science Collaboration, 2015).",
    "is_useful": true,
    "question": "What practices are being endorsed by scientists to enhance methods and results reproducibility in research?"
  },
  {
    "text": "Pennycook (2018) further notes that failures to replicate findings do not necessarily imply the use of QRPs or flawed methodological designs. To correct this perception, replication studies could be implemented in the training of social scientists, requiring, for instance, the implementation of replication studies during PhD programs (Kochari & Ostarek, 2018; Zwaan et al., 2018). However, direct replications may be difficult to implement for certain types of research (e.g., longitudinal studies), and critics have argued replication studies to only provide minimal value as long as publication bias distorts the literature (Coyne, 2016; Schmidt & Oh, 2016; for a detailed discussion, see Zwaan et al., 2018).\n\nOpen Science. Scientists are also increasingly endorsing open science practices to enhance methods and results reproducibility (Nosek et al., 2015; Open Science Collaboration, 2015). This includes making hypotheses and data available to the public and other researchers to increase transparency in science (Munaf\u00f2 et al., 2017). Current publication practices have been counteracting transparency by incentivizing \"good story scripts\", thereby requiring researchers to leave out large amounts of information (Munaf\u00f2 et al., 2017). Open science principles are argued to improve verification of experimental analyses, heighten accountability among scientists, and eliminate the file-drawer effect (Lerner & Tetlock, 1999; Nosek & Lakens, 2014; Nosek et al., 2015).",
    "is_useful": true,
    "question": "How do open science practices contribute to the reproducibility and transparency of scientific research?"
  },
  {
    "text": "Open Science. Scientists are also increasingly endorsing open science practices to enhance methods and results reproducibility (Nosek et al., 2015; Open Science Collaboration, 2015). This includes making hypotheses and data available to the public and other researchers to increase transparency in science (Munaf\u00f2 et al., 2017). Current publication practices have been counteracting transparency by incentivizing \"good story scripts\", thereby requiring researchers to leave out large amounts of information (Munaf\u00f2 et al., 2017). Open science principles are argued to improve verification of experimental analyses, heighten accountability among scientists, and eliminate the file-drawer effect (Lerner & Tetlock, 1999; Nosek & Lakens, 2014; Nosek et al., 2015). Recently, the Transparency and Openness Promotion (TOP) committee suggested a set of guidelines to transform journals' incentive structure (Nosek et al., 2015). These guidelines target transparency in design, analysis, and data, as well as standards for citations, preregistration, and replication studies (Nosek et al., 2015). The journal Science, for instance, is advocating to implement the TOP guidelines (McNutt, 2016). However, Drummond (2018) argued that open science is a laborious process which would require reviewers to spend increasing amounts of time on verifying the accuracy of results rather than focusing on the quality of research.",
    "is_useful": true,
    "question": "What are the benefits of adopting open science practices in scientific research?"
  },
  {
    "text": "Open science principles are argued to improve verification of experimental analyses, heighten accountability among scientists, and eliminate the file-drawer effect (Lerner & Tetlock, 1999; Nosek & Lakens, 2014; Nosek et al., 2015). Recently, the Transparency and Openness Promotion (TOP) committee suggested a set of guidelines to transform journals' incentive structure (Nosek et al., 2015). These guidelines target transparency in design, analysis, and data, as well as standards for citations, preregistration, and replication studies (Nosek et al., 2015). The journal Science, for instance, is advocating to implement the TOP guidelines (McNutt, 2016). However, Drummond (2018) argued that open science is a laborious process which would require reviewers to spend increasing amounts of time on verifying the accuracy of results rather than focusing on the quality of research. In addition, the frequency of QRPs is still contested, and intentional fraud is not a frequent occurrence nor considered to bias the psychological literature extensively, leaving doubts about the benefits of complete transparency (Fanelli, 2018).\n\nAlpha-Threshold Adjustment. Furthermore, a recent article by Benjamin and colleagues (2018) advocated adjusting the threshold for statistical significance from 0.05 to 0.005. While a lower alpha criterion does not guard against QRPs, p-hacking or HARKing, Benjamin and collaborators (2018) emphasized that a lower alpha threshold may reduce the amount of false-positive results.",
    "is_useful": true,
    "question": "What are some of the proposed benefits and challenges associated with the implementation of open science principles?"
  },
  {
    "text": "The journal Science, for instance, is advocating to implement the TOP guidelines (McNutt, 2016). However, Drummond (2018) argued that open science is a laborious process which would require reviewers to spend increasing amounts of time on verifying the accuracy of results rather than focusing on the quality of research. In addition, the frequency of QRPs is still contested, and intentional fraud is not a frequent occurrence nor considered to bias the psychological literature extensively, leaving doubts about the benefits of complete transparency (Fanelli, 2018).\n\nAlpha-Threshold Adjustment. Furthermore, a recent article by Benjamin and colleagues (2018) advocated adjusting the threshold for statistical significance from 0.05 to 0.005. While a lower alpha criterion does not guard against QRPs, p-hacking or HARKing, Benjamin and collaborators (2018) emphasized that a lower alpha threshold may reduce the amount of false-positive results. (Benjamin et al., 2018; Fanelli, Costas, & Ioannidis, 2017). However, McShane, Gal, Gelman, Robert, and Tackett (2017) argued that a more stringent alpha threshold might amplify biases that are associated with a dichotomous decision rule, such as overconfidence in results and disregard for meaningful contributions that do not meet the new threshold. Instead, the p-value should be relegated and treated continuously, shifting attention to factors that are usually viewed as secondary, such as quality and soundness of the design and hypotheses (McShane et al., 2017).",
    "is_useful": true,
    "question": "What are some challenges and recommendations associated with the implementation of open science practices?"
  },
  {
    "text": "While a lower alpha criterion does not guard against QRPs, p-hacking or HARKing, Benjamin and collaborators (2018) emphasized that a lower alpha threshold may reduce the amount of false-positive results. (Benjamin et al., 2018; Fanelli, Costas, & Ioannidis, 2017). However, McShane, Gal, Gelman, Robert, and Tackett (2017) argued that a more stringent alpha threshold might amplify biases that are associated with a dichotomous decision rule, such as overconfidence in results and disregard for meaningful contributions that do not meet the new threshold. Instead, the p-value should be relegated and treated continuously, shifting attention to factors that are usually viewed as secondary, such as quality and soundness of the design and hypotheses (McShane et al., 2017). McShane and colleagues (2017) also pointed towards a recent statement by the American Statistical Association, which indicated that statistical significance should not be considered synonymous with good scientific practice (Wasserstein & Lazar, 2016).\n\nAlternative Statistics. These considerations have led to suggestions to replace p-values in favor of alternative statistical approaches, such as confidence intervals and effect sizes (Gardner & Altman, 1986; Sullivan & Feinn, 2012; Thompson, 2002). Cumming (2014) argued that confidence intervals, in contract to p-values, express a range of uncertainty and are therefore suited to support a cumulative scientific process.",
    "is_useful": true,
    "question": "What are the criticisms regarding the use of p-values in scientific research and what alternative approaches are suggested?"
  },
  {
    "text": "Instead, the p-value should be relegated and treated continuously, shifting attention to factors that are usually viewed as secondary, such as quality and soundness of the design and hypotheses (McShane et al., 2017). McShane and colleagues (2017) also pointed towards a recent statement by the American Statistical Association, which indicated that statistical significance should not be considered synonymous with good scientific practice (Wasserstein & Lazar, 2016).\n\nAlternative Statistics. These considerations have led to suggestions to replace p-values in favor of alternative statistical approaches, such as confidence intervals and effect sizes (Gardner & Altman, 1986; Sullivan & Feinn, 2012; Thompson, 2002). Cumming (2014) argued that confidence intervals, in contract to p-values, express a range of uncertainty and are therefore suited to support a cumulative scientific process. Combined with meta-analyses and replication studies, the use of confidence intervals might be able to yield increasingly precise estimates over time (Cumming, 2014). The journal Psychological Science, for example, actively encourages researchers to report confidence intervals and effect sizes in order to avoid dichotomous decision-making (Eich, 2013).\n\nHowever, interpretations drawn from confidence intervals are contested (Morey, Hoekstra, Rouder, & Wagenmakers, 2016), and critics of frequentist statistics argue to embrace Bayesian methods (Dienes, 2011). For example, Wagenmakers and colleagues (2017) argued that a benefit of the Bayesian framework is that it allows quantifying evidence under the alternative hypothesis.",
    "is_useful": true,
    "question": "What statistical approaches are suggested as alternatives to p-values in open science research?"
  },
  {
    "text": "Cumming (2014) argued that confidence intervals, in contract to p-values, express a range of uncertainty and are therefore suited to support a cumulative scientific process. Combined with meta-analyses and replication studies, the use of confidence intervals might be able to yield increasingly precise estimates over time (Cumming, 2014). The journal Psychological Science, for example, actively encourages researchers to report confidence intervals and effect sizes in order to avoid dichotomous decision-making (Eich, 2013).\n\nHowever, interpretations drawn from confidence intervals are contested (Morey, Hoekstra, Rouder, & Wagenmakers, 2016), and critics of frequentist statistics argue to embrace Bayesian methods (Dienes, 2011). For example, Wagenmakers and colleagues (2017) argued that a benefit of the Bayesian framework is that it allows quantifying evidence under the alternative hypothesis. Specifically, the Bayes factor expresses the plausibility of the data under the alternative compared to the null hypothesis as a likelihood ratio, which is argued to provide interpretations that appeal to an intuitive understanding of statistics (Dienes, 2011; Jeffreys, 1961). Moreover, the Bayesian framework encourages accumulation of evidence by taking the prior probability into account and allowing for continuous decision-making (Wagenmakers et al., 2017). Further, sequential testing is not considered a QRP, but researchers are able to observe how strongly the data shifts their beliefs as the sample size grows (Etz & Vandekerckhove, 2016; Rouder, 2014).",
    "is_useful": true,
    "question": "What are the benefits of using confidence intervals and Bayesian methods in scientific research as opposed to p-values?"
  },
  {
    "text": "For example, Wagenmakers and colleagues (2017) argued that a benefit of the Bayesian framework is that it allows quantifying evidence under the alternative hypothesis. Specifically, the Bayes factor expresses the plausibility of the data under the alternative compared to the null hypothesis as a likelihood ratio, which is argued to provide interpretations that appeal to an intuitive understanding of statistics (Dienes, 2011; Jeffreys, 1961). Moreover, the Bayesian framework encourages accumulation of evidence by taking the prior probability into account and allowing for continuous decision-making (Wagenmakers et al., 2017). Further, sequential testing is not considered a QRP, but researchers are able to observe how strongly the data shifts their beliefs as the sample size grows (Etz & Vandekerckhove, 2016; Rouder, 2014). However, Bayesian statistics does not resolve publication bias nor guard against the use of QRPs to inflate the importance of research findings (Banks et al., 2016; Savalei & Dunn, 2015). In fact, a common criticism is that the specification of a prior probability can introduce additional bias into the statistical analysis (Efron, 2013).\n\n#### Conclusion\n\nTo conclude, the current debate on reproducibility in psychology originated from a variety of reports indicating that results cannot be directly replicated (Baker, 2016; Open Science Collaboration, 2015).",
    "is_useful": true,
    "question": "What advantages and challenges are associated with using the Bayesian framework in research, particularly regarding evidence accumulation and bias?"
  },
  {
    "text": "Further, sequential testing is not considered a QRP, but researchers are able to observe how strongly the data shifts their beliefs as the sample size grows (Etz & Vandekerckhove, 2016; Rouder, 2014). However, Bayesian statistics does not resolve publication bias nor guard against the use of QRPs to inflate the importance of research findings (Banks et al., 2016; Savalei & Dunn, 2015). In fact, a common criticism is that the specification of a prior probability can introduce additional bias into the statistical analysis (Efron, 2013).\n\n#### Conclusion\n\nTo conclude, the current debate on reproducibility in psychology originated from a variety of reports indicating that results cannot be directly replicated (Baker, 2016; Open Science Collaboration, 2015). Publication bias assumes a central role in the debate on reproducibility and is argued to cause a skewed incentive system that rewards publication over sound science, thereby encouraging QRPs and methodologically flawed studies that worsen results reproducibility (Greenwald, 1975; Simmons et al., 2011). As such, the view that psychology faces a crisis is increasingly common (Baker, 2016; Fanelli, 2018). However, there is no precise definition of results reproducibility, and concerns about methodological aspects of previous replication efforts have been raised (Gilbert et al., 2016; Goodman et al., 2016).",
    "is_useful": true,
    "question": "What challenges does publication bias pose to the reproducibility of research findings in the field of psychology?"
  },
  {
    "text": "#### Conclusion\n\nTo conclude, the current debate on reproducibility in psychology originated from a variety of reports indicating that results cannot be directly replicated (Baker, 2016; Open Science Collaboration, 2015). Publication bias assumes a central role in the debate on reproducibility and is argued to cause a skewed incentive system that rewards publication over sound science, thereby encouraging QRPs and methodologically flawed studies that worsen results reproducibility (Greenwald, 1975; Simmons et al., 2011). As such, the view that psychology faces a crisis is increasingly common (Baker, 2016; Fanelli, 2018). However, there is no precise definition of results reproducibility, and concerns about methodological aspects of previous replication efforts have been raised (Gilbert et al., 2016; Goodman et al., 2016). Therefore, researchers have argued that the reproducibility crisis is exaggerated and that supporting the perception of a reproducibility crisis may be damaging to psychology (Fanelli, 2018; Fiedler & Schwarz, 2016).\n\nSeveral possibilities to enhance methods, results, and inferential reproducibility have been proposed, including preregistered reports, replication studies, open science, alpha-threshold adjustment, and the use of alternative statistical approaches (Benjamin et al., 2018; Cumming, 2014; Dienes, 2011; Nosek & Lakens, 2014; Nosek et al., 2015; Zwaan et al., 2018).",
    "is_useful": true,
    "question": "What strategies have been proposed to improve reproducibility in scientific research?"
  },
  {
    "text": "However, there is no precise definition of results reproducibility, and concerns about methodological aspects of previous replication efforts have been raised (Gilbert et al., 2016; Goodman et al., 2016). Therefore, researchers have argued that the reproducibility crisis is exaggerated and that supporting the perception of a reproducibility crisis may be damaging to psychology (Fanelli, 2018; Fiedler & Schwarz, 2016).\n\nSeveral possibilities to enhance methods, results, and inferential reproducibility have been proposed, including preregistered reports, replication studies, open science, alpha-threshold adjustment, and the use of alternative statistical approaches (Benjamin et al., 2018; Cumming, 2014; Dienes, 2011; Nosek & Lakens, 2014; Nosek et al., 2015; Zwaan et al., 2018). These options are widely discussed and while they may potentially resolve longstanding issues (e.g., publication bias), adopting some of these guidelines may be costly and shift the focus from conducting innovative research (Crandall & Sherman, 2016; Drummond, 2018; Nosek & Lakens, 2014). Choosing and implementing the appropriate methods will require a joint effort from all members of the psychological scientific community (Nosek et al., 2015).\n\n## Acknowledgements\n\nI am grateful to Oliver Lindemann for providing valuable comments on an earlier version of this manuscript. In addition, I thank Mhairi Gador-Whyte for proofreading.",
    "is_useful": true,
    "question": "What are some proposed methods to enhance results reproducibility in research?"
  },
  {
    "text": "These options are widely discussed and while they may potentially resolve longstanding issues (e.g., publication bias), adopting some of these guidelines may be costly and shift the focus from conducting innovative research (Crandall & Sherman, 2016; Drummond, 2018; Nosek & Lakens, 2014). Choosing and implementing the appropriate methods will require a joint effort from all members of the psychological scientific community (Nosek et al., 2015).\n\n## Acknowledgements\n\nI am grateful to Oliver Lindemann for providing valuable comments on an earlier version of this manuscript. In addition, I thank Mhairi Gador-Whyte for proofreading.\n\n## Conflicts of Interest\n\nThe author has no conflicts of interest to declare.\n\n### References\n\n- Association for Psychological Science. (2018). Registered Replication Reports. Retrieved from\n- https://www.psychologicalscience.org/publications/replication Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature News, 533(7604), 452\u2013454. https://doi.org/10.1038/533452a\n- Bakker, M., van Dijk, A., & Wicherts, J. M. (2012). The rules of the game called psychological science. Perspectives on Psychological Science, 7(6), 543\u2013554.",
    "is_useful": true,
    "question": "What challenges might arise from implementing new guidelines in the field of psychological science, particularly concerning research innovation and publication practices?"
  },
  {
    "text": "(2005). Teacher expectations and selffulfilling prophecies: Knowns and unknowns, resolved and unresolved controversies. Personality and Social Psychology Review, 9(2), 131\u2013155.\n- https://doi.org/10.1207/s15327957pspr0902_3 Kerr, N. L. (1998). HARKing: Hypothesizing after the results are known. Personality and Social Psychology Review, 2(3), 196\u2013217. https://doi.org/10.1207/s15327957pspr0203_4\n- Kidwell, M. C., Lazarevi\u0107, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., ... Errington, T. M. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLOS Biology, 14(5). https://doi.org/10.1371/journal.pbio.1002456\n- Klein, R. A., Ratliff, K. A., Vianello, M., Adams Jr, R. B., Bahn\u00edk, \u0160., Bernstein, M. J., ... Cemalcilar, Z. (2014). Investigating variation in replicability. Social Psychology, 45, 142\u2013152.",
    "is_useful": true,
    "question": "What are some methods proposed to increase transparency and replicability in research practices within open science?"
  },
  {
    "text": "![](_page_0_Picture_0.jpeg)\n\nAccident Analysis and Prevention 36 (2004) 495\u2013500\n\n![](_page_0_Picture_2.jpeg)\n\n### Viewpoint\n\n# The harm done by tests of significance\n\n## Ezra Hauer\u2217\n\n*35 Merton Street, Apt. 1706, Toronto, Ont., Canada M4S 3G4*\n\n#### **Abstract**\n\nThree historical episodes in which the application of null hypothesis significance testing (NHST) led to the mis-interpretation of data are described. It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.\n\n\u00a9 2003 Elsevier Ltd. All rights reserved.\n\n*Keywords:* Significance; Statistical hypothesis; Scientific method\n\n#### **1. Introduction**\n\nMost university students are taught the rudiments of statistical null hypothesis significance testing (NHST for short). As a result, later in life, either as users of scientific knowledge or as its creators, they tend to regard NHST to be the hallmark of sound science, an effective safeguard against spurious findings. That the logical foundation and the scientific merit of NHST is questioned by prominent statisticians and scientists is not mentioned in text books and courses on introductory statistics; therefore it is not common knowledge. Yet, volumes have been written about the 'significance controversy' (see, e.g. books by Chow, 1996; Harlow et al., 1997).",
    "is_useful": true,
    "question": "What are the criticisms associated with the application of null hypothesis significance testing in the scientific method?"
  },
  {
    "text": "It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.\n\n\u00a9 2003 Elsevier Ltd. All rights reserved.\n\n*Keywords:* Significance; Statistical hypothesis; Scientific method\n\n#### **1. Introduction**\n\nMost university students are taught the rudiments of statistical null hypothesis significance testing (NHST for short). As a result, later in life, either as users of scientific knowledge or as its creators, they tend to regard NHST to be the hallmark of sound science, an effective safeguard against spurious findings. That the logical foundation and the scientific merit of NHST is questioned by prominent statisticians and scientists is not mentioned in text books and courses on introductory statistics; therefore it is not common knowledge. Yet, volumes have been written about the 'significance controversy' (see, e.g. books by Chow, 1996; Harlow et al., 1997). I have written about the paralyzing effect of statistical significance on road safety research a long time ago (Hauer, 1983) and did not plan to return to this topic again. However, the road safety literature is a constant reminder of the continuing real harm done by NHST. The harm is that of using sound data to reach unsound conclusions thereby giving sustenance to non-sensical beliefs. In the end, these non-sensical beliefs cause needless loss of life and limb.\n\nIn this paper, I will not dwell on the common criticisms of NHST. Therefore, it will not be necessary to explain its fine points.",
    "is_useful": true,
    "question": "What is one criticism of the widely taught statistical null hypothesis significance testing (NHST) regarding its impact on scientific knowledge? "
  },
  {
    "text": "Yet, volumes have been written about the 'significance controversy' (see, e.g. books by Chow, 1996; Harlow et al., 1997). I have written about the paralyzing effect of statistical significance on road safety research a long time ago (Hauer, 1983) and did not plan to return to this topic again. However, the road safety literature is a constant reminder of the continuing real harm done by NHST. The harm is that of using sound data to reach unsound conclusions thereby giving sustenance to non-sensical beliefs. In the end, these non-sensical beliefs cause needless loss of life and limb.\n\nIn this paper, I will not dwell on the common criticisms of NHST. Therefore, it will not be necessary to explain its fine points. What I wish to demonstrate here is that NHST, as applied in research on road safety, often leads to the subversion of reason and of science. I will do so by relating three historical episodes.\n\n#### **2. Episode 1: the right-turn-on-red story**\n\nThis is an old story (see Hauer, 1991). The practice of allowing right-turn-on-red' (or RTOR) at signalized intersections started in California in 1937 (some say that is started earlier, in New York City). For a long time, it was frowned upon by engineers in other states who had safety concerns. A major impetus for the general adoption of RTOR was the 1973 oil crisis and the \"Energy Policy and Savings Act\" adopted by Congress in 1975.",
    "is_useful": true,
    "question": "What are some negative consequences of misapplying statistical methods in road safety research?"
  },
  {
    "text": "In this paper, I will not dwell on the common criticisms of NHST. Therefore, it will not be necessary to explain its fine points. What I wish to demonstrate here is that NHST, as applied in research on road safety, often leads to the subversion of reason and of science. I will do so by relating three historical episodes.\n\n#### **2. Episode 1: the right-turn-on-red story**\n\nThis is an old story (see Hauer, 1991). The practice of allowing right-turn-on-red' (or RTOR) at signalized intersections started in California in 1937 (some say that is started earlier, in New York City). For a long time, it was frowned upon by engineers in other states who had safety concerns. A major impetus for the general adoption of RTOR was the 1973 oil crisis and the \"Energy Policy and Savings Act\" adopted by Congress in 1975. Our story begins in 1976 when a consultant submitted a report about the safety repercussions of RTOR to the Governor and General Assembly of Virginia. The studies then extant were deemed deficient and the consultant did his own before\u2013after study at 20 intersections with the results in Table 1.\n\nLooking at the data in Table 1, persons without training in statistics would think that after RTOR was allowed, these intersections were somewhat less safe. However, the consultant concluded, quite correctly, that the change was not statistically significant.",
    "is_useful": true,
    "question": "What are some potential consequences of using Null Hypothesis Significance Testing (NHST) in research on road safety?"
  },
  {
    "text": "Thus, the consultant said one\n\n<sup>\u2217</sup> Tel.: +1-416-978-5976; fax: +1-416-978-5054.\n\n<sup><</sup>i>E-mail address: ezra.hauer@utoronto.ca (E. Hauer).\n\nTable 1 The Virginia RTOR study\n\n|  | Before RTOR | After RTOR |\n| --- | --- | --- |\n|  | signing | signing |\n| Fatal crashes | 0 | 0 |\n| Personal injury crashes | 43 | 60 |\n| Persons injured | 69 | 72 |\n| Property damage crashes | 265 | 277 |\n| Property damage (US$) | 161243 | 170807 |\n| Total crashes | 308 | 337 |\n\nthing and the Commissioner transmitted something entirely different.\n\nMore published studies followed. One study in 1977 found that there were 19 crashes involving right turning vehicles before and 24 after allowing RTOR and \"this increase in accidents in not statistically significant, and therefore it cannot be said that this increase in RTOR accidents is attributable to RTOR\". And so the sequence of small studies all pointing in the same direction but with statistically not significant results continued to accumulate, till that last study which I followed was published in 1983. While 287 crashes to right turning vehicles were expected, 313 were counted. The authors concluded, once again, that there was no significant difference in vehicular crashes. Similarly for pedestrians.",
    "is_useful": true,
    "question": "What does research indicate about the significance of crash increases after allowing right turns on red (RTOR)?"
  },
  {
    "text": "More published studies followed. One study in 1977 found that there were 19 crashes involving right turning vehicles before and 24 after allowing RTOR and \"this increase in accidents in not statistically significant, and therefore it cannot be said that this increase in RTOR accidents is attributable to RTOR\". And so the sequence of small studies all pointing in the same direction but with statistically not significant results continued to accumulate, till that last study which I followed was published in 1983. While 287 crashes to right turning vehicles were expected, 313 were counted. The authors concluded, once again, that there was no significant difference in vehicular crashes. Similarly for pedestrians. In one state, 74 were expected and 92 occurred; in another state 81 were expected and 87 occurred. An yet, the authors concluded \"... that there is no statistically significant difference ... (in) pedestrian accidents before and after RTOR. There is no reason to suspect that pedestrian accidents involving RT operations (right turns) have increased after the adoption of RTOR in either state\".\n\nAfter RTOR became nearly universally used in North America, several large data sets became available and the adverse effect of RTOR could be established (Zador et al., 1982; Preusser et al., 1982).\n\nThe problem is clear. Researchers obtain real data which, while noisy, time and again point in a certain direction.",
    "is_useful": true,
    "question": "What challenges do researchers face in obtaining statistically significant data in studies related to vehicular and pedestrian accidents after implementing right turn on red (RTOR) policies?"
  },
  {
    "text": "After RTOR became nearly universally used in North America, several large data sets became available and the adverse effect of RTOR could be established (Zador et al., 1982; Preusser et al., 1982).\n\nThe problem is clear. Researchers obtain real data which, while noisy, time and again point in a certain direction. However, instead of saying: \"here is my estimate of the safety effect, here is its precision, and this is how what I found relates to previous findings\", the data is processed by NHST, and the researcher says, correctly but pointlessly: \"I cannot be sure that the safety effect is not zero\". Occasionally, the researcher adds, this time incorrectly and unjustifiably, a statement to the effect that: \"since the result is not statistically significant, it is best to assume the safety effect to be zero\". In this manner, good data are drained of real content, the direction of empirical conclusions reversed, and ordinary human and scientific reasoning is turned on its head for the sake of a venerable ritual. As to the habit of subjecting the data from each study to the NHST separately, as if no previous knowledge existed, Edwards (1976, p. 180) notes that \"it is like trying to sink a battleship by firing lead shot at it for a long time\".",
    "is_useful": true,
    "question": "What issues arise from the use of statistical methods like NHST in the interpretation of research data in the context of open science?"
  },
  {
    "text": "And yet, because of the paucity of the data, none of these reductions proved statistically significant. But quasi-science wins again; and so, in their Conclusion section the authors write:\n\nThe study could not discern any statistically significant differences in either crash rate or severity rate between two- and four-foot shoulder installations. Unless (other) benefits ... are considered important to practitioners, this study does not show the increased construction cost of four-foot shoulders on state routes to be justified by an increase in traffic safety (p. 37).\n\nThe authors are right in saying that none of the differences were statistically significant. However, the authors are entirely wrong to spin this into meaning that shoulder paving cannot be justified by its safety effect. They did not do any cost-effect calculation. They seemed to have assumed that because the estimates in the rightmost column of Table 1 are not statistically significant, it is good form to take them to be zero. This makes no sense. It is the estimates in the rightmost column of Table 2, not zero, which represent the most likely safety effect of shoulder paving when based on this study. The absence of statistical significance does not mean and should never be taken to mean that 0 is the most likely estimate.\n\nJust as with RTOR, when articles of this kind are published in the professional literature, they are taken seriously and may affect real decisions. In this case, because money could be saved if shoulders do not need to be paved, these research results were already considered by a committee of the Association of State Highway and Transportation Officials.",
    "is_useful": true,
    "question": "What is the implication of a study when it does not find statistically significant differences in data, particularly in relation to decision-making in public safety and infrastructure spending?"
  },
  {
    "text": "However, the authors are entirely wrong to spin this into meaning that shoulder paving cannot be justified by its safety effect. They did not do any cost-effect calculation. They seemed to have assumed that because the estimates in the rightmost column of Table 1 are not statistically significant, it is good form to take them to be zero. This makes no sense. It is the estimates in the rightmost column of Table 2, not zero, which represent the most likely safety effect of shoulder paving when based on this study. The absence of statistical significance does not mean and should never be taken to mean that 0 is the most likely estimate.\n\nJust as with RTOR, when articles of this kind are published in the professional literature, they are taken seriously and may affect real decisions. In this case, because money could be saved if shoulders do not need to be paved, these research results were already considered by a committee of the Association of State Highway and Transportation Officials. The danger is that the use of the NHST ritual may lead to incorrect guidance for practice and thereby to unjustified loss of life and limb.\n\nIt can be argued that these two episodes represent a mis-application of the NHST. The mis-application being that the statements about absence of statistical significance were taken to mean that a measure or intervention had no effect. Unfortunately, neither the readers of the professional literature nor many contributors to it are clear about this distinction. Therefore, if what has been presented are instances of mis-application, one must conclude, at least, that NHST is given to common mis-application in the hands of many users.",
    "is_useful": true,
    "question": "What are the potential consequences of misinterpreting statistical significance in research findings related to public safety decisions? "
  },
  {
    "text": "Just as with RTOR, when articles of this kind are published in the professional literature, they are taken seriously and may affect real decisions. In this case, because money could be saved if shoulders do not need to be paved, these research results were already considered by a committee of the Association of State Highway and Transportation Officials. The danger is that the use of the NHST ritual may lead to incorrect guidance for practice and thereby to unjustified loss of life and limb.\n\nIt can be argued that these two episodes represent a mis-application of the NHST. The mis-application being that the statements about absence of statistical significance were taken to mean that a measure or intervention had no effect. Unfortunately, neither the readers of the professional literature nor many contributors to it are clear about this distinction. Therefore, if what has been presented are instances of mis-application, one must conclude, at least, that NHST is given to common mis-application in the hands of many users. Advocates of NHST could perhaps argue that what is needed are better educated users. This is a vain hope. As will be shown next, not only readers and contributors to learned journals are given to mis-application of the NHST, prominent statisticians suffer from the same affliction.\n\n#### **4. Episode 3: speed limit increases**\n\nBalkin and Ord (2002) published an influential study about the effect on fatal crashes of speed limit increases in the interstate highway system. Using structural modeling of time series data they estimated for each state what change in fatal crashes could be attributed to the speed limit increases implemented on interstate highways in 1987 and in 1995.",
    "is_useful": true,
    "question": "What are the potential consequences of mis-applying the NHST in professional literature?"
  },
  {
    "text": "As in the previous episodes, data painted a characteristically fuzzy but reasonable reflection of reality. However, when good data is passed through the NHST filter, a negative tends to emerge; black turns to white and white to black.\n\nThe Balkin and Ord paper has been formally discussed (pp. 13\u201326) by several prominent statisticians (J. Ledolter, M.D. Fotaine, T.T. Qu, K. Zimmerman, C.H. Spiegelman and A. Harvey). They comment extensively about several aspects of the statistical approach. Surprisingly, no question was raised about the use of NHST, about the appropriateness of subjecting each state separately to a NHST, or about the legitimacy of conclusions drawn in this manner. The use of NHST has received no comment.\n\n#### **5. Summary**\n\nThe RTOR story shows how easy it is for laymen to confuse 'not significant' in the statistical sense with 'not important' in the common sense. The confusion is not merely semantic and is not confined to persons without statistical education. As is evident from the shoulder-paving episode, even the statistically sophisticated believed that a non-rejection of the 'no-effect' null hypothesis amounts to some kind of confirmation that the data show that there was no effect.",
    "is_useful": true,
    "question": "What issues arise from the use of null hypothesis significance testing (NHST) in statistical analysis, particularly regarding the interpretation of results?"
  },
  {
    "text": "Fotaine, T.T. Qu, K. Zimmerman, C.H. Spiegelman and A. Harvey). They comment extensively about several aspects of the statistical approach. Surprisingly, no question was raised about the use of NHST, about the appropriateness of subjecting each state separately to a NHST, or about the legitimacy of conclusions drawn in this manner. The use of NHST has received no comment.\n\n#### **5. Summary**\n\nThe RTOR story shows how easy it is for laymen to confuse 'not significant' in the statistical sense with 'not important' in the common sense. The confusion is not merely semantic and is not confined to persons without statistical education. As is evident from the shoulder-paving episode, even the statistically sophisticated believed that a non-rejection of the 'no-effect' null hypothesis amounts to some kind of confirmation that the data show that there was no effect. Moreover, the speed-limit episode shows that even scholars who teach statistics and write text books on the subject, even they do not always distinguish between what is an estimate and what is a not-rejected (but otherwise unsupported) null hypothesis; even they, under the spell of the not-rejected zeros, read into the data the opposite of what it says. Therefore, one should not entertain the hope that a more informed use of the NHST might help avoid its many pitfalls and shortcomings. Experience shows that the ritual is so pervasively misapplied as to be simply unfit for use.\n\nIn all three episodes, the use of NHST led to conclusions which are incorrect and contrary to a straightforward interpretation of the data.",
    "is_useful": true,
    "question": "What are the potential misunderstandings and pitfalls associated with the use of null hypothesis significance testing (NHST) in research?"
  },
  {
    "text": "As is evident from the shoulder-paving episode, even the statistically sophisticated believed that a non-rejection of the 'no-effect' null hypothesis amounts to some kind of confirmation that the data show that there was no effect. Moreover, the speed-limit episode shows that even scholars who teach statistics and write text books on the subject, even they do not always distinguish between what is an estimate and what is a not-rejected (but otherwise unsupported) null hypothesis; even they, under the spell of the not-rejected zeros, read into the data the opposite of what it says. Therefore, one should not entertain the hope that a more informed use of the NHST might help avoid its many pitfalls and shortcomings. Experience shows that the ritual is so pervasively misapplied as to be simply unfit for use.\n\nIn all three episodes, the use of NHST led to conclusions which are incorrect and contrary to a straightforward interpretation of the data. The authors did not say: \"it looks as if the measure (allowing RTOR, not paving shoulders, or increasing speed limits) increases accidents, but we are not sufficiently certain\". The authors said that: \"there is no reason to suspect that pedestrian accidents involving RT operations (right turns) have increased after the adoption of RTOR in either state\" and that \"... this study does not show the increased construction cost of four-foot shoulders on state routes to be justified by an increase in traffic safety\" and that \"the results cast doubt on the blanket claim that higher speed limits and higher fatalities are directly related\". This tendency of NHST to strip useful data of their meaning, is the most disturbing aspect of its wide application.",
    "is_useful": true,
    "question": "What are some of the issues and pitfalls associated with the use of Null Hypothesis Significance Testing (NHST) in research conclusions?"
  },
  {
    "text": "This tendency of NHST to strip useful data of their meaning, is the most disturbing aspect of its wide application. In the context of progress towards better factual knowledge it amounts to a learning disability. In the context of an aid to public decision-making and policy it leads to misapplication of resources and unnecessary loss of life and limb.\n\nIn all three episodes, the important question was about the likely effect of a measure on crashes. But, instead of asking \"how many more crashes?\" and the authors chose to ask \"are we sufficiently sure that the effect was not zero?\" This substitution of questions led to all the subsequent entanglements. These can all be avoided by not testing statistical hypotheses when the research question is about the effect of some treatment; by returning to common sense and the mainstream of science and providing estimates of effect magnitude and its standard error instead.\n\nThe notion that progress in road safety research would be faster were the habitual use of NHST abandoned evokes strong reactions. The usual comment is that the fault is not in the procedure itself but in its frequent misapplication. Frequent misapplication, when it comes after generations of teaching and education, is in itself a serious fault. Misapplication in the hands of even the tone-givers of statistical practice should give pause to all.\n\n#### **6. A postscript**\n\nI sent a draft of this paper to a colleague who, after reading, commented that the paper would be more balanced if I noted that NHST could perhaps lead to correct conclusions if those who used it remembered that the application of the Neyman\u2013Pearson procedure requires the ascertainment of the Type II error (i.e.",
    "is_useful": true,
    "question": "What are the potential consequences of the over-reliance on null hypothesis significance testing (NHST) in research, particularly in public decision-making and road safety?"
  },
  {
    "text": "of not rejecting the null hypothesis when some alternative hypothesis is true). Attached to his e-mail was a draft paper of his own. He argues in his paper that the kind of data aggregation which is inherent in multivariate regressions can lead to incorrect results. To illustrate this, he generated pedestrian crash data by simulation on the assumption that the probability of a crash is a function of traffic speed and volume. The output of the simulation was then used as input into a regression the aim of which is to estimate the probability of a pedestrian crash as a function of traffic volume and speed. Looking at the result he notes that \"... the (regression) coefficient corresponding to traffic volume was significantly different from zero, but that corresponding to mean speed was not. This suggests that ... speed does not need to be considered in assessing pedestrian risk\". His principal argument is that multivariate regression incorrectly shows that speed has no effect on crashes when in the data-generating process speed did play a role. However, the regression did show that speed is positively associated with crashes. It is only after he subjected it to an NHST that he concluded that because the parameter is not significant, it is zero! Thus, to the same message in which he advocates the correct use of NHST, he appends a paper in which no attempt is made to do so. In this case, as in the myriads other regressions performed daily, if the 't' statistic of a regression coefficient is not significant, the regression constant is taken to be 0 and the variable is dropped.",
    "is_useful": true,
    "question": "What are the potential pitfalls of using multivariate regression when assessing the impact of variables on outcomes in research?"
  },
  {
    "text": "# P Values and Statistical Practice\n\n*Andrew Gelman*\n\nSander Greenland and Charles Poole1 accept that P values are here to stay but recognize that some of their most common interpretations have problems. The casual view of the P value as posterior probability of the truth of the null hypothesis is false and not even close to valid under any reasonable model, yet this misunderstanding persists even in high-stakes settings (as discussed, for example, by Greenland in 2011).2 The formal view of the P value as a probability conditional on the null is mathematically correct but typically irrelevant to research goals (hence, the popularity of alternative\u2014if wrong\u2014interpretations). A Bayesian interpretation based on a spike-and-slab model makes little sense in applied contexts in epidemiology, political science, and other fields in which true effects are typically nonzero and bounded (thus violating both the \"spike\" and the \"slab\" parts of the model).\n\nI find Greenland and Poole's1 perspective to be valuable: it is important to go beyond criticism and to understand what information is actually contained in a P value. These authors discuss some connections between P values and Bayesian posterior probabilities. I am not so optimistic about the practical value of these connections. Conditional on the continuing omnipresence of P values in applications, however, these are important results that should be generally understood.\n\nGreenland and Poole1 make two points. First, they describe how P values approximate posterior probabilities under prior distributions that contain little information relative to the data:\n\n> This misuse [of P values] may be lessened by recognizing correct Bayesian interpretations.",
    "is_useful": true,
    "question": "What are some common misconceptions about P values in research, and why is it important to understand their proper interpretation in the context of open science?"
  },
  {
    "text": "A Bayesian interpretation based on a spike-and-slab model makes little sense in applied contexts in epidemiology, political science, and other fields in which true effects are typically nonzero and bounded (thus violating both the \"spike\" and the \"slab\" parts of the model).\n\nI find Greenland and Poole's1 perspective to be valuable: it is important to go beyond criticism and to understand what information is actually contained in a P value. These authors discuss some connections between P values and Bayesian posterior probabilities. I am not so optimistic about the practical value of these connections. Conditional on the continuing omnipresence of P values in applications, however, these are important results that should be generally understood.\n\nGreenland and Poole1 make two points. First, they describe how P values approximate posterior probabilities under prior distributions that contain little information relative to the data:\n\n> This misuse [of P values] may be lessened by recognizing correct Bayesian interpretations. For example, under weak priors, 95% confidence intervals approximate 95% posterior probability intervals, one-sided P values approximate directional posterior probabilities, and point estimates approximate posterior medians.\n\nI used to think this way, too (see many examples in our books), but in recent years have moved to the position that I do not trust such direct posterior probabilities. Unfortunately, I think we cannot avoid informative priors if we wish to make reasonable unconditional probability statements. To put it another way, I agree with the mathematical truth of the quotation above, but I think it can mislead in practice because of serious problems with apparently noninformative or weak priors.",
    "is_useful": true,
    "question": "What is the significance of understanding P values in the context of Bayesian interpretations for applied fields?"
  },
  {
    "text": "Greenland and Poole1 make two points. First, they describe how P values approximate posterior probabilities under prior distributions that contain little information relative to the data:\n\n> This misuse [of P values] may be lessened by recognizing correct Bayesian interpretations. For example, under weak priors, 95% confidence intervals approximate 95% posterior probability intervals, one-sided P values approximate directional posterior probabilities, and point estimates approximate posterior medians.\n\nI used to think this way, too (see many examples in our books), but in recent years have moved to the position that I do not trust such direct posterior probabilities. Unfortunately, I think we cannot avoid informative priors if we wish to make reasonable unconditional probability statements. To put it another way, I agree with the mathematical truth of the quotation above, but I think it can mislead in practice because of serious problems with apparently noninformative or weak priors.\n\nSecond, the main proposal made by Greenland and Poole is to interpret P values as bounds on posterior probabilities:\n\n> [U]nder certain conditions, a one-sided P value for a prior median provides an approximate lower bound on the posterior probability that the point estimate is on the wrong side of that median.\n\nThis is fine, but when sample sizes are moderate or small (as is common in epidemiology and social science), posterior probabilities will depend strongly on the prior distribution. Although I do not see much direct value in a lower bound, I am intrigued by Greenland and Poole's1 point that \"if one uses an informative prior to derive the posterior probability of\n\n*Editors' note: Related articles appear on pages 62 and 73.",
    "is_useful": true,
    "question": "What are the implications of P values and prior distributions in Bayesian interpretations of statistical results in open science?"
  },
  {
    "text": "*\n\nISSN: 1044-3983/13/2401-0069\n\nEpidemiology\n\nGelman\n\nDeepa\n\n00\n\n00\n\n2012\n\n00\n\n00\n\n2012\n\nP Values and Statistical Practice\n\n10.1097/EDE.0b013e31827886f7\n\nPartially supported by the Institute for Education Sciences (grant ED-GRANTS-032309-005) and the U.S. National Science Foundation (grant SES-1023189). From the Departments of Statistics and Political Science, Columbia University, New York, NY.\n\nCorrespondence: Andrew Gelman, Departments of Statistics and Political Science, Columbia University, New York, NY 10027. E-mail: gelman@stat.columbia.edu. Copyright \u00a9 2012 by Lippincott William and Wilkins\n\nDOI: 10.1097/EDE.0b013e31827886f7\n\nthe point estimate being in the wrong direction, P0 /2 provides a reference point indicating how much the prior information influenced that posterior probability.\" This connection could be useful to researchers working in an environment in which P values are central to communication of statistical results.\n\nIn presenting my view of the limitations of Greenland and Poole's1 points, I am leaning heavily on their own work, in particular on their emphasis that, in real problems, prior information is always available and is often strong enough to have an appreciable impact on inferences.\n\nBefore explaining my position, I will briefly summarize how I view classical P values and my experiences.",
    "is_useful": true,
    "question": "What role do prior information and P values play in the communication of statistical results in epidemiology?"
  },
  {
    "text": "E-mail: gelman@stat.columbia.edu. Copyright \u00a9 2012 by Lippincott William and Wilkins\n\nDOI: 10.1097/EDE.0b013e31827886f7\n\nthe point estimate being in the wrong direction, P0 /2 provides a reference point indicating how much the prior information influenced that posterior probability.\" This connection could be useful to researchers working in an environment in which P values are central to communication of statistical results.\n\nIn presenting my view of the limitations of Greenland and Poole's1 points, I am leaning heavily on their own work, in particular on their emphasis that, in real problems, prior information is always available and is often strong enough to have an appreciable impact on inferences.\n\nBefore explaining my position, I will briefly summarize how I view classical P values and my experiences. For more background, I recommend the discussion by Krantz3 of null hypothesis testing in psychology research.\n\n# **WHAT IS A** P **VALUE IN PRACTICE?**\n\nThe P value is a measure of discrepancy of the fit of a model or \"null hypothesis\" H to data y. Mathematically, it is defined as Pr(T(yrep)>T(y)|H), where yrep represents a hypothetical replication under the null hypothesis and T is a test statistic (ie, a summary of the data, perhaps tailored to be sensitive to departures of interest from the model).",
    "is_useful": true,
    "question": "What role do P values play in communicating statistical results in research?"
  },
  {
    "text": "In presenting my view of the limitations of Greenland and Poole's1 points, I am leaning heavily on their own work, in particular on their emphasis that, in real problems, prior information is always available and is often strong enough to have an appreciable impact on inferences.\n\nBefore explaining my position, I will briefly summarize how I view classical P values and my experiences. For more background, I recommend the discussion by Krantz3 of null hypothesis testing in psychology research.\n\n# **WHAT IS A** P **VALUE IN PRACTICE?**\n\nThe P value is a measure of discrepancy of the fit of a model or \"null hypothesis\" H to data y. Mathematically, it is defined as Pr(T(yrep)>T(y)|H), where yrep represents a hypothetical replication under the null hypothesis and T is a test statistic (ie, a summary of the data, perhaps tailored to be sensitive to departures of interest from the model). In a model with free parameters (a \"composite null hypothesis\"), the P value can depend on these parameters, and there are various ways to get around this, by plugging in point estimates, averaging over a posterior distribution, or adjusting for the estimation process. I do not go into these complexities further, bringing them up here only to make the point that the construction of P values is not always a simple or direct process. (Even something as simple as the classical chi-square test has complexities to be discovered; see the article by Perkins et al4 ).",
    "is_useful": true,
    "question": "What role does prior information play in the context of statistical inference and P values?"
  },
  {
    "text": "**\n\nThe P value is a measure of discrepancy of the fit of a model or \"null hypothesis\" H to data y. Mathematically, it is defined as Pr(T(yrep)>T(y)|H), where yrep represents a hypothetical replication under the null hypothesis and T is a test statistic (ie, a summary of the data, perhaps tailored to be sensitive to departures of interest from the model). In a model with free parameters (a \"composite null hypothesis\"), the P value can depend on these parameters, and there are various ways to get around this, by plugging in point estimates, averaging over a posterior distribution, or adjusting for the estimation process. I do not go into these complexities further, bringing them up here only to make the point that the construction of P values is not always a simple or direct process. (Even something as simple as the classical chi-square test has complexities to be discovered; see the article by Perkins et al4 ).\n\nAWnYQp/IlQrHD3i3D0OdRyi7TvSFl4Cf3VC4/OAVpDDa8K2+Ya6H515kE= on 12/05/2024\n\nIn theory, the P value is a continuous measure of evidence, but in practice it is typically trichotomized approximately into *strong evidence*, *weak evidence*, and *no evidence* (these can also be labeled highly significant, marginally significant, and not statistically significant at conventional levels), with cutoffs roughly at P = 0.01 and 0.10.\n\nOne big practical problem with P values is that they cannot easily be compared.",
    "is_useful": true,
    "question": "What are the common classifications of P values in statistical hypothesis testing?"
  },
  {
    "text": "(Even something as simple as the classical chi-square test has complexities to be discovered; see the article by Perkins et al4 ).\n\nAWnYQp/IlQrHD3i3D0OdRyi7TvSFl4Cf3VC4/OAVpDDa8K2+Ya6H515kE= on 12/05/2024\n\nIn theory, the P value is a continuous measure of evidence, but in practice it is typically trichotomized approximately into *strong evidence*, *weak evidence*, and *no evidence* (these can also be labeled highly significant, marginally significant, and not statistically significant at conventional levels), with cutoffs roughly at P = 0.01 and 0.10.\n\nOne big practical problem with P values is that they cannot easily be compared. The difference between a highly significant P value and a clearly nonsignificant P value is itself not necessarily statistically significant. (Here, I am using \"significant\" to refer to the 5% level that is standard in statistical practice in much of biostatistics, epidemiology, social science, and many other areas of application.) Consider a simple example of two independent experiments with estimates (standard error) of 25 (10) and 10 (10). The first experiment is highly statistically significant (two and a half standard errors away from zero, corresponding to a normal-theory P value of about 0.01) while the second is not significant at all. Most disturbingly here, the difference is 15 (14), which is not close to significant.",
    "is_useful": true,
    "question": "What is a common issue with the interpretation of P values in statistical analysis? "
  },
  {
    "text": "One big practical problem with P values is that they cannot easily be compared. The difference between a highly significant P value and a clearly nonsignificant P value is itself not necessarily statistically significant. (Here, I am using \"significant\" to refer to the 5% level that is standard in statistical practice in much of biostatistics, epidemiology, social science, and many other areas of application.) Consider a simple example of two independent experiments with estimates (standard error) of 25 (10) and 10 (10). The first experiment is highly statistically significant (two and a half standard errors away from zero, corresponding to a normal-theory P value of about 0.01) while the second is not significant at all. Most disturbingly here, the difference is 15 (14), which is not close to significant. The naive (and common) approach of summarizing an experiment by a P value and then contrasting results based on significance levels, fails here, in implicitly giving the imprimatur of statistical significance on a comparison that could easily be explained by chance alone. As discussed by Gelman and Stern,5 this is not simply the well-known problem of arbitrary thresholds, the idea that a sharp cutoff at a 5% level, for example, misleadingly separates the P = 0.051 cases from P = 0.049. This is a more serious problem: even an apparently huge difference between clearly significant and clearly nonsignificant is not itself statistically significant.\n\nIn short, the P value is itself a statistic and can be a noisy measure of evidence.",
    "is_useful": true,
    "question": "What are some limitations of P values in statistical analysis?"
  },
  {
    "text": "Most disturbingly here, the difference is 15 (14), which is not close to significant. The naive (and common) approach of summarizing an experiment by a P value and then contrasting results based on significance levels, fails here, in implicitly giving the imprimatur of statistical significance on a comparison that could easily be explained by chance alone. As discussed by Gelman and Stern,5 this is not simply the well-known problem of arbitrary thresholds, the idea that a sharp cutoff at a 5% level, for example, misleadingly separates the P = 0.051 cases from P = 0.049. This is a more serious problem: even an apparently huge difference between clearly significant and clearly nonsignificant is not itself statistically significant.\n\nIn short, the P value is itself a statistic and can be a noisy measure of evidence. This is a problem not just with P values but with any mathematically equivalent procedure, such as summarizing results by whether the 95% confidence interval includes zero.\n\n# **GOOD, MEDIOCRE, AND BAD** P **VALUES**\n\nFor all their problems, P values sometimes \"work\" to convey an important aspect of the relation of data to model. Other times, a P value sends a reasonable message but does not add anything beyond a simple confidence interval. In yet other situations, a P value can actively mislead. Before going on, I will give examples of each of these three scenarios.",
    "is_useful": true,
    "question": "What are the potential issues associated with using P values in statistical analyses?"
  },
  {
    "text": "This is a more serious problem: even an apparently huge difference between clearly significant and clearly nonsignificant is not itself statistically significant.\n\nIn short, the P value is itself a statistic and can be a noisy measure of evidence. This is a problem not just with P values but with any mathematically equivalent procedure, such as summarizing results by whether the 95% confidence interval includes zero.\n\n# **GOOD, MEDIOCRE, AND BAD** P **VALUES**\n\nFor all their problems, P values sometimes \"work\" to convey an important aspect of the relation of data to model. Other times, a P value sends a reasonable message but does not add anything beyond a simple confidence interval. In yet other situations, a P value can actively mislead. Before going on, I will give examples of each of these three scenarios.\n\n# A P **Value that Worked**\n\nSeveral years ago, I was contacted by a person who suspected fraud in a local election.6 Partial counts had been released throughout the voting process and he thought the proportions for the various candidates looked suspiciously stable, as if they had been rigged to aim for a particular result. Excited to possibly be at the center of an explosive news story, I took a look at the data right away. After some preliminary graphs\u2014which indeed showed stability of the vote proportions as they evolved during election day\u2014I set up a hypothesis test comparing the variation in the data to what would be expected from independent binomial sampling.",
    "is_useful": true,
    "question": "What are the limitations of using P values in statistical analysis?"
  },
  {
    "text": "Other times, a P value sends a reasonable message but does not add anything beyond a simple confidence interval. In yet other situations, a P value can actively mislead. Before going on, I will give examples of each of these three scenarios.\n\n# A P **Value that Worked**\n\nSeveral years ago, I was contacted by a person who suspected fraud in a local election.6 Partial counts had been released throughout the voting process and he thought the proportions for the various candidates looked suspiciously stable, as if they had been rigged to aim for a particular result. Excited to possibly be at the center of an explosive news story, I took a look at the data right away. After some preliminary graphs\u2014which indeed showed stability of the vote proportions as they evolved during election day\u2014I set up a hypothesis test comparing the variation in the data to what would be expected from independent binomial sampling. When applied to the entire data set (27 candidates running for six offices), the result was not statistically significant: there was no less (and, in fact, no more) variance than would be expected by chance alone. In addition, an analysis of the 27 separate chisquare statistics revealed no particular patterns. I was left to conclude that the election results were consistent with random voting (even though, in reality, voting was certainly not random\u2014for example, married couples are likely to vote at the same time, and the sorts of people who vote in the middle of the day will differ from those who cast their ballots in the early morning or evening). I regretfully told my correspondent that he had no case.",
    "is_useful": true,
    "question": "How can the interpretation of P values impact the investigation of potential electoral fraud?"
  },
  {
    "text": "Rather, nonsignificance revealed the data to be compatible with the null hypothesis; thus, my correspondent could not argue that the data indicated fraud.\n\n# A P **Value that Was Reasonable but Unnecessary**\n\nIt is common for a research project to culminate in the estimation of one or two parameters, with publication turning\n\nDownloaded from http://journals.lww.com/epidem by BhDMf5ePHKav1zEoum1tQfN4a+kJLhEZgbsIHo4XMi0hCywCX1AWnYQp/IlQrHD3i3D0OdRyi7TvSFl4Cf3VC4/OAVpDDa8K2+Ya6H515kE= on 12/05/2024\n\non a P value being less than a conventional level of significance. For example, in our study of the effects of redistricting in state legislatures (Gelman and King),7 the key parameters were interactions in regression models for partisan bias and electoral responsiveness. Although we did not actually report P values, we could have: what made our article complete was that our findings of interest were more than two standard errors from zero, thus reaching the P < 0.05 level. Had our significance level been much greater (eg, estimates that were four or more standard errors from zero), we would doubtless have broken up our analysis (eg, studying Democrats and Republicans separately) to broaden the set of claims that we could confidently assert.",
    "is_useful": true,
    "question": "What role does the estimation of parameters and P values play in the publication of research findings related to open science?"
  },
  {
    "text": "For example, in our study of the effects of redistricting in state legislatures (Gelman and King),7 the key parameters were interactions in regression models for partisan bias and electoral responsiveness. Although we did not actually report P values, we could have: what made our article complete was that our findings of interest were more than two standard errors from zero, thus reaching the P < 0.05 level. Had our significance level been much greater (eg, estimates that were four or more standard errors from zero), we would doubtless have broken up our analysis (eg, studying Democrats and Republicans separately) to broaden the set of claims that we could confidently assert. Conversely, had our regressions not reached statistical significance at the conventional level, we would have performed some sort of pooling or constraining of our model to arrive at some weaker assertion that reached the 5% level. (Just to be clear: we are not saying that we would have performed data dredging, fishing for significance; rather, we accept that sample size dictates how much we can learn with confidence; when data are weaker, it can be possible to find reliable patterns by averaging.)\n\nIn any case, my point is that in this example it would have been just fine to summarize our results in this example via P values even though we did not happen to use that formulation.\n\n# **A Misleading** P **Value**\n\nFinally, in many scenarios P values can distract or even mislead, either a nonsignificant result wrongly interpreted as a confidence statement in support of the null hypothesis or a significant P value that is taken as proof of an effect.",
    "is_useful": true,
    "question": "How can P values lead to misunderstandings in interpreting statistical results in research?"
  },
  {
    "text": "In any case, my point is that in this example it would have been just fine to summarize our results in this example via P values even though we did not happen to use that formulation.\n\n# **A Misleading** P **Value**\n\nFinally, in many scenarios P values can distract or even mislead, either a nonsignificant result wrongly interpreted as a confidence statement in support of the null hypothesis or a significant P value that is taken as proof of an effect. A notorious example of the latter is the recent article by Bem,8 which reported statistically significant results from several experiments on extrasensory perception (ESP). At brief glance, it seems impressive to see multiple independent findings that are statistically significant (and combining the P values using classical rules would yield an even stronger result), but with enough effort it is possible to find statistical significance anywhere (see the report by Simmons et al9 ).\n\nThe focus on P values seems to have both weakened that study (by encouraging the researcher to present only some of his data so as to draw attention away from nonsignificant results) and to have led reviewers to inappropriately view a low P value (indicating a misfit of the null hypothesis to data) as strong evidence in favor of a specific alternative hypothesis (ESP) rather than other, perhaps more scientifically plausible, alternatives such as measurement error and selection bias.",
    "is_useful": true,
    "question": "What are some potential consequences of an overemphasis on P values in scientific research?"
  },
  {
    "text": "# **PRIORS, POSTERIORS, AND** P **VALUES**\n\nNow that I have established my credentials as a pragmatist who finds P values useful in some settings but not others, I want to discuss Greenland and Poole's proposal to either interpret one-sided P values as probability statements under uniform priors (an idea they trace back to Gossett)10 or else to\n\n*\u00a9 2012 Lippincott Williams & Wilkins* www.epidem.com | 71\n\nuse one-sided P values as bounds on posterior probabilities (a result they trace back to Casella and Berger).11\n\nThe general problem I have with noninformatively derived Bayesian probabilities is that they tend to be too strong. At first, this may sound paradoxical, that a noninformative or weakly informative prior yields posteriors that are too forceful\u2014and let me deepen the paradox by stating that a stronger, more informative prior will tend to yield weaker, more plausible posterior statements.\n\nHow can it be that adding prior information weakens the posterior? It has to do with the sort of probability statements we are often interested in making. Here is an example from Gelman and Weakliem.12 A sociologist examining a publicly available survey discovered a pattern relating attractiveness of parents to the sexes of their children. He found that 56% of the children of the most attractive parents were girls, when compared with 48% of the children of the other parents, and the difference was statistically significant at P < 0.02.",
    "is_useful": true,
    "question": "What is a potential issue with noninformatively derived Bayesian probabilities in the context of statistical analysis?"
  },
  {
    "text": "In that case, an experimental result that is 1 standard error from zero\u2014that is, exactly what one might expect from chance alone\u2014would imply an 83% posterior probability that the true effect in the population has the same direction as the observed pattern in the data at hand. It does not make sense to me to claim 83% certainty\u20145-to-1 odds\u2014based on data that not only could occur by chance alone but in fact represent an expected level of discrepancy. This system-level analysis accords with my criticism of the flat prior: as Greenland and Poole1 note in their article, the effects being studied in epidemiology are typically range from \u22121 to 1 on the logit scale; hence, analyses assuming broader priors will systematically overstate the probabilities of very large effects and will overstate the probability that an estimate from a small sample will agree in sign with the corresponding population quantity.\n\nRather than relying on noninformative priors, I prefer the suggestion of Greenland and Poole1 to bound posterior probabilities using real prior information. I would prefer to perform my Bayesian inferences directly without using P values as in intermediate step, but given the ubiquity of P values in much applied work, I can see that it can be helpful for researchers to understand their connection to posterior probabilities under informative priors.\n\n# **SUMMARY**\n\nLike many Bayesians, I have often represented classical confidence intervals as posterior probability intervals and interpreted one-sided P values as the posterior probability of a positive effect. These are valid conditional on the assumed noninformative prior but typically do not make sense as unconditional probability statements.",
    "is_useful": true,
    "question": "What is the importance of using informative priors in Bayesian analysis, particularly in epidemiological studies?"
  },
  {
    "text": "Rather than relying on noninformative priors, I prefer the suggestion of Greenland and Poole1 to bound posterior probabilities using real prior information. I would prefer to perform my Bayesian inferences directly without using P values as in intermediate step, but given the ubiquity of P values in much applied work, I can see that it can be helpful for researchers to understand their connection to posterior probabilities under informative priors.\n\n# **SUMMARY**\n\nLike many Bayesians, I have often represented classical confidence intervals as posterior probability intervals and interpreted one-sided P values as the posterior probability of a positive effect. These are valid conditional on the assumed noninformative prior but typically do not make sense as unconditional probability statements. As Sander Greenland has discussed in much of his work over the years, epidemiologists and applied scientists in general have knowledge of the sizes of plausible effects and biases. I believe that a direct interpretation of P values as posterior probabilities can be a useful start\u2014if we recognize that such summaries systematically overestimate the strength of claims from any particular dataset. In this way, I am in agreement with Greenland and Poole's interpretation of the one-sided P value as a lower bound of a posterior probability, although I am less convinced of the practical utility of this bound, given that the closeness of the bound depends on a combination of sample size and prior distribution.\n\nThe default conclusion from a noninformative prior analysis will almost invariably put too much probability on extreme values.",
    "is_useful": true,
    "question": "What are the implications of using noninformative priors in Bayesian inference for representing confidence intervals and interpreting P values?"
  },
  {
    "text": "These are valid conditional on the assumed noninformative prior but typically do not make sense as unconditional probability statements. As Sander Greenland has discussed in much of his work over the years, epidemiologists and applied scientists in general have knowledge of the sizes of plausible effects and biases. I believe that a direct interpretation of P values as posterior probabilities can be a useful start\u2014if we recognize that such summaries systematically overestimate the strength of claims from any particular dataset. In this way, I am in agreement with Greenland and Poole's interpretation of the one-sided P value as a lower bound of a posterior probability, although I am less convinced of the practical utility of this bound, given that the closeness of the bound depends on a combination of sample size and prior distribution.\n\nThe default conclusion from a noninformative prior analysis will almost invariably put too much probability on extreme values. A vague prior distribution assigns much of its probability on values that are never going to be plausible, and this disturbs the posterior probabilities more than we tend to expect\u2014something that we probably do not think about enough in our routine applications of standard statistical methods. Greenland and Poole1 perform a valuable service by opening up these calculations and placing them in an applied context.\n\n#### **REFERENCES**\n\n- 1. Greenland S, Poole C. Living with P-values: resurrecting a Bayesian perspective on frequentist statistics. *Epidemiology*. 2013;24:62\u201368.\n- 2. Greenland S. Null misinterpretation in statistical testing and its impact on health risk assessment. *Prev Med*. 2011;53:225\u2013228.",
    "is_useful": true,
    "question": "What issues arise from the use of noninformative priors in statistical analysis, particularly in relation to P values and posterior probabilities?"
  },
  {
    "text": "https://www.comparativepoliticsnewsletter.org/wp-content/uploads/2021/04/2016_spring.pdf\n\nJacobs, Alan M., Tim B\u00fcthe, Ana Arjona, Leonardo R. Arriola, Eva Bellin, Andrew Bennett, Lisa Bj\u00f6rkman, et al. 2021. \"The Qualitative Transparency Deliberations: Insights and Implications.\" *Perspectives on Politics*, 1\u201338. https://doi.org/10.1017/ S1537592720001164.\n\nSaunders, Elizabeth. 2014. \"Transparency without Tears: A Pragmatic Approach to Transparent Security Studies Research.\" *Security Studies* 23, no. 4 (December): 689\u201398. https://doi.org/10.1080/09636412.2014.970405.\n\nQualitative and Multi-Method Research Spring 2021, Vol. 19, No. 1 https://doi.org/10.5281/zenodo.5495552\n\n# **Using Pre-Analysis Plans in Qualitative Research**\n\nVer\u00f3nica P\u00e9rez Bentancur Rafael Pi\u00f1eiro Rodr\u00edguez *Universidad de la Rep\u00fablica, Uruguay Universidad Cat\u00f3lica del Uruguay*\n\nFernando Rosenblatt *Universidad Diego Portales, Chile*\n\n## **Introduction**\n\nI n the last decade, there has been a significant push for greater transparency in the social sciences. For example, epistemological and methodological debates have addressed the scope, meaning, and appropriateness of research transparency, and scholars have developed tools and practices to facilitate the process.",
    "is_useful": true,
    "question": "What has been a significant trend in the social sciences over the last decade regarding the dissemination of research? "
  },
  {
    "text": "*Security Studies* 23, no. 4 (December): 689\u201398. https://doi.org/10.1080/09636412.2014.970405.\n\nQualitative and Multi-Method Research Spring 2021, Vol. 19, No. 1 https://doi.org/10.5281/zenodo.5495552\n\n# **Using Pre-Analysis Plans in Qualitative Research**\n\nVer\u00f3nica P\u00e9rez Bentancur Rafael Pi\u00f1eiro Rodr\u00edguez *Universidad de la Rep\u00fablica, Uruguay Universidad Cat\u00f3lica del Uruguay*\n\nFernando Rosenblatt *Universidad Diego Portales, Chile*\n\n## **Introduction**\n\nI n the last decade, there has been a significant push for greater transparency in the social sciences. For example, epistemological and methodological debates have addressed the scope, meaning, and appropriateness of research transparency, and scholars have developed tools and practices to facilitate the process. One such approach is preregistration, the practice of recording a priori a study's design and its plan of analysis in open and public repositories (Haven et al. 2020). While it is a standard practice in experimental social science, it has been a matter of contested debate in observational work, both quantitative and qualitative. Arguments in favor of using this practice in qualitative inquiry, as well as opposing views, have recently been published (B\u00fcthe et al. 2015; Elman and Kapiszewski 2014; Elman and Lupia 2016; Kern and Gleditsch 2017; Haven et al.",
    "is_useful": true,
    "question": "What is preregistration in the context of qualitative research and why is it significant for transparency in the social sciences?"
  },
  {
    "text": "For example, epistemological and methodological debates have addressed the scope, meaning, and appropriateness of research transparency, and scholars have developed tools and practices to facilitate the process. One such approach is preregistration, the practice of recording a priori a study's design and its plan of analysis in open and public repositories (Haven et al. 2020). While it is a standard practice in experimental social science, it has been a matter of contested debate in observational work, both quantitative and qualitative. Arguments in favor of using this practice in qualitative inquiry, as well as opposing views, have recently been published (B\u00fcthe et al. 2015; Elman and Kapiszewski 2014; Elman and Lupia 2016; Kern and Gleditsch 2017; Haven et al. 2020; Jacobs et al. 2021; Kapiszewski and Karcher 2020; Moravcsik 2014; Pi\u00f1eiro and Rosenblatt 2016).\n\nPreregistration serves both the overarching goal of improving research transparency and, in our experience, also improves the research process itself. Regarding the former, preregistration increases the credibility of research because it facilitates the scientific community's access to a researcher's theoretical and methodological decisions (Nosek et al. 2015).",
    "is_useful": true,
    "question": "What is preregistration and how does it contribute to research transparency in scientific studies?"
  },
  {
    "text": "Arguments in favor of using this practice in qualitative inquiry, as well as opposing views, have recently been published (B\u00fcthe et al. 2015; Elman and Kapiszewski 2014; Elman and Lupia 2016; Kern and Gleditsch 2017; Haven et al. 2020; Jacobs et al. 2021; Kapiszewski and Karcher 2020; Moravcsik 2014; Pi\u00f1eiro and Rosenblatt 2016).\n\nPreregistration serves both the overarching goal of improving research transparency and, in our experience, also improves the research process itself. Regarding the former, preregistration increases the credibility of research because it facilitates the scientific community's access to a researcher's theoretical and methodological decisions (Nosek et al. 2015). Regarding the latter, preregistration benefits the research process in several ways: it helps one develop parsimonious theories; it encourages one to articulate a clear relationship between theory, hypotheses, and evidence; it improves the dialogue between data and theory; and it fosters efficiency in fieldwork (P\u00e9rez Bentancur, Pi\u00f1eiro Rodr\u00edguez, and Rosenblatt 2018b; Pi\u00f1eiro and Rosenblatt 2016).\n\nA Pre-Analysis Plan (PAP) is one tool that scholars including those who conduct qualitative inquiry\u2014can use to preregister their research.",
    "is_useful": true,
    "question": "What are the benefits of preregistration in qualitative research?"
  },
  {
    "text": "Preregistration serves both the overarching goal of improving research transparency and, in our experience, also improves the research process itself. Regarding the former, preregistration increases the credibility of research because it facilitates the scientific community's access to a researcher's theoretical and methodological decisions (Nosek et al. 2015). Regarding the latter, preregistration benefits the research process in several ways: it helps one develop parsimonious theories; it encourages one to articulate a clear relationship between theory, hypotheses, and evidence; it improves the dialogue between data and theory; and it fosters efficiency in fieldwork (P\u00e9rez Bentancur, Pi\u00f1eiro Rodr\u00edguez, and Rosenblatt 2018b; Pi\u00f1eiro and Rosenblatt 2016).\n\nA Pre-Analysis Plan (PAP) is one tool that scholars including those who conduct qualitative inquiry\u2014can use to preregister their research. As defined in Evidence in Governance and Politics (EGAP)'s methods guide on the tool, a PAP is a document that \"\u2026formalizes and declares the design and analysis plan for your study. It is written before the analysis is conducted and is generally registered on a third-party website\" (Chen and Grady, n.d.). There is no general agreement about what a PAP for qualitative studies (PAP-Q) should contain. There are several general PAP guidelines, models, and templates for preregistering qualitative research (Kern and Gleditsch 2017; Haven et al. 2020; Pi\u00f1eiro and Rosenblatt 2016).",
    "is_useful": true,
    "question": "What are the benefits of preregistration in research?"
  },
  {
    "text": "A Pre-Analysis Plan (PAP) is one tool that scholars including those who conduct qualitative inquiry\u2014can use to preregister their research. As defined in Evidence in Governance and Politics (EGAP)'s methods guide on the tool, a PAP is a document that \"\u2026formalizes and declares the design and analysis plan for your study. It is written before the analysis is conducted and is generally registered on a third-party website\" (Chen and Grady, n.d.). There is no general agreement about what a PAP for qualitative studies (PAP-Q) should contain. There are several general PAP guidelines, models, and templates for preregistering qualitative research (Kern and Gleditsch 2017; Haven et al. 2020; Pi\u00f1eiro and Rosenblatt 2016). Haven et al. (2020) conducted a study identifying the main sections that scholars who conduct qualitative research in various disciplines should include in a preregistration template. Their findings suggest that a PAP for qualitative studies (PAP-Q) should include four basic categories of information: study information, the\n\nQualitative and Multi-Method Research | 9\n\ndesign plan, data collection method, and analysis plan.1 A PAP-Q further develops a conventional research project. It provides additional specifications of the theory and more details on the methodological design, type of data and its sources, and the probative value of the evidence for assessing each hypothesis.",
    "is_useful": true,
    "question": "What is a Pre-Analysis Plan (PAP) and what categories of information should it include for qualitative research?"
  },
  {
    "text": "There are several general PAP guidelines, models, and templates for preregistering qualitative research (Kern and Gleditsch 2017; Haven et al. 2020; Pi\u00f1eiro and Rosenblatt 2016). Haven et al. (2020) conducted a study identifying the main sections that scholars who conduct qualitative research in various disciplines should include in a preregistration template. Their findings suggest that a PAP for qualitative studies (PAP-Q) should include four basic categories of information: study information, the\n\nQualitative and Multi-Method Research | 9\n\ndesign plan, data collection method, and analysis plan.1 A PAP-Q further develops a conventional research project. It provides additional specifications of the theory and more details on the methodological design, type of data and its sources, and the probative value of the evidence for assessing each hypothesis.\n\nIn this short paper, we explore the practical use of preregistration in qualitative research through detailing the experience of preregistering our study of the origins and reproduction of activism in Uruguay's Frente Amplio (FA, or Broad Front) (P\u00e9rez Bentancur, Pi\u00f1eiro Rodr\u00edguez, and Rosenblatt 2020; for the PAP-Q see Pi\u00f1eiro, P\u00e9rez, and Rosenblatt 2016a). We emphasize how the process of drafting a PAP-Q improved the theoretical and analytical quality of our work.",
    "is_useful": true,
    "question": "What are the key components recommended for preregistration templates in qualitative research?"
  },
  {
    "text": "It provides additional specifications of the theory and more details on the methodological design, type of data and its sources, and the probative value of the evidence for assessing each hypothesis.\n\nIn this short paper, we explore the practical use of preregistration in qualitative research through detailing the experience of preregistering our study of the origins and reproduction of activism in Uruguay's Frente Amplio (FA, or Broad Front) (P\u00e9rez Bentancur, Pi\u00f1eiro Rodr\u00edguez, and Rosenblatt 2020; for the PAP-Q see Pi\u00f1eiro, P\u00e9rez, and Rosenblatt 2016a). We emphasize how the process of drafting a PAP-Q improved the theoretical and analytical quality of our work. We also describe how creating a PAP-Q significantly improved the efficiency of our field research by forcing us to think through the type of evidence necessary to test a given hypothesis or claim. Our contribution to this symposium builds on the results of the Qualitative Transparency Deliberations (QTD), summarized in Jacobs et al. (2021). We also take into account the recommended practices for studies that use process tracing, as outlined by Bennett, Fairfield, and Soifer (2019). Thus, this essay may prove useful to researchers who would like to follow the latter's recommendations in the future.\n\n#### **PAP-Q, Research Transparency and Fieldwork Efficiency**\n\nIn what follows, we illustrate how we carried out preregistration and used a PAP-Q in our in-depth case study of the reproduction of activism in the FA in Uruguay.",
    "is_useful": true,
    "question": "What benefits does preregistration offer to qualitative research in terms of research quality and efficiency?"
  },
  {
    "text": "We emphasize how the process of drafting a PAP-Q improved the theoretical and analytical quality of our work. We also describe how creating a PAP-Q significantly improved the efficiency of our field research by forcing us to think through the type of evidence necessary to test a given hypothesis or claim. Our contribution to this symposium builds on the results of the Qualitative Transparency Deliberations (QTD), summarized in Jacobs et al. (2021). We also take into account the recommended practices for studies that use process tracing, as outlined by Bennett, Fairfield, and Soifer (2019). Thus, this essay may prove useful to researchers who would like to follow the latter's recommendations in the future.\n\n#### **PAP-Q, Research Transparency and Fieldwork Efficiency**\n\nIn what follows, we illustrate how we carried out preregistration and used a PAP-Q in our in-depth case study of the reproduction of activism in the FA in Uruguay. In this study, we describe and explain the development and reproduction of the FA as a party with a grassroots structure through which activists regularly engage with the party. The FA is a deviant case that helps explicate the reproduction of activism. We argue that the internal structure of the FA\u2014and the rules that ensure a role for grassroots activists in the highest decisionmaking bodies of the party\u2014are the product of the extraordinary political conditions that existed at the time of the party's birth in 1971. The intense autonomous activism that occurred during this stage thus acts as an historical cause.",
    "is_useful": true,
    "question": "How does the use of a PAP-Q enhance research efficiency and quality in the context of open science?"
  },
  {
    "text": "Thus, this essay may prove useful to researchers who would like to follow the latter's recommendations in the future.\n\n#### **PAP-Q, Research Transparency and Fieldwork Efficiency**\n\nIn what follows, we illustrate how we carried out preregistration and used a PAP-Q in our in-depth case study of the reproduction of activism in the FA in Uruguay. In this study, we describe and explain the development and reproduction of the FA as a party with a grassroots structure through which activists regularly engage with the party. The FA is a deviant case that helps explicate the reproduction of activism. We argue that the internal structure of the FA\u2014and the rules that ensure a role for grassroots activists in the highest decisionmaking bodies of the party\u2014are the product of the extraordinary political conditions that existed at the time of the party's birth in 1971. The intense autonomous activism that occurred during this stage thus acts as an historical cause. Our study shows that the decisionmaking authority of the grassroots activists, granted incrementally since the FA's foundational stage, enables activists to block changes that reduce their power, engendering a lock-in effect and positive feedback. We show how these rules grant FA activists a significant voice, which imbues activists' participation with a strong sense of efficacy. This perceived efficacy operates as a selective incentive for activists to engage with the party.\n\nWe registered a PAP-Q before conducting fieldwork and introduced amendments as our fieldwork proceeded to register updates in our theory and empirical strategy.",
    "is_useful": true,
    "question": "How can preregistration and the use of a PAP-Q improve research transparency and fieldwork efficiency in studies of grassroots activism?"
  },
  {
    "text": "We argue that the internal structure of the FA\u2014and the rules that ensure a role for grassroots activists in the highest decisionmaking bodies of the party\u2014are the product of the extraordinary political conditions that existed at the time of the party's birth in 1971. The intense autonomous activism that occurred during this stage thus acts as an historical cause. Our study shows that the decisionmaking authority of the grassroots activists, granted incrementally since the FA's foundational stage, enables activists to block changes that reduce their power, engendering a lock-in effect and positive feedback. We show how these rules grant FA activists a significant voice, which imbues activists' participation with a strong sense of efficacy. This perceived efficacy operates as a selective incentive for activists to engage with the party.\n\nWe registered a PAP-Q before conducting fieldwork and introduced amendments as our fieldwork proceeded to register updates in our theory and empirical strategy. Thus, preregistration established a \"formal beginning of the iteration between theory, evidence, and the interpretation of the evidence\" (Pi\u00f1eiro and Rosenblatt 2016, 788). This PAP-Q covers three of the main dimensions of a study that Jacobs et al. (2021, 176) suggest that scholars should be transparent about: \"research goals,\" \"processes of generating evidence,\" and the \"analytic processes.\"\n\nOur PAP-Q included a theoretical section in which we specified the main concepts and our causal argument.",
    "is_useful": true,
    "question": "What role does grassroots activism play in the decision-making structure of political organizations?"
  },
  {
    "text": "We show how these rules grant FA activists a significant voice, which imbues activists' participation with a strong sense of efficacy. This perceived efficacy operates as a selective incentive for activists to engage with the party.\n\nWe registered a PAP-Q before conducting fieldwork and introduced amendments as our fieldwork proceeded to register updates in our theory and empirical strategy. Thus, preregistration established a \"formal beginning of the iteration between theory, evidence, and the interpretation of the evidence\" (Pi\u00f1eiro and Rosenblatt 2016, 788). This PAP-Q covers three of the main dimensions of a study that Jacobs et al. (2021, 176) suggest that scholars should be transparent about: \"research goals,\" \"processes of generating evidence,\" and the \"analytic processes.\"\n\nOur PAP-Q included a theoretical section in which we specified the main concepts and our causal argument. It described in detail the causal mechanisms of the reproduction of activism and the theoretical leverage of the FA (i.e., the FA as a deviant case), as was later recommended by Bennett, Fairfield, and Soifer (2019). The document also specified the empirical strategy and design. It included a set of concrete working hypotheses (both descriptive and causal). Each hypothesis was accompanied by a list of the pieces of evidence required to confirm it, the sources that could provide the needed evidence (e.g., documents, interviews, survey, administrative data, and observation of party's activities), the potential biases in the types of evidence collected. The PAP-Q also stated alternative hypotheses and the related evidence that could challenge our theory.",
    "is_useful": true,
    "question": "What is the significance of preregistration in the context of open science and research transparency?"
  },
  {
    "text": "Our PAP-Q included a theoretical section in which we specified the main concepts and our causal argument. It described in detail the causal mechanisms of the reproduction of activism and the theoretical leverage of the FA (i.e., the FA as a deviant case), as was later recommended by Bennett, Fairfield, and Soifer (2019). The document also specified the empirical strategy and design. It included a set of concrete working hypotheses (both descriptive and causal). Each hypothesis was accompanied by a list of the pieces of evidence required to confirm it, the sources that could provide the needed evidence (e.g., documents, interviews, survey, administrative data, and observation of party's activities), the potential biases in the types of evidence collected. The PAP-Q also stated alternative hypotheses and the related evidence that could challenge our theory. Finally, the PAP-Q contained conventional process-tracing terminology to establish the probative value of each piece of evidence (see Pi\u00f1eiro, P\u00e9rez, and Rosenblatt 2016a, 10-8). Thus, our PAP-Q disclosed our initial research goals, the process we planned to follow to generate the evidence needed to test our hypothesis, and the analytic process that we committed to pursue.\n\n#### **The Benefits of Preregistration for Transparency and the Importance of Flexibility**\n\nPreregistering is a way of generating ex ante transparency that improves post hoc transparency. Ex ante transparency refers to clarifying one's theoretical starting point and empirical expectations. This allows readers to trace the research process and to understand the researcher's iteration between theory and fieldwork.",
    "is_useful": true,
    "question": "What are the benefits of preregistration in research regarding transparency and flexibility?"
  },
  {
    "text": "The PAP-Q also stated alternative hypotheses and the related evidence that could challenge our theory. Finally, the PAP-Q contained conventional process-tracing terminology to establish the probative value of each piece of evidence (see Pi\u00f1eiro, P\u00e9rez, and Rosenblatt 2016a, 10-8). Thus, our PAP-Q disclosed our initial research goals, the process we planned to follow to generate the evidence needed to test our hypothesis, and the analytic process that we committed to pursue.\n\n#### **The Benefits of Preregistration for Transparency and the Importance of Flexibility**\n\nPreregistering is a way of generating ex ante transparency that improves post hoc transparency. Ex ante transparency refers to clarifying one's theoretical starting point and empirical expectations. This allows readers to trace the research process and to understand the researcher's iteration between theory and fieldwork. It also entails a researcher's commitment to search for a specific set of evidence and its probatory nature. Thus, if the researcher does not find certain evidence mentioned in her preregistered design, or finds evidence that challenges her prior theoretical expectations, the researcher is compelled to explicitly address it. This reduces the moral hazard associated with the temptation to cherry-pick evidence, or engage in ad hoc analyses tied\n\n<sup>1</sup> The template is available at: https://osf.io/w4ac2. 10 | Using Pre-Analysis Plans in Qualitative Research\n\nto the evidence collected (Jacobs 2020).",
    "is_useful": true,
    "question": "What is the purpose of preregistration in research, particularly in relation to transparency and evidence evaluation?"
  },
  {
    "text": "Ex ante transparency refers to clarifying one's theoretical starting point and empirical expectations. This allows readers to trace the research process and to understand the researcher's iteration between theory and fieldwork. It also entails a researcher's commitment to search for a specific set of evidence and its probatory nature. Thus, if the researcher does not find certain evidence mentioned in her preregistered design, or finds evidence that challenges her prior theoretical expectations, the researcher is compelled to explicitly address it. This reduces the moral hazard associated with the temptation to cherry-pick evidence, or engage in ad hoc analyses tied\n\n<sup>1</sup> The template is available at: https://osf.io/w4ac2. 10 | Using Pre-Analysis Plans in Qualitative Research\n\nto the evidence collected (Jacobs 2020). Preregistering makes post-hoc disclosure of the research process more meaningful as it covers the entire research process and not merely what the researcher decides to disclose at the end of the research.\n\nThe qualitative research process implies an iterative process between theory and evidence (Elman and Lupia 2016; Mahoney and Rueschemeyer 2003; Yom 2015). Preregistration in qualitative research needs to take the iterative logic of qualitative research into account. It must allow for the possibility of updating the theory by amending a PAP-Q (Pi\u00f1eiro and Rosenblatt 2016). In our study, for instance, we amended our theoretical argument in light of evidence collected in our fieldwork and specified the new evidence required to test the amended theory.",
    "is_useful": true,
    "question": "What is the significance of ex ante transparency and preregistration in qualitative research?"
  },
  {
    "text": "10 | Using Pre-Analysis Plans in Qualitative Research\n\nto the evidence collected (Jacobs 2020). Preregistering makes post-hoc disclosure of the research process more meaningful as it covers the entire research process and not merely what the researcher decides to disclose at the end of the research.\n\nThe qualitative research process implies an iterative process between theory and evidence (Elman and Lupia 2016; Mahoney and Rueschemeyer 2003; Yom 2015). Preregistration in qualitative research needs to take the iterative logic of qualitative research into account. It must allow for the possibility of updating the theory by amending a PAP-Q (Pi\u00f1eiro and Rosenblatt 2016). In our study, for instance, we amended our theoretical argument in light of evidence collected in our fieldwork and specified the new evidence required to test the amended theory. Specifically, after 22 in-depth interviews and a focus group with party activists, we were able to determine more precisely the role grassroots activists play in the process of building the FA. In one of the amendments (Pi\u00f1eiro, P\u00e9rez, and Rosenblatt 2016b), then, we amended the theory and the hypothesis preregistered in the original PAP-Q, further specifying the components of the causal path between the causes and the dependent variable, and we explained why the changes were introduced.",
    "is_useful": true,
    "question": "What is the significance of preregistration in qualitative research?"
  },
  {
    "text": "Preregistration in qualitative research needs to take the iterative logic of qualitative research into account. It must allow for the possibility of updating the theory by amending a PAP-Q (Pi\u00f1eiro and Rosenblatt 2016). In our study, for instance, we amended our theoretical argument in light of evidence collected in our fieldwork and specified the new evidence required to test the amended theory. Specifically, after 22 in-depth interviews and a focus group with party activists, we were able to determine more precisely the role grassroots activists play in the process of building the FA. In one of the amendments (Pi\u00f1eiro, P\u00e9rez, and Rosenblatt 2016b), then, we amended the theory and the hypothesis preregistered in the original PAP-Q, further specifying the components of the causal path between the causes and the dependent variable, and we explained why the changes were introduced.\n\nGiven that our PAP-Q included the pieces of evidence, the sources, and the tools to collect the evidence that we had anticipated using, amendments also allowed us to publicly update the evidence or the method of collecting it in response to problems that emerged during fieldwork. In our study, we first planned to conduct a self-administered survey of grassroots activists. We encountered several problems during its implementation and decided to change to an online survey. The online survey allowed us to obtain information not only from activists, but also from party adherents whom we initially had not planned to study and, thus, to collect additional evidence.",
    "is_useful": true,
    "question": "What considerations should be made for preregistration in qualitative research to accommodate its iterative nature?"
  },
  {
    "text": "In one of the amendments (Pi\u00f1eiro, P\u00e9rez, and Rosenblatt 2016b), then, we amended the theory and the hypothesis preregistered in the original PAP-Q, further specifying the components of the causal path between the causes and the dependent variable, and we explained why the changes were introduced.\n\nGiven that our PAP-Q included the pieces of evidence, the sources, and the tools to collect the evidence that we had anticipated using, amendments also allowed us to publicly update the evidence or the method of collecting it in response to problems that emerged during fieldwork. In our study, we first planned to conduct a self-administered survey of grassroots activists. We encountered several problems during its implementation and decided to change to an online survey. The online survey allowed us to obtain information not only from activists, but also from party adherents whom we initially had not planned to study and, thus, to collect additional evidence. This change in plans led us to introduce two new amendments to our PAP-Q related to the evidence collected with the survey instrument (Pi\u00f1eiro, P\u00e9rez, and Rosenblatt 2016c, 2016d).\n\nPAP-Qs and their amendments allow readers to understand changes in the research process. This brings more transparency to the research process and promotes understanding of the challenges that emerge during fieldwork, which published papers rarely discuss. In this vein, readers are able to evaluate crucial decisions that the researcher made, and the value of the evidence presented.",
    "is_useful": true,
    "question": "How do amendments to research protocols enhance transparency and understanding in the research process?"
  },
  {
    "text": "In our study, we first planned to conduct a self-administered survey of grassroots activists. We encountered several problems during its implementation and decided to change to an online survey. The online survey allowed us to obtain information not only from activists, but also from party adherents whom we initially had not planned to study and, thus, to collect additional evidence. This change in plans led us to introduce two new amendments to our PAP-Q related to the evidence collected with the survey instrument (Pi\u00f1eiro, P\u00e9rez, and Rosenblatt 2016c, 2016d).\n\nPAP-Qs and their amendments allow readers to understand changes in the research process. This brings more transparency to the research process and promotes understanding of the challenges that emerge during fieldwork, which published papers rarely discuss. In this vein, readers are able to evaluate crucial decisions that the researcher made, and the value of the evidence presented. The disclosure of fieldwork problems and how they were resolved might also be useful to scholars who want to conduct similar empirical strategies. Preregistration serves the goal of improving causal inference through the iteration of theory and fieldwork. The PAP-Q reflects, in a formal and public document, the natural iterartive process of qualitative research, which researchers usually do not explicitly present (Yom 2015).",
    "is_useful": true,
    "question": "How does the use of PAP-Qs and their amendments contribute to transparency and understanding in the research process?"
  },
  {
    "text": "PAP-Qs and their amendments allow readers to understand changes in the research process. This brings more transparency to the research process and promotes understanding of the challenges that emerge during fieldwork, which published papers rarely discuss. In this vein, readers are able to evaluate crucial decisions that the researcher made, and the value of the evidence presented. The disclosure of fieldwork problems and how they were resolved might also be useful to scholars who want to conduct similar empirical strategies. Preregistration serves the goal of improving causal inference through the iteration of theory and fieldwork. The PAP-Q reflects, in a formal and public document, the natural iterartive process of qualitative research, which researchers usually do not explicitly present (Yom 2015).\n\n## **How Preregistration Benefits Fieldwork Efficiency**\n\nPAP-Q not only improves transparency, but also enhances fieldwork efficiency in at least two interrelated dimensions: First, it facilitates the calibration of research instruments, helping authors to maximize their potential to collect relevant data, and, second, it improves the practical organization and planning of fieldwork. As Kapiszewski, MacLean, and Read (2015) note, planning one's fieldwork is essential. Many fieldwork activities constitute a one-shot opportunity to collect evidence (e.g., there may be few opportunities to travel or to interview an important political leader). A PAP-Q provides an opportunity to maximize the results of the fieldwork.\n\nIn our study, the PAP-Q prompted us to calibrate our survey and in-depth interview questionnaires so as to ask information directly tied to our outcome of interest.",
    "is_useful": true,
    "question": "What are the ways in which PAP-Q enhances the transparency and efficiency of the research process in open science?"
  },
  {
    "text": "## **How Preregistration Benefits Fieldwork Efficiency**\n\nPAP-Q not only improves transparency, but also enhances fieldwork efficiency in at least two interrelated dimensions: First, it facilitates the calibration of research instruments, helping authors to maximize their potential to collect relevant data, and, second, it improves the practical organization and planning of fieldwork. As Kapiszewski, MacLean, and Read (2015) note, planning one's fieldwork is essential. Many fieldwork activities constitute a one-shot opportunity to collect evidence (e.g., there may be few opportunities to travel or to interview an important political leader). A PAP-Q provides an opportunity to maximize the results of the fieldwork.\n\nIn our study, the PAP-Q prompted us to calibrate our survey and in-depth interview questionnaires so as to ask information directly tied to our outcome of interest. In addition, it helped us to establish the precise requirements of the archival research (e.g., types of documents, availability). Specifically, the PAP-Q encouraged us to thoroughly evaluate the logical validity, measurability, reliability, and the viability of collecting the evidence (both in terms of ethical restrictions and resource limitations).\n\nPAP-Q also guided the organization of our fieldwork. A clear specification of the different tools and the required evidence and sources helped us decide where to invest our scarce resources (both time and money), a critical issue for any scholar, and especially for those of us in the global South.",
    "is_useful": true,
    "question": "What are the benefits of using PAP-Q in the context of fieldwork efficiency?"
  },
  {
    "text": "A PAP-Q provides an opportunity to maximize the results of the fieldwork.\n\nIn our study, the PAP-Q prompted us to calibrate our survey and in-depth interview questionnaires so as to ask information directly tied to our outcome of interest. In addition, it helped us to establish the precise requirements of the archival research (e.g., types of documents, availability). Specifically, the PAP-Q encouraged us to thoroughly evaluate the logical validity, measurability, reliability, and the viability of collecting the evidence (both in terms of ethical restrictions and resource limitations).\n\nPAP-Q also guided the organization of our fieldwork. A clear specification of the different tools and the required evidence and sources helped us decide where to invest our scarce resources (both time and money), a critical issue for any scholar, and especially for those of us in the global South. Planning fieldwork entailed preparing a draft list of potential interviewees, checking beforehand the availability of archives (how much was avaiable and the characteristics of the material), and considering other strategies of data gathering. Data gathering has to be planned and prioritized as a function of the probative nature of the evidence that would be collected from the alternative sources (Bennett and Checkel 2015). This is only possible when the researcher articulates beforehand a set of woking hypotheses, the possible sources from which information may be gathered, and the probative nature of the evidence to be collected.\n\nQualitative scholars, especially those who conduct process tracing, usually develop a more or less formal pre-analysis plan of their research before conducting fieldwork.",
    "is_useful": true,
    "question": "What is the purpose of developing a pre-analysis plan in qualitative research?"
  },
  {
    "text": "A clear specification of the different tools and the required evidence and sources helped us decide where to invest our scarce resources (both time and money), a critical issue for any scholar, and especially for those of us in the global South. Planning fieldwork entailed preparing a draft list of potential interviewees, checking beforehand the availability of archives (how much was avaiable and the characteristics of the material), and considering other strategies of data gathering. Data gathering has to be planned and prioritized as a function of the probative nature of the evidence that would be collected from the alternative sources (Bennett and Checkel 2015). This is only possible when the researcher articulates beforehand a set of woking hypotheses, the possible sources from which information may be gathered, and the probative nature of the evidence to be collected.\n\nQualitative scholars, especially those who conduct process tracing, usually develop a more or less formal pre-analysis plan of their research before conducting fieldwork. They also transform it during the research process (Kapiszewski, MacLean, and Read 2015). Making this practice public through preregistration incentivizes the adoption of such practices by the academic community as a whole. For those who usually perform these tasks, preregistration improves the transparency of their work. For those who do not, preregistration forces them to adopt these good practices associated with the research process.\n\n## **Conclusions and Open Questions**\n\nIn this brief note, we have presented how preregistering qualitative research improves transparency and the quality of research in general.",
    "is_useful": true,
    "question": "How does preregistration of qualitative research contribute to transparency and the quality of research?"
  },
  {
    "text": "This is only possible when the researcher articulates beforehand a set of woking hypotheses, the possible sources from which information may be gathered, and the probative nature of the evidence to be collected.\n\nQualitative scholars, especially those who conduct process tracing, usually develop a more or less formal pre-analysis plan of their research before conducting fieldwork. They also transform it during the research process (Kapiszewski, MacLean, and Read 2015). Making this practice public through preregistration incentivizes the adoption of such practices by the academic community as a whole. For those who usually perform these tasks, preregistration improves the transparency of their work. For those who do not, preregistration forces them to adopt these good practices associated with the research process.\n\n## **Conclusions and Open Questions**\n\nIn this brief note, we have presented how preregistering qualitative research improves transparency and the quality of research in general. We have used this practice in various research projects.2 We described the contents of a PAP-Q, and how each part promotes different recommended research transparency practices. We believe that preregistering research designs not only improves the transparency of different aspects of the research process, but also promotes greater investment in research design and improves efficiency in the field.\n\nA potential limit to preregistering qualitative research is that preregistering implies some degree of knowledge of the cases to be analyzed.",
    "is_useful": true,
    "question": "What are the benefits of preregistering qualitative research in terms of research transparency and overall quality?"
  },
  {
    "text": "Making this practice public through preregistration incentivizes the adoption of such practices by the academic community as a whole. For those who usually perform these tasks, preregistration improves the transparency of their work. For those who do not, preregistration forces them to adopt these good practices associated with the research process.\n\n## **Conclusions and Open Questions**\n\nIn this brief note, we have presented how preregistering qualitative research improves transparency and the quality of research in general. We have used this practice in various research projects.2 We described the contents of a PAP-Q, and how each part promotes different recommended research transparency practices. We believe that preregistering research designs not only improves the transparency of different aspects of the research process, but also promotes greater investment in research design and improves efficiency in the field.\n\nA potential limit to preregistering qualitative research is that preregistering implies some degree of knowledge of the cases to be analyzed. Because scholars who conduct field research often learn a great deal as their work in the field progresses, preregistering a study might entail investigators introducing several amendments to their preregistered plan, increasing the burden on them in the initial phases of the research process. Yet, once the researcher has gained some footing in the study (e.g., after pre-fieldwork) or after finishing fieldwork for one of her cases, she might register the PAP-Q for other cases in which the theory will be tested.\n\nSome authors have raised concerns about employing this practice in qualitative research, ranging from epistemological and methodological critiques to practical considerations.",
    "is_useful": true,
    "question": "How does preregistration influence transparency and research practices in the academic community?"
  },
  {
    "text": "We believe that preregistering research designs not only improves the transparency of different aspects of the research process, but also promotes greater investment in research design and improves efficiency in the field.\n\nA potential limit to preregistering qualitative research is that preregistering implies some degree of knowledge of the cases to be analyzed. Because scholars who conduct field research often learn a great deal as their work in the field progresses, preregistering a study might entail investigators introducing several amendments to their preregistered plan, increasing the burden on them in the initial phases of the research process. Yet, once the researcher has gained some footing in the study (e.g., after pre-fieldwork) or after finishing fieldwork for one of her cases, she might register the PAP-Q for other cases in which the theory will be tested.\n\nSome authors have raised concerns about employing this practice in qualitative research, ranging from epistemological and methodological critiques to practical considerations. For example, these questions were discussed in the Fall 2018 issue of *Qualitative and Multi-Method Research*, which examined DA-RT guidelines, raised again during the Qualitative Transparency\n\nDeliberations process, and have been summarized by Jacobs et al. (2021). As we have argued elsewhere (P\u00e9rez Bentancur, Pi\u00f1eiro Rodr\u00edguez, and Rosenblatt 2018a), preregistration likely would not benefit scholars who work in non-positivist traditions and whose research has different epistemological grounds. Thus, our claims in favor of PAP-Q (cf.",
    "is_useful": true,
    "question": "What are the potential benefits and limitations of preregistering qualitative research in the context of open science?"
  },
  {
    "text": "Some authors have raised concerns about employing this practice in qualitative research, ranging from epistemological and methodological critiques to practical considerations. For example, these questions were discussed in the Fall 2018 issue of *Qualitative and Multi-Method Research*, which examined DA-RT guidelines, raised again during the Qualitative Transparency\n\nDeliberations process, and have been summarized by Jacobs et al. (2021). As we have argued elsewhere (P\u00e9rez Bentancur, Pi\u00f1eiro Rodr\u00edguez, and Rosenblatt 2018a), preregistration likely would not benefit scholars who work in non-positivist traditions and whose research has different epistemological grounds. Thus, our claims in favor of PAP-Q (cf. P\u00e9rez Bentancur, Pi\u00f1eiro Rodr\u00edguez, and Rosenblatt 2018b, Pi\u00f1eiro and Rosenblatt 2016) do not imply that preregistration should become a hegemonic practice in the discipline. In more positivist work, however, we believe there are clear rebuttals to some of the most common critiques of preregistration in qualitative research.\n\nFor instance, one of the most significant methodological critiques of preregistration is that, according to some scholars, preregistration functions as a straitjacket that inhibits potential inductive findings and limits a researcher's creativity. In our experience, preregistration orders what would otherwise be an overwhelming world. It equips researchers to assess discoveries. PAP-Q helps researchers clarify their theoretical and empirical expectations.",
    "is_useful": true,
    "question": "What critiques have been raised regarding preregistration in qualitative research, and how does it potentially impact researchers' creativity and methodological approach?"
  },
  {
    "text": "Thus, our claims in favor of PAP-Q (cf. P\u00e9rez Bentancur, Pi\u00f1eiro Rodr\u00edguez, and Rosenblatt 2018b, Pi\u00f1eiro and Rosenblatt 2016) do not imply that preregistration should become a hegemonic practice in the discipline. In more positivist work, however, we believe there are clear rebuttals to some of the most common critiques of preregistration in qualitative research.\n\nFor instance, one of the most significant methodological critiques of preregistration is that, according to some scholars, preregistration functions as a straitjacket that inhibits potential inductive findings and limits a researcher's creativity. In our experience, preregistration orders what would otherwise be an overwhelming world. It equips researchers to assess discoveries. PAP-Q helps researchers clarify their theoretical and empirical expectations. This, in turn, helps elucidate the value of the emergence of unexpected evidence.\n\nAnother critique of preregistration is that excessive research transparency requirements might increase the overall costs of research (Jacobs et al. 2021), creating a barrier for those with fewer financial resources and thereby widening the research gap between scholars from the global North and those in the global South. However, by encouraging researchers to better organize the research process prior to embarking on it, preregistration can actually reduce the costs of fieldwork. In our own case, preregistration made our fieldwork more efficient, which, in turn, helped us use our scarce resources wisely.\n\n### **References**\n\n- Bennett, Andrew, and Jeffrey T Checkel, eds.",
    "is_useful": true,
    "question": "What are some critiques and advantages of preregistration in qualitative research?"
  },
  {
    "text": "In our experience, preregistration orders what would otherwise be an overwhelming world. It equips researchers to assess discoveries. PAP-Q helps researchers clarify their theoretical and empirical expectations. This, in turn, helps elucidate the value of the emergence of unexpected evidence.\n\nAnother critique of preregistration is that excessive research transparency requirements might increase the overall costs of research (Jacobs et al. 2021), creating a barrier for those with fewer financial resources and thereby widening the research gap between scholars from the global North and those in the global South. However, by encouraging researchers to better organize the research process prior to embarking on it, preregistration can actually reduce the costs of fieldwork. In our own case, preregistration made our fieldwork more efficient, which, in turn, helped us use our scarce resources wisely.\n\n### **References**\n\n- Bennett, Andrew, and Jeffrey T Checkel, eds. 2015. *Process Tracing. From Metaphor to Analytic Tool*. New York: Cambridge University Press.\n- Bennett, Andrew, Tasha Fairfield, and Hillel David Soifer. 2019. Comparative Methods and Process Tracing. Report III.1 In *Qualitative Transparency Deliberations. Working Group Final Reports*, edited by American Political Science Association Organized Section for Qualitative and Multi-Method Research. Available at SSRN: https://ssrn.com/abstract=3333405 or http:// dx.doi.org/10.2139/ssrn.3333405.",
    "is_useful": true,
    "question": "How does preregistration affect the organization and cost efficiency of the research process?"
  },
  {
    "text": "In our own case, preregistration made our fieldwork more efficient, which, in turn, helped us use our scarce resources wisely.\n\n### **References**\n\n- Bennett, Andrew, and Jeffrey T Checkel, eds. 2015. *Process Tracing. From Metaphor to Analytic Tool*. New York: Cambridge University Press.\n- Bennett, Andrew, Tasha Fairfield, and Hillel David Soifer. 2019. Comparative Methods and Process Tracing. Report III.1 In *Qualitative Transparency Deliberations. Working Group Final Reports*, edited by American Political Science Association Organized Section for Qualitative and Multi-Method Research. Available at SSRN: https://ssrn.com/abstract=3333405 or http:// dx.doi.org/10.2139/ssrn.3333405.\n- B\u00fcthe, Tim, Alan M. Jacobs, Erik Bleich, Robert Pekkanen, Marc Trachtenberg, Katherine Cramer, Victor Shih, Sarah Elizabeth Parkinson, Elisabeth Jean Wood, Timothy Pachirat, David Romney, Brandon Stewart, Dustin H. Tingley, Andrew Davison, Carsten Q. Schneider, Claudius Wagemann, and Tasha Fairfield. 2015. \"Transparency in Qualitative and Multi-Method Research: A Symposium.\" *Qualitative and Multi-Method Research: Newsletter of the American Political Science Association's QMMR Section* 13, no. 1 (Spring): 2-64.\n- Chen, Lula, and Chris Grady. n.d.",
    "is_useful": true,
    "question": "How can preregistration enhance the efficiency of fieldwork in research?"
  },
  {
    "text": "# **Rate and success of study replication in ecology and evolution**\n\n## Clint D. Kelly\n\nD\u00e9partement des Sciences biologiques, Universit\u00e9 du Qu\u00e9bec \u00e0 Montr\u00e9al, Montr\u00e9al, Quebec, Canada\n\n## **ABSTRACT**\n\nThe recent replication crisis has caused several scientific disciplines to self-reflect on the frequency with which they replicate previously published studies and to assess their success in such endeavours. The rate of replication, however, has yet to be assessed for ecology and evolution. Here, I survey the open-access ecology and evolution literature to determine how often ecologists and evolutionary biologists replicate, or at least claim to replicate, previously published studies. I found that approximately 0.023% of ecology and evolution studies are described by their authors as replications. Two of the 11 original-replication study pairs provided sufficient statistical detail for three effects so as to permit a formal analysis of replication success. Replicating authors correctly concluded that they replicated an original effect in two cases; in the third case, my analysis suggests that the finding by the replicating authors was consistent with the original finding, contrary the conclusion of ''replication failure'' by the authors.",
    "is_useful": true,
    "question": "What is the rate of study replication reported in the fields of ecology and evolution?"
  },
  {
    "text": "The rate of replication, however, has yet to be assessed for ecology and evolution. Here, I survey the open-access ecology and evolution literature to determine how often ecologists and evolutionary biologists replicate, or at least claim to replicate, previously published studies. I found that approximately 0.023% of ecology and evolution studies are described by their authors as replications. Two of the 11 original-replication study pairs provided sufficient statistical detail for three effects so as to permit a formal analysis of replication success. Replicating authors correctly concluded that they replicated an original effect in two cases; in the third case, my analysis suggests that the finding by the replicating authors was consistent with the original finding, contrary the conclusion of ''replication failure'' by the authors.\n\n**Subjects** Ecology, Evolutionary Studies, Science Policy **Keywords** Study replication, Effect size\n\n## **INTRODUCTION**\n\nReplicability is the cornerstone of science yet its importance has gained widespread support only in recent years (*Kelly, 2006*; *Nakagawa & Parker, 2015*; *Mueller-Langer et al., 2019*; *Open Science Collaboration, 2015*; *Makel, Plucker & Hegarty, 2012*).",
    "is_useful": true,
    "question": "What is the frequency of study replication reported in ecology and evolution literature?"
  },
  {
    "text": "Two of the 11 original-replication study pairs provided sufficient statistical detail for three effects so as to permit a formal analysis of replication success. Replicating authors correctly concluded that they replicated an original effect in two cases; in the third case, my analysis suggests that the finding by the replicating authors was consistent with the original finding, contrary the conclusion of ''replication failure'' by the authors.\n\n**Subjects** Ecology, Evolutionary Studies, Science Policy **Keywords** Study replication, Effect size\n\n## **INTRODUCTION**\n\nReplicability is the cornerstone of science yet its importance has gained widespread support only in recent years (*Kelly, 2006*; *Nakagawa & Parker, 2015*; *Mueller-Langer et al., 2019*; *Open Science Collaboration, 2015*; *Makel, Plucker & Hegarty, 2012*). The ''replication crisis'' of the past decade has not only stimulated considerable reflection and discussion within scientific disciplines (*Parker et al., 2016*; *Brandt et al., 2014*; *Kelly, 2006*; *Schmidt, 2009*; *Zwaan et al., 2017*) but it has also produced large-scale, systematic efforts to replicate foundational studies in psychology and biomedicine (*Iorns, 2013*; *Open Science Collaboration, 2015*).",
    "is_useful": true,
    "question": "What is the significance of replicability in scientific research?"
  },
  {
    "text": "The ''replication crisis'' of the past decade has not only stimulated considerable reflection and discussion within scientific disciplines (*Parker et al., 2016*; *Brandt et al., 2014*; *Kelly, 2006*; *Schmidt, 2009*; *Zwaan et al., 2017*) but it has also produced large-scale, systematic efforts to replicate foundational studies in psychology and biomedicine (*Iorns, 2013*; *Open Science Collaboration, 2015*). Although behavioural ecologists have a poor track record of exactly replicating studies (*Kelly, 2006*), little is known about the extent to which the replication crisis plagues the broader community of studies in ecology and evolution. Indeed, evidence suggests that the issues causing low rates of replication in other scientific disciplines are also present in ecology and evolution (*Fidler et al., 2017*; *Fraser et al., 2018*).\n\nThere is much debate and confusion surrounding what constitutes a replication (*Palmer, 2000*; *Kelly, 2006*; *Nakagawa & Parker, 2015*; *Brandt et al., 2014*; *Simons, 2014*). Ecologists and evolutionary biologists frequently repeat studies using a different species or study system (*Palmer, 2000*). *Palmer (2000)* called this phenomenon ''quasireplication''.",
    "is_useful": true,
    "question": "What challenges does the replication crisis pose to the fields of ecology and evolution?"
  },
  {
    "text": "Although behavioural ecologists have a poor track record of exactly replicating studies (*Kelly, 2006*), little is known about the extent to which the replication crisis plagues the broader community of studies in ecology and evolution. Indeed, evidence suggests that the issues causing low rates of replication in other scientific disciplines are also present in ecology and evolution (*Fidler et al., 2017*; *Fraser et al., 2018*).\n\nThere is much debate and confusion surrounding what constitutes a replication (*Palmer, 2000*; *Kelly, 2006*; *Nakagawa & Parker, 2015*; *Brandt et al., 2014*; *Simons, 2014*). Ecologists and evolutionary biologists frequently repeat studies using a different species or study system (*Palmer, 2000*). *Palmer (2000)* called this phenomenon ''quasireplication''. Quasireplication differs from true replication in that the latter is, at its most basic level, performed using the same species to test the same hypothesis.",
    "is_useful": true,
    "question": "What are the challenges related to replication in the fields of ecology and evolution? "
  },
  {
    "text": "Indeed, evidence suggests that the issues causing low rates of replication in other scientific disciplines are also present in ecology and evolution (*Fidler et al., 2017*; *Fraser et al., 2018*).\n\nThere is much debate and confusion surrounding what constitutes a replication (*Palmer, 2000*; *Kelly, 2006*; *Nakagawa & Parker, 2015*; *Brandt et al., 2014*; *Simons, 2014*). Ecologists and evolutionary biologists frequently repeat studies using a different species or study system (*Palmer, 2000*). *Palmer (2000)* called this phenomenon ''quasireplication''. Quasireplication differs from true replication in that the latter is, at its most basic level, performed using the same species to test the same hypothesis. There are three types of\n\nSubmitted 24 May 2019 Accepted 10 August 2019 Published 10 September 2019\n\nCorresponding author Clint D. Kelly, clintdkelly@icloud.com\n\nAcademic editor Todd Vision\n\nAdditional Information and Declarations can be found on page 8\n\nDOI **10.7717/peerj.7654**\n\nCopyright 2019 Kelly\n\nDistributed under Creative Commons CC-BY 4.0\n\n#### **OPEN ACCESS**\n\ntrue replication (*Lykken, 1968*; *Schmidt, 2009*; *Reid, Soley & Winner, 1981*): exact, partial, or conceptual.",
    "is_useful": true,
    "question": "What are the main types of replication in scientific research, particularly in ecology and evolution?"
  },
  {
    "text": "*Palmer (2000)* called this phenomenon ''quasireplication''. Quasireplication differs from true replication in that the latter is, at its most basic level, performed using the same species to test the same hypothesis. There are three types of\n\nSubmitted 24 May 2019 Accepted 10 August 2019 Published 10 September 2019\n\nCorresponding author Clint D. Kelly, clintdkelly@icloud.com\n\nAcademic editor Todd Vision\n\nAdditional Information and Declarations can be found on page 8\n\nDOI **10.7717/peerj.7654**\n\nCopyright 2019 Kelly\n\nDistributed under Creative Commons CC-BY 4.0\n\n#### **OPEN ACCESS**\n\ntrue replication (*Lykken, 1968*; *Schmidt, 2009*; *Reid, Soley & Winner, 1981*): exact, partial, or conceptual. Exact replications - also called direct, literal, operational, or constructive generally entail some notion that the study is a duplication of another study (*Schmidt, 2009*; *Simons, 2014*). However, this is nearly impossible to achieve for obvious reasons (e.g., must use different pool of research subjects), and so most true replications are ''close'' or partial replications (*Brandt et al., 2014*). Partial replications involve some procedural modifications while conceptual replications (also called instrumental replication) test the same hypothesis (and predictions) using markedly different experimental approaches (*Schmidt, 2009*).",
    "is_useful": true,
    "question": "What is the difference between quasireplication and true replication in research studies?"
  },
  {
    "text": "Exact replications - also called direct, literal, operational, or constructive generally entail some notion that the study is a duplication of another study (*Schmidt, 2009*; *Simons, 2014*). However, this is nearly impossible to achieve for obvious reasons (e.g., must use different pool of research subjects), and so most true replications are ''close'' or partial replications (*Brandt et al., 2014*). Partial replications involve some procedural modifications while conceptual replications (also called instrumental replication) test the same hypothesis (and predictions) using markedly different experimental approaches (*Schmidt, 2009*). It is useful to think of exact replications being at one end of the replication spectrum with quasi-replications at the other; partial and conceptual replications occupy the space between these extremes.\n\nThe replication of research studies (or at least the publication of replications) has been rather poor across disciplines in the social and natural sciences despite its need.",
    "is_useful": true,
    "question": "What are the different types of research replications and why is their publication considered important in scientific disciplines?"
  },
  {
    "text": "However, this is nearly impossible to achieve for obvious reasons (e.g., must use different pool of research subjects), and so most true replications are ''close'' or partial replications (*Brandt et al., 2014*). Partial replications involve some procedural modifications while conceptual replications (also called instrumental replication) test the same hypothesis (and predictions) using markedly different experimental approaches (*Schmidt, 2009*). It is useful to think of exact replications being at one end of the replication spectrum with quasi-replications at the other; partial and conceptual replications occupy the space between these extremes.\n\nThe replication of research studies (or at least the publication of replications) has been rather poor across disciplines in the social and natural sciences despite its need. For example, only 1% of papers published in the top 100 psychology journals were partial replications (*Makel, Plucker & Hegarty, 2012*) and estimates across other disciplines show equally low rates in economics (0.1%, *Mueller-Langer et al., 2019*), marketing (0%, *Hubbard & Armstrong, 1994*; 1.2%, *Evanschitzky et al., 2007*), advertising/marketing/communication (0.8%, *Reid, Soley & Winner, 1981*), education (0.13%, *Makel & Plucker, 2014*), forecasting (8.4%, *Evanschitzky & Armstrong, 2010*), and finance (0.1%, *Hubbard & Vetter, 1991*).",
    "is_useful": true,
    "question": "What are the different types of replication in research studies, and how common are they in various academic disciplines?"
  },
  {
    "text": "For example, only 1% of papers published in the top 100 psychology journals were partial replications (*Makel, Plucker & Hegarty, 2012*) and estimates across other disciplines show equally low rates in economics (0.1%, *Mueller-Langer et al., 2019*), marketing (0%, *Hubbard & Armstrong, 1994*; 1.2%, *Evanschitzky et al., 2007*), advertising/marketing/communication (0.8%, *Reid, Soley & Winner, 1981*), education (0.13%, *Makel & Plucker, 2014*), forecasting (8.4%, *Evanschitzky & Armstrong, 2010*), and finance (0.1%, *Hubbard & Vetter, 1991*). *Kelly (2006)* found that 25\u201334% of the published papers in behavioural ecology's top three journals (*Animal Behaviour*, *Behavioural Ecology and Sociobiology*, and *Behavioural Ecology*) were partial/conceptual replications whereas no exact replications were found.\n\nNot only are scientists failing to conduct (or publish) replications, but more worryingly, we are failing to replicate original research findings when studies are repeated. Two separate studies conducted by The Many Labs project successfully replicated 77% (*Klein et al., 2014*) and 54% (*Klein et al., 2018*) of psychology studies.",
    "is_useful": true,
    "question": "What are some statistics regarding the rates of replication studies in various scientific disciplines?"
  },
  {
    "text": "*Kelly (2006)* found that 25\u201334% of the published papers in behavioural ecology's top three journals (*Animal Behaviour*, *Behavioural Ecology and Sociobiology*, and *Behavioural Ecology*) were partial/conceptual replications whereas no exact replications were found.\n\nNot only are scientists failing to conduct (or publish) replications, but more worryingly, we are failing to replicate original research findings when studies are repeated. Two separate studies conducted by The Many Labs project successfully replicated 77% (*Klein et al., 2014*) and 54% (*Klein et al., 2018*) of psychology studies. The *Open Science Collaboration (2015)* found that 36% of 100 studies in psychology successfully replicated. Other bioscience fields of research have shown equally poor replication success. For example, 11% of landmark preclinical cancer trials (*Begley & Ellis, 2012*) and 35% of pharmacology studies were found to replicate (*Prinz, Schlange & Asadullah, 2011*). Only 44% of the 49 most widely-cited clinical research studies replicated (*Ioannidis, 2005*). Similar rates are found in the social sciences as replications contradicted previously published findings in 60% of finance studies (*Hubbard & Vetter, 1991*), 40% in advertising, marketing and communication (*Reid, Soley & Winner, 1981*), and 54% in accounting, economics, finance, management, and marketing (*Hubbard & Vetter, 1996*).",
    "is_useful": true,
    "question": "What evidence suggests that replication rates in various scientific fields are low?"
  },
  {
    "text": "Other bioscience fields of research have shown equally poor replication success. For example, 11% of landmark preclinical cancer trials (*Begley & Ellis, 2012*) and 35% of pharmacology studies were found to replicate (*Prinz, Schlange & Asadullah, 2011*). Only 44% of the 49 most widely-cited clinical research studies replicated (*Ioannidis, 2005*). Similar rates are found in the social sciences as replications contradicted previously published findings in 60% of finance studies (*Hubbard & Vetter, 1991*), 40% in advertising, marketing and communication (*Reid, Soley & Winner, 1981*), and 54% in accounting, economics, finance, management, and marketing (*Hubbard & Vetter, 1996*). *Camerer et al. (2016)* found that 61% of laboratory studies in economics successfully replicated a previous finding.\n\nPerhaps the apparent low success of replication studies stems from the manner in which success is judged. There is no single standard for evaluating replication success (*Open Science Collaboration, 2015*) but most replications are deemed successful if they find a result that is statistically significant in the same direction as the result from the original study (*Simonsohn, 2015*).",
    "is_useful": true,
    "question": "What are some of the documented challenges related to replication success in various scientific fields?"
  },
  {
    "text": "Similar rates are found in the social sciences as replications contradicted previously published findings in 60% of finance studies (*Hubbard & Vetter, 1991*), 40% in advertising, marketing and communication (*Reid, Soley & Winner, 1981*), and 54% in accounting, economics, finance, management, and marketing (*Hubbard & Vetter, 1996*). *Camerer et al. (2016)* found that 61% of laboratory studies in economics successfully replicated a previous finding.\n\nPerhaps the apparent low success of replication studies stems from the manner in which success is judged. There is no single standard for evaluating replication success (*Open Science Collaboration, 2015*) but most replications are deemed successful if they find a result that is statistically significant in the same direction as the result from the original study (*Simonsohn, 2015*). This approach has several shortcomings (*Cumming, 2008*) not least of which is that our confidence in the original study is unnecessarily undermined when replications are underpowered (i.e., a low-powered, non-significant study calls the original finding into question) (*Simonsohn, 2015*). A second approach is to use meta-analytic techniques to assess whether the 95% confidence intervals of the original and replicate overlap. Rather than asking whether the replication differs from zero, this approach asks whether it differs from the original estimate. This method, however, is poor at detecting false-positives (*Simonsohn, 2015*).",
    "is_useful": true,
    "question": "What factors contribute to the challenges of replication studies in the social sciences?"
  },
  {
    "text": "There is no single standard for evaluating replication success (*Open Science Collaboration, 2015*) but most replications are deemed successful if they find a result that is statistically significant in the same direction as the result from the original study (*Simonsohn, 2015*). This approach has several shortcomings (*Cumming, 2008*) not least of which is that our confidence in the original study is unnecessarily undermined when replications are underpowered (i.e., a low-powered, non-significant study calls the original finding into question) (*Simonsohn, 2015*). A second approach is to use meta-analytic techniques to assess whether the 95% confidence intervals of the original and replicate overlap. Rather than asking whether the replication differs from zero, this approach asks whether it differs from the original estimate. This method, however, is poor at detecting false-positives (*Simonsohn, 2015*). *Simonsohn (2015)* proposed an alternative approach based on the premise that if an original effect size was seen with a small sample size then it should also be seen with a larger sample size in a replicate study. If the effect size of the replicate study could not be detected with the sample size of the original study, then the effect is too small to have been reliably detected by the original experiment, and doubt is cast on the original observation.\n\nWe do not know if there is a 'replication crisis' in ecology and evolution.",
    "is_useful": true,
    "question": "What approaches are used to evaluate the success of replication studies in scientific research?"
  },
  {
    "text": "A second approach is to use meta-analytic techniques to assess whether the 95% confidence intervals of the original and replicate overlap. Rather than asking whether the replication differs from zero, this approach asks whether it differs from the original estimate. This method, however, is poor at detecting false-positives (*Simonsohn, 2015*). *Simonsohn (2015)* proposed an alternative approach based on the premise that if an original effect size was seen with a small sample size then it should also be seen with a larger sample size in a replicate study. If the effect size of the replicate study could not be detected with the sample size of the original study, then the effect is too small to have been reliably detected by the original experiment, and doubt is cast on the original observation.\n\nWe do not know if there is a 'replication crisis' in ecology and evolution. However, in order to make informed decisions on whether we need to change our views and behaviour toward replications, we need, as a first step, an empirical assessment of their frequency and success in the published literature. My aim in this paper is two-fold. First, I attempt to quantify the frequency of *Ecology, Evolution, Behavior, and Systematics* studies claiming to be true replications and then compare this rate with that of a general biology open access publication (*PeerJ*). Second, I calculate the success rate of replications found in these journals.",
    "is_useful": true,
    "question": "What empirical assessments are necessary to evaluate the reliability of replication studies in Ecology and Evolution?"
  },
  {
    "text": "If the effect size of the replicate study could not be detected with the sample size of the original study, then the effect is too small to have been reliably detected by the original experiment, and doubt is cast on the original observation.\n\nWe do not know if there is a 'replication crisis' in ecology and evolution. However, in order to make informed decisions on whether we need to change our views and behaviour toward replications, we need, as a first step, an empirical assessment of their frequency and success in the published literature. My aim in this paper is two-fold. First, I attempt to quantify the frequency of *Ecology, Evolution, Behavior, and Systematics* studies claiming to be true replications and then compare this rate with that of a general biology open access publication (*PeerJ*). Second, I calculate the success rate of replications found in these journals.\n\n## **METHODS AND MATERIALS**\n\nOn 4 June 2017, I downloaded as .xml files the 1,641,366 Open Access papers available in the PubMed database representing 7,439 journals. I then selected only those papers from journals categorized as *Ecology, Evolution, Behavior, and Systematics* within the ''Agricultural and Biological Sciences'' subject area of the SCImago Journal & Country Rank portal (https://www.scimagojr.com/). This resulted in a subset of 38,730 papers from 160 journals (see Supplemental Information for list of journals).",
    "is_useful": true,
    "question": "What is necessary to evaluate the credibility of original observations in ecological and evolutionary studies?"
  },
  {
    "text": "My aim in this paper is two-fold. First, I attempt to quantify the frequency of *Ecology, Evolution, Behavior, and Systematics* studies claiming to be true replications and then compare this rate with that of a general biology open access publication (*PeerJ*). Second, I calculate the success rate of replications found in these journals.\n\n## **METHODS AND MATERIALS**\n\nOn 4 June 2017, I downloaded as .xml files the 1,641,366 Open Access papers available in the PubMed database representing 7,439 journals. I then selected only those papers from journals categorized as *Ecology, Evolution, Behavior, and Systematics* within the ''Agricultural and Biological Sciences'' subject area of the SCImago Journal & Country Rank portal (https://www.scimagojr.com/). This resulted in a subset of 38,730 papers from 160 journals (see Supplemental Information for list of journals).\n\nI used code written in the Python language (available at the Open Science Framework: DOI 10.17605/OSF.IO/WR286) to text-mine this subset of papers for any permutation of the word ''replicate'' (i.e., ''replic*'') in the Introduction and Discussion (see also *Head et al., 2015*). For each instance of ''replic*'' I extracted the sentence as well as the paper's meta-data (doi, ISSN, etc). Each of these instances was added as a row to a .csv file.",
    "is_useful": true,
    "question": "What is the focus of research related to replication rates in a specific subset of open access journals?"
  },
  {
    "text": "This resulted in a subset of 38,730 papers from 160 journals (see Supplemental Information for list of journals).\n\nI used code written in the Python language (available at the Open Science Framework: DOI 10.17605/OSF.IO/WR286) to text-mine this subset of papers for any permutation of the word ''replicate'' (i.e., ''replic*'') in the Introduction and Discussion (see also *Head et al., 2015*). For each instance of ''replic*'' I extracted the sentence as well as the paper's meta-data (doi, ISSN, etc). Each of these instances was added as a row to a .csv file. I eliminated from this group papers published in *PLoS Computational Biology* because these studies did not empirically test ecological or evolutionary hypotheses with living systems. Text-mined papers were from non-open access journals (e.g., Animal Cognition) that provided an open access publishing option as well as open access journals (e.g., *Ecology & Evolution*). In order to compare rates of study replication in discipline-specific (i.e., *Ecology & Evolution*) open access (and hybrid) journals with a multidisciplinary open access journal I also text-mined 3,343 papers published in *PeerJ*.\n\nI then included/excluded each paper based on whether the content of the extracted sentence dealt with a true replication (i.e., exact, partial or conceptual).",
    "is_useful": true,
    "question": "What approach was used to analyze the presence of the word 'replicate' in a subset of scientific papers, and what criteria were applied to include or exclude these papers?"
  },
  {
    "text": "Each of these instances was added as a row to a .csv file. I eliminated from this group papers published in *PLoS Computational Biology* because these studies did not empirically test ecological or evolutionary hypotheses with living systems. Text-mined papers were from non-open access journals (e.g., Animal Cognition) that provided an open access publishing option as well as open access journals (e.g., *Ecology & Evolution*). In order to compare rates of study replication in discipline-specific (i.e., *Ecology & Evolution*) open access (and hybrid) journals with a multidisciplinary open access journal I also text-mined 3,343 papers published in *PeerJ*.\n\nI then included/excluded each paper based on whether the content of the extracted sentence dealt with a true replication (i.e., exact, partial or conceptual). I did not include quasireplications (*Palmer, 2000*) or studies that re-analyzed previously published data (e.g., *Amos, 2009*). Many articles that used the term ''replic*'' but were not true replications, instead using the term in the context of stating that the results needed to be replicated, explaining an experimental design (e.g., replicated treatments), or making reference to DNA studies (e.g., replicated sequences). If the article reported on a replicated study, I retrieved the original study and the replication from the literature. By reading the replication study I was able to ascertain whether the authors of the replication deemed their study a successful replication of the original.",
    "is_useful": true,
    "question": "What criteria were used to determine whether a study was considered a true replication in the context of evaluating open science practices?"
  },
  {
    "text": "I then included/excluded each paper based on whether the content of the extracted sentence dealt with a true replication (i.e., exact, partial or conceptual). I did not include quasireplications (*Palmer, 2000*) or studies that re-analyzed previously published data (e.g., *Amos, 2009*). Many articles that used the term ''replic*'' but were not true replications, instead using the term in the context of stating that the results needed to be replicated, explaining an experimental design (e.g., replicated treatments), or making reference to DNA studies (e.g., replicated sequences). If the article reported on a replicated study, I retrieved the original study and the replication from the literature. By reading the replication study I was able to ascertain whether the authors of the replication deemed their study a successful replication of the original. I also extracted from the original and replication, where possible, the statistical information (e.g., t-value and sample size) required to calculate an effect size (Cohen's d). In cases where the original effect size differs from zero but the replicate does not, I used *Simonsohn*'s (*2015*) detectability approach to determine whether replication results are consistent with an effect size that was large enough to have been detectable in the original study. This approach rests on defining the effect size that would give the original study 33% power (d33%).",
    "is_useful": true,
    "question": "What criteria can be used to determine whether a study is considered a true replication in the context of scientific research?"
  },
  {
    "text": "If the article reported on a replicated study, I retrieved the original study and the replication from the literature. By reading the replication study I was able to ascertain whether the authors of the replication deemed their study a successful replication of the original. I also extracted from the original and replication, where possible, the statistical information (e.g., t-value and sample size) required to calculate an effect size (Cohen's d). In cases where the original effect size differs from zero but the replicate does not, I used *Simonsohn*'s (*2015*) detectability approach to determine whether replication results are consistent with an effect size that was large enough to have been detectable in the original study. This approach rests on defining the effect size that would give the original study 33% power (d33%). A replication having an effect size significantly smaller than d33% is inconsistent with the studied effect being big enough to have been detectable with the original sample size, which then casts doubt on the validity of the original finding (*Simonsohn, 2015*).\n\n## **RESULTS AND DISCUSSION**\n\n#### **The number of replications in the literature**\n\nI found n = 11 papers that claimed to have replicated a previously published study (Table 1); however, one of these papers (*Amos, 2009*) re-analyzed the data of a previously published study (*M\u00f8ller & Cuervo, 2003*) and another (*Cath et al., 2008*) did not replicate a specific study.",
    "is_useful": true,
    "question": "What criteria can be used to assess the validity of replication studies in open science?"
  },
  {
    "text": "This approach rests on defining the effect size that would give the original study 33% power (d33%). A replication having an effect size significantly smaller than d33% is inconsistent with the studied effect being big enough to have been detectable with the original sample size, which then casts doubt on the validity of the original finding (*Simonsohn, 2015*).\n\n## **RESULTS AND DISCUSSION**\n\n#### **The number of replications in the literature**\n\nI found n = 11 papers that claimed to have replicated a previously published study (Table 1); however, one of these papers (*Amos, 2009*) re-analyzed the data of a previously published study (*M\u00f8ller & Cuervo, 2003*) and another (*Cath et al., 2008*) did not replicate a specific study. Therefore, I found that only 0.023% (9/38730) of papers in *Ecology, Evolution, Behavior, and Systematics* journals claimed to truly replicate a previously published study.\n\nExamination of *PeerJ* revealed one replication (*Holman, 2014*) out of 3,343 papers, giving a replication rate of 0.03%, a value that is nearly identical (\u03c7 2 = 6.7e \u221229, p = 1, df = 1) to that observed in *Ecology, Evolution, Behavior, and Systematics* journals.",
    "is_useful": true,
    "question": "What is the significance of effect size in relation to the validity of original research findings according to replication studies?"
  },
  {
    "text": "Therefore, I found that only 0.023% (9/38730) of papers in *Ecology, Evolution, Behavior, and Systematics* journals claimed to truly replicate a previously published study.\n\nExamination of *PeerJ* revealed one replication (*Holman, 2014*) out of 3,343 papers, giving a replication rate of 0.03%, a value that is nearly identical (\u03c7 2 = 6.7e \u221229, p = 1, df = 1) to that observed in *Ecology, Evolution, Behavior, and Systematics* journals.\n\nMy analysis of the *Ecology, Evolution, Behavior, and Systematics* literature suggests that approximately 0.023% of studies in ecology and evolutionary biology truly replicated (or at least claimed to have) a previosuly published study. Although this rate is on par with other disciplines in the natural and social sciences (*Makel, Plucker & Hegarty, 2012*; *Mueller-Langer et al., 2019*; *Evanschitzky et al., 2007*; *Reid, Soley & Winner, 1981*) it is considerably lower than that reported by *Kelly (2006)* for the behavioural ecology literature. *Kelly (2006)* found that 25\u201334% of studies in behaviuoural ecology are partially/conceptually replicated while no studies were exactly replicated. I did not subdivide true replications in the current study and so direct comparisons with *Kelly (2006)* are not possible.",
    "is_useful": true,
    "question": "What was the rate of true replications found in studies within the fields of ecology and evolutionary biology?"
  },
  {
    "text": "I did not subdivide true replications in the current study and so direct comparisons with *Kelly (2006)* are not possible. However, the low replication rate observed in the current study might be due to an underestimation of conceptual replications if authors of this type of replication are less inclined to use the word ''replication'' in their paper (i.e., those studies would not have been highlighted here by text-mining). In other words, perhaps the bulk of the papers in *Kelly (2006)* that were categorized as partial/conceptual replications were conceptual and those types of studies were not identified here by my text-mining protocol.\n\n**Table 1 Replication studies found in full-text searches of (A)** *Ecology, Evolution, Behavior, and Systematic* **journals, and (B)** *PeerJ*. The statistical significance of the studied effect (as judged by the author(s)) in both the original and replication are noted.\n\n| Original study | Replication study | Significant effect | Claimed to have | Note |\n| --- | --- | --- | --- | --- |\n|  |  | in original study? | replicated |  |\n|  |  |  | original study? |  |\n| (a) Ecology, Evolution, Behaviour, and Systematics |  |  |  |  |\n| M\u00f8ller & Cuervo (2003) | Amos (2009) | Yes | No | Data re-analysis |\n| No specific study | Cath et al. (2008) | Various | No claim made |  |\n| Cresswell et al.",
    "is_useful": true,
    "question": "What factors might contribute to the observed low replication rate in studies related to open science?"
  },
  {
    "text": "It seems unlikely that an author who is explicitly conducting a replication, particualrly an exact replication, would not use the word ''replication'' somewhere in their article. However, perhaps authors are reticent to claim their study as a replication because the stigma of study replication persists and thus reduces publication success. Alternatively, perhaps the low rate observed here is due to a higher likelihood of replications being published in multidisciplinary biology journals rather than more targeted sources such as those in *Ecology, Evolution, Behavior, and Systematics*. Contrary to this prediction, I found no difference between the rate of publication in *PeerJ* (a multidisciplinary bioscience journal) and that in *Ecology, Evolution, Behavior, and Systematics* journals. Finally, perhaps the rate of replication observed here is not representative of the field as a whole because publication rates of replications might\n\n![](_page_5_Figure_1.jpeg)\n\n![](_page_5_Figure_2.jpeg)\n\nFull-size DOI: 10.7717/peerj.7654/fig-1\n\nbe higher in non-open access *Ecology, Evolution, Behavior, and Systematics* journals. This seems counter-intuitive, however, as anecdotal evidence suggests that open access journals are expected to have the highest likelihood of publishing a replication. Given these caveats it is important to emphasize that 0.023% likely underestimates the actual rate of study replication in ecology and evolution.",
    "is_useful": true,
    "question": "What factors may influence the publication rates of replication studies in open science?"
  },
  {
    "text": "Contrary to this prediction, I found no difference between the rate of publication in *PeerJ* (a multidisciplinary bioscience journal) and that in *Ecology, Evolution, Behavior, and Systematics* journals. Finally, perhaps the rate of replication observed here is not representative of the field as a whole because publication rates of replications might\n\n![](_page_5_Figure_1.jpeg)\n\n![](_page_5_Figure_2.jpeg)\n\nFull-size DOI: 10.7717/peerj.7654/fig-1\n\nbe higher in non-open access *Ecology, Evolution, Behavior, and Systematics* journals. This seems counter-intuitive, however, as anecdotal evidence suggests that open access journals are expected to have the highest likelihood of publishing a replication. Given these caveats it is important to emphasize that 0.023% likely underestimates the actual rate of study replication in ecology and evolution.\n\n#### **The success of replications**\n\nThe n = 10 replication studies (including *Holman (2014)* but not including *Amos (2009)*; *Cath et al., (2008)*) yielded n = 11 effects (n = 2 effects in the *Jennings, Snook & Hoikkala (2014)*/*Ala-Honkola, Ritchie & Veltsos (2016)* replicate pair). Of these 11 effects, the replication authors concluded that their replication was successful in 36% of cases.",
    "is_useful": true,
    "question": "What observations suggest that open access journals might be expected to have a higher likelihood of publishing replication studies in the field of ecology and evolution?"
  },
  {
    "text": "This seems counter-intuitive, however, as anecdotal evidence suggests that open access journals are expected to have the highest likelihood of publishing a replication. Given these caveats it is important to emphasize that 0.023% likely underestimates the actual rate of study replication in ecology and evolution.\n\n#### **The success of replications**\n\nThe n = 10 replication studies (including *Holman (2014)* but not including *Amos (2009)*; *Cath et al., (2008)*) yielded n = 11 effects (n = 2 effects in the *Jennings, Snook & Hoikkala (2014)*/*Ala-Honkola, Ritchie & Veltsos (2016)* replicate pair). Of these 11 effects, the replication authors concluded that their replication was successful in 36% of cases. I was able to calculate an effect size for both the original and replicate in only three cases; n = 1 for the *Pasukonis et al. (2016)*/*Ringler et al. (2013)* pair and n = 2 for the *Jennings, Snook & Hoikkala (2014)*/*Ala-Honkola, Ritchie & Veltsos (2016)* pair. In the other eight cases, both the replication and original were experiments with qualitative outcomes and did not record quantitative data, or either the original study or replication did not provide the data required to calculate an effect size (Table 1).\n\n*Pasukonis et al. (2016)* replicated *Ringler et al.",
    "is_useful": true,
    "question": "What is the estimated rate of study replication in ecology and evolution according to anecdotal evidence related to open access journals?"
  },
  {
    "text": "Of these 11 effects, the replication authors concluded that their replication was successful in 36% of cases. I was able to calculate an effect size for both the original and replicate in only three cases; n = 1 for the *Pasukonis et al. (2016)*/*Ringler et al. (2013)* pair and n = 2 for the *Jennings, Snook & Hoikkala (2014)*/*Ala-Honkola, Ritchie & Veltsos (2016)* pair. In the other eight cases, both the replication and original were experiments with qualitative outcomes and did not record quantitative data, or either the original study or replication did not provide the data required to calculate an effect size (Table 1).\n\n*Pasukonis et al. (2016)* replicated *Ringler et al.*'s (*2013*) study examining whether male *Allobates femoralis* frogs anticipate the distance to tadpole deposition sites. *Pasukonis et al. (2016)* declared that their replication study was successful since they found a significant positive correlation between distance traveled and tadpole number, the same finding as *Ringler et al. (2013)*. Assessing the effect sizes and confidence intervals supports their conclusion as both effect sizes were positive and neither of the confidence intervals included\n\n!",
    "is_useful": true,
    "question": "What was the success rate of replication in the studies mentioned, and what challenges were faced in calculating effect sizes?"
  },
  {
    "text": "In the other eight cases, both the replication and original were experiments with qualitative outcomes and did not record quantitative data, or either the original study or replication did not provide the data required to calculate an effect size (Table 1).\n\n*Pasukonis et al. (2016)* replicated *Ringler et al.*'s (*2013*) study examining whether male *Allobates femoralis* frogs anticipate the distance to tadpole deposition sites. *Pasukonis et al. (2016)* declared that their replication study was successful since they found a significant positive correlation between distance traveled and tadpole number, the same finding as *Ringler et al. (2013)*. Assessing the effect sizes and confidence intervals supports their conclusion as both effect sizes were positive and neither of the confidence intervals included\n\n![](_page_6_Figure_1.jpeg)\n\n**Figure 2 Mate preference of female** *Drosophila montana* **from (A) Colorado for males from Colorado or Vancouver, and from (B) Vancouver for males from Colorado or Vancouver.** (A) shows that both the original (*Jennings, Snook & Hoikkala, 2014*) and replicate (*Ala-Honkola, Ritchie & Veltsos, 2016*) studies support a lack of preference by Colorado females since both effects overlap zero. The effect of the replicate in (B) does not differ from d33% and so does not refute the claim in the original paper that Vancouver females show a mate preference.",
    "is_useful": true,
    "question": "What are some challenges in replicating studies with qualitative outcomes in open science?"
  },
  {
    "text": "(2013)*. Assessing the effect sizes and confidence intervals supports their conclusion as both effect sizes were positive and neither of the confidence intervals included\n\n![](_page_6_Figure_1.jpeg)\n\n**Figure 2 Mate preference of female** *Drosophila montana* **from (A) Colorado for males from Colorado or Vancouver, and from (B) Vancouver for males from Colorado or Vancouver.** (A) shows that both the original (*Jennings, Snook & Hoikkala, 2014*) and replicate (*Ala-Honkola, Ritchie & Veltsos, 2016*) studies support a lack of preference by Colorado females since both effects overlap zero. The effect of the replicate in (B) does not differ from d33% and so does not refute the claim in the original paper that Vancouver females show a mate preference.\n\nFull-size DOI: 10.7717/peerj.7654/fig-2\n\nzero (Fig. 1). These data suggest that father frogs can indeed anticipate the distance they need to travel to deposit their offspring.\n\n*Jennings, Snook & Hoikkala (2014)* found little evidence of pre-mating isolation in populations of *Drosophila montana*. They found that females from Colorado accepted as mates males from Colorado and Vancouver with equal probability. *Ala-Honkola, Ritchie & Veltsos (2016)* found the same result (based on p-values) in their replication study.",
    "is_useful": true,
    "question": "What findings are reported regarding mate preference in *Drosophila montana* populations from different locations?"
  },
  {
    "text": "The effect of the replicate in (B) does not differ from d33% and so does not refute the claim in the original paper that Vancouver females show a mate preference.\n\nFull-size DOI: 10.7717/peerj.7654/fig-2\n\nzero (Fig. 1). These data suggest that father frogs can indeed anticipate the distance they need to travel to deposit their offspring.\n\n*Jennings, Snook & Hoikkala (2014)* found little evidence of pre-mating isolation in populations of *Drosophila montana*. They found that females from Colorado accepted as mates males from Colorado and Vancouver with equal probability. *Ala-Honkola, Ritchie & Veltsos (2016)* found the same result (based on p-values) in their replication study. Figure 2A supports this conclusion as the effect sizes for both the original and replication overlap zero.\n\nIn contrast, *Jennings, Snook & Hoikkala (2014)* found that Vancouver females were more discriminating: they preferred to mate with males from Vancouver rather than those from Colorado. *Ala-Honkola, Ritchie & Veltsos (2016)* concluded that they did not replicate this finding, instead concluding that Vancouver females showed no significant mate preference one way or the other (Fig. 2B).",
    "is_useful": true,
    "question": "What do studies suggest about the mate preference of Vancouver females in relation to males from different populations?"
  },
  {
    "text": "Importantly, this suggests that the effect discovered by *Jennings, Snook & Hoikkala (2014)* might be biologically important and worthy of further investgation. *Simonsohn*'s (*2015*) detectability approach suggests that *Ala-Honkola, Ritchie & Veltsos*'s (*2016*) replication is in line with *Jennings, Snook & Hoikkala*'s (*2014*) original finding and was not a replication failure as *Ala-Honkola, Ritchie & Veltsos* (*2016*) reported in their paper.\n\n## **CONCLUSION**\n\nThis formal analysis of three replicated effect sizes shows that the replicate study supported the original conclusion in two-thirds of cases. This sample size is too small to support sweeping generalizations of the efficacy of replications in *Ecology, Evolution, Behavior, and Systematics*; however, that authors provided the relevant information to calculate an effect size in only three of 11 cases is cause for concern. I recommend that future authors either include a detectability analysis (*Simonsohn, 2015*) as part of their replication study or at least provide the information required for its calculation.\n\n## **ACKNOWLEDGEMENTS**\n\nThank Tim Parker, Hannah Fraser, and Scott Chamberlain for improving an earlier version of the manuscript. Rob Lanfear kindly offered feedback and advice on modifications to his Python code.\n\n## **ADDITIONAL INFORMATION AND DECLARATIONS**\n\n### **Funding**\n\nThis work was supported by the Canada Research Chairs program and by a Natural Sciences and Engineering Research Council Discovery Grant.",
    "is_useful": true,
    "question": "What is the significance of replicating effect sizes in the context of open science and research methodologies?"
  },
  {
    "text": "This sample size is too small to support sweeping generalizations of the efficacy of replications in *Ecology, Evolution, Behavior, and Systematics*; however, that authors provided the relevant information to calculate an effect size in only three of 11 cases is cause for concern. I recommend that future authors either include a detectability analysis (*Simonsohn, 2015*) as part of their replication study or at least provide the information required for its calculation.\n\n## **ACKNOWLEDGEMENTS**\n\nThank Tim Parker, Hannah Fraser, and Scott Chamberlain for improving an earlier version of the manuscript. Rob Lanfear kindly offered feedback and advice on modifications to his Python code.\n\n## **ADDITIONAL INFORMATION AND DECLARATIONS**\n\n### **Funding**\n\nThis work was supported by the Canada Research Chairs program and by a Natural Sciences and Engineering Research Council Discovery Grant. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\n\n### **Grant Disclosures**\n\nThe following grant information was disclosed by the author: Canada Research Chairs. Natural Sciences and Engineering Research Council Discovery Grant.\n\n## **Competing Interests**\n\nClint D. Kelly is an Academic Editor for PeerJ.\n\n#### **Author Contributions**\n\n- Clint D. Kelly conceived and designed the experiments, performed the experiments, analyzed the data, contributed reagents/materials/analysis tools, prepared figures and/or tables, authored or reviewed drafts of the paper, approved the final draft.",
    "is_useful": true,
    "question": "What recommendations are made for authors regarding replication studies in open science?"
  },
  {
    "text": "Rob Lanfear kindly offered feedback and advice on modifications to his Python code.\n\n## **ADDITIONAL INFORMATION AND DECLARATIONS**\n\n### **Funding**\n\nThis work was supported by the Canada Research Chairs program and by a Natural Sciences and Engineering Research Council Discovery Grant. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\n\n### **Grant Disclosures**\n\nThe following grant information was disclosed by the author: Canada Research Chairs. Natural Sciences and Engineering Research Council Discovery Grant.\n\n## **Competing Interests**\n\nClint D. Kelly is an Academic Editor for PeerJ.\n\n#### **Author Contributions**\n\n- Clint D. Kelly conceived and designed the experiments, performed the experiments, analyzed the data, contributed reagents/materials/analysis tools, prepared figures and/or tables, authored or reviewed drafts of the paper, approved the final draft.\n#### **Data Availability**\n\nThe following information was supplied regarding data availability:\n\nData, Python code, and R code are available at the Open Science Framework (OSF): Kelly, Clint D. 2019. ''Replication in Ecology and Evolution.'' OSF. July 20. 10.17605/OSF.IO/WR286.\n\n## **Supplemental Information**\n\nSupplemental information for this article can be found online at http://dx.doi.org/10.7717/ peerj.7654#supplemental-information.\n\n## **REFERENCES**\n\n- **Ala-Honkola O, Ritchie MG, Veltsos P. 2016.",
    "is_useful": true,
    "question": "What initiative supports the availability of data and code in open science?"
  },
  {
    "text": "#### **Author Contributions**\n\n- Clint D. Kelly conceived and designed the experiments, performed the experiments, analyzed the data, contributed reagents/materials/analysis tools, prepared figures and/or tables, authored or reviewed drafts of the paper, approved the final draft.\n#### **Data Availability**\n\nThe following information was supplied regarding data availability:\n\nData, Python code, and R code are available at the Open Science Framework (OSF): Kelly, Clint D. 2019. ''Replication in Ecology and Evolution.'' OSF. July 20. 10.17605/OSF.IO/WR286.\n\n## **Supplemental Information**\n\nSupplemental information for this article can be found online at http://dx.doi.org/10.7717/ peerj.7654#supplemental-information.\n\n## **REFERENCES**\n\n- **Ala-Honkola O, Ritchie MG, Veltsos P. 2016.** Postmating\u2013prezygotic isolation between two allopatric populations of *Drosophila montana*: fertilisation success differs under sperm competition. *Ecology and Evolution* 6:1679\u20131691 DOI 10.1002/ece3.1995.\n- **Amos W. 2009.** Sexual selection does not influence minisatellite mutation rate. BMC *Evolutionary Biology* 9:5 DOI 10.1186/1471-2148-9-5.\n- **Begley CG, Ellis LM. 2012.** Raise standards for preclinical cancer research.",
    "is_useful": true,
    "question": "What resources related to research findings are made available at the Open Science Framework?"
  },
  {
    "text": "*BMC Evolutionary Biology* 3:6 DOI 10.1186/1471-2148-3-6.\n- **M\u00fcller CA, Riemer S, Vir\u00e1nyi Z, Huber L, Range F. 2014.** Dogs learn to solve the support problem based on perceptual cues. *Animal Cognition* **17(5)**:1071\u20131080.\n- **Mueller-Langer F, Fecher B, Harhoff D, Wagner GG. 2019.** Replication studies in economics\u2014how many and which papers are chosen for replication, and why. *Research Policy* **48(1)**:62\u201383 DOI 10.1016/j.respol.2018.07.019.\n- **Nakagawa S, Parker TH. 2015.** Replicating research in ecology and evolution: feasibility, incentives, and the cost-benefit conundrum. *BMC Biology* **13(1)**:88 DOI 10.1186/s12915-015-0196-3.\n- **Open Science Collaboration. 2015.** Estimating the reproducibility of psychological science. *Science* **349(6251)**:aac4716 DOI 10.1126/science.aac4716.\n- **Palmer AR. 2000.** Quasi-replication and the contract of error: lessons from sex ratios, heritabilities and fluctuating asymmetry.",
    "is_useful": true,
    "question": "What is the importance of replication studies in the context of open science?"
  },
  {
    "text": "*Research Policy* **48(1)**:62\u201383 DOI 10.1016/j.respol.2018.07.019.\n- **Nakagawa S, Parker TH. 2015.** Replicating research in ecology and evolution: feasibility, incentives, and the cost-benefit conundrum. *BMC Biology* **13(1)**:88 DOI 10.1186/s12915-015-0196-3.\n- **Open Science Collaboration. 2015.** Estimating the reproducibility of psychological science. *Science* **349(6251)**:aac4716 DOI 10.1126/science.aac4716.\n- **Palmer AR. 2000.** Quasi-replication and the contract of error: lessons from sex ratios, heritabilities and fluctuating asymmetry. *Annual Review of Ecology and Systematics* **31(1)**:441\u2013480 DOI 10.1146/annurev.ecolsys.31.1.441.\n- **Parker T, Forstmeier W, Koricheva J, Fidler F, Hadfield J, Chee Y, Kelly C, Gurevitch J, Nakagawa S. 2016.** Transparency in ecology and evolution: real problems, real solutions. *Trends in Ecology & Evolution* **31(9)**:711\u2013719 DOI 10.1016/j.tree.2016.07.002.",
    "is_useful": true,
    "question": "What are some key aspects and challenges of transparency and reproducibility in research within the fields of ecology and evolution?"
  },
  {
    "text": "## R E S EAR CH A R TIC L E Open Access\n\n![](_page_0_Picture_4.jpeg)\n\n# Clinical trial registration and reporting: a survey of academic organizations in the United States\n\nEvan Mayo-Wilson1* , James Heyward1 , Anthony Keyes2 , Jesse Reynolds3 , Sarah White4 , Nidhi Atri5 , G. Caleb Alexander6 , Audrey Omar3 , Daniel E. Ford5 and on behalf of the National Clinical Trials Registration and Results Reporting Taskforce Survey Subcommittee\n\n#### Abstract\n\nBackground: Many clinical trials conducted by academic organizations are not published, or are not published completely. Following the US Food and Drug Administration Amendments Act of 2007, \"The Final Rule\" (compliance date April 18, 2017) and a National Institutes of Health policy clarified and expanded trial registration and results reporting requirements. We sought to identify policies, procedures, and resources to support trial registration and reporting at academic organizations.\n\nMethods: We conducted an online survey from November 21, 2016 to March 1, 2017, before organizations were expected to comply with The Final Rule. We included active Protocol Registration and Results System (PRS) accounts classified by ClinicalTrials.gov as a \"University/Organization\" in the USA. PRS administrators manage information on ClinicalTrials.gov. We invited one PRS administrator to complete the survey for each organization account, which was the unit of analysis.\n\nResults: Eligible organization accounts (N = 783) included 47,701 records (e.g., studies) in August 2016.",
    "is_useful": true,
    "question": "What are the challenges associated with clinical trial registration and reporting in academic organizations?"
  },
  {
    "text": "Following the US Food and Drug Administration Amendments Act of 2007, \"The Final Rule\" (compliance date April 18, 2017) and a National Institutes of Health policy clarified and expanded trial registration and results reporting requirements. We sought to identify policies, procedures, and resources to support trial registration and reporting at academic organizations.\n\nMethods: We conducted an online survey from November 21, 2016 to March 1, 2017, before organizations were expected to comply with The Final Rule. We included active Protocol Registration and Results System (PRS) accounts classified by ClinicalTrials.gov as a \"University/Organization\" in the USA. PRS administrators manage information on ClinicalTrials.gov. We invited one PRS administrator to complete the survey for each organization account, which was the unit of analysis.\n\nResults: Eligible organization accounts (N = 783) included 47,701 records (e.g., studies) in August 2016. Participating organizations (366/783; 47%) included 40,351/47,701 (85%) records. Compared with other organizations, Clinical and Translational Science Award (CTSA) holders, cancer centers, and large organizations were more likely to participate. A minority of accounts have a registration (156/366; 43%) or results reporting policy (129/366; 35%). Of those with policies, 15/156 (11%) and 49/156 (35%) reported that trials must be registered before institutional review board approval is granted or before beginning enrollment, respectively.",
    "is_useful": true,
    "question": "What policies and practices have been identified to enhance compliance with trial registration and results reporting in academic organizations?"
  },
  {
    "text": "PRS administrators manage information on ClinicalTrials.gov. We invited one PRS administrator to complete the survey for each organization account, which was the unit of analysis.\n\nResults: Eligible organization accounts (N = 783) included 47,701 records (e.g., studies) in August 2016. Participating organizations (366/783; 47%) included 40,351/47,701 (85%) records. Compared with other organizations, Clinical and Translational Science Award (CTSA) holders, cancer centers, and large organizations were more likely to participate. A minority of accounts have a registration (156/366; 43%) or results reporting policy (129/366; 35%). Of those with policies, 15/156 (11%) and 49/156 (35%) reported that trials must be registered before institutional review board approval is granted or before beginning enrollment, respectively. Few organizations use computer software to monitor compliance (68/366; 19%). One organization had penalized an investigator for non-compliance. Among the 287/366 (78%) accounts reporting that they allocate staff to fulfill ClinicalTrials.gov registration and reporting requirements, the median number of full-time equivalent staff is 0.08 (interquartile range = 0.02\u20130.25). Because of non-response and social desirability, this could be a \"best case\" scenario.\n\nConclusions: Before the compliance date for The Final Rule, some academic organizations had policies and resources that facilitate clinical trial registration and reporting.",
    "is_useful": true,
    "question": "What trends were observed in the participation and compliance of organizations in clinical trial registration and reporting prior to the compliance date for The Final Rule?"
  },
  {
    "text": "Of those with policies, 15/156 (11%) and 49/156 (35%) reported that trials must be registered before institutional review board approval is granted or before beginning enrollment, respectively. Few organizations use computer software to monitor compliance (68/366; 19%). One organization had penalized an investigator for non-compliance. Among the 287/366 (78%) accounts reporting that they allocate staff to fulfill ClinicalTrials.gov registration and reporting requirements, the median number of full-time equivalent staff is 0.08 (interquartile range = 0.02\u20130.25). Because of non-response and social desirability, this could be a \"best case\" scenario.\n\nConclusions: Before the compliance date for The Final Rule, some academic organizations had policies and resources that facilitate clinical trial registration and reporting. Most organizations appear to be unprepared to meet the new requirements. Organizations could enact the following: adopt policies that require trial registration and reporting, allocate resources (e.g., staff, software) to support registration and reporting, and ensure there are consequences for investigators who do not follow standards for clinical research.\n\nKeywords: Clinical trials, Trial registration, Results reporting, Reporting bias\n\n* Correspondence: evan.mayo-wilson@jhu.edu 1\n\nFull list of author information is available at the end of the article\n\n![](_page_0_Picture_16.jpeg)\n\n\u00a9 The Author(s).",
    "is_useful": true,
    "question": "What are some recommended actions for organizations to improve compliance with clinical trial registration and reporting standards?"
  },
  {
    "text": "Because of non-response and social desirability, this could be a \"best case\" scenario.\n\nConclusions: Before the compliance date for The Final Rule, some academic organizations had policies and resources that facilitate clinical trial registration and reporting. Most organizations appear to be unprepared to meet the new requirements. Organizations could enact the following: adopt policies that require trial registration and reporting, allocate resources (e.g., staff, software) to support registration and reporting, and ensure there are consequences for investigators who do not follow standards for clinical research.\n\nKeywords: Clinical trials, Trial registration, Results reporting, Reporting bias\n\n* Correspondence: evan.mayo-wilson@jhu.edu 1\n\nFull list of author information is available at the end of the article\n\n![](_page_0_Picture_16.jpeg)\n\n\u00a9 The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.",
    "is_useful": true,
    "question": "What measures can academic organizations take to enhance compliance with clinical trial registration and reporting requirements?"
  },
  {
    "text": "Keywords: Clinical trials, Trial registration, Results reporting, Reporting bias\n\n* Correspondence: evan.mayo-wilson@jhu.edu 1\n\nFull list of author information is available at the end of the article\n\n![](_page_0_Picture_16.jpeg)\n\n\u00a9 The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\n\nDepartment of Epidemiology, Johns Hopkins University Bloomberg School of Public Health, 615 North Wolfe Street, E6036, Baltimore, MD 21205, USA\n\n#### Background\n\nClinical trials provide evidence about the safety and effectiveness of interventions (Table 1). They underpin health policy and regulation, and they inform patient and provider healthcare decision-making. Because many trials are not published [1\u20136], and because many published reports do not include all of the information needed to understand trial methods [7\u201310] and results [11\u201317], decisions based on published evidence alone may not lead to the best balance of benefits and harms for patients [18\u201321].",
    "is_useful": true,
    "question": "What is the significance of clinical trials in healthcare decision-making?"
  },
  {
    "text": "The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\n\nDepartment of Epidemiology, Johns Hopkins University Bloomberg School of Public Health, 615 North Wolfe Street, E6036, Baltimore, MD 21205, USA\n\n#### Background\n\nClinical trials provide evidence about the safety and effectiveness of interventions (Table 1). They underpin health policy and regulation, and they inform patient and provider healthcare decision-making. Because many trials are not published [1\u20136], and because many published reports do not include all of the information needed to understand trial methods [7\u201310] and results [11\u201317], decisions based on published evidence alone may not lead to the best balance of benefits and harms for patients [18\u201321].\n\nTo help participants enroll in trials, improve access to information, and reduce bias, authors have long proposed registering all trials prospectively [22\u201327]. The Food and Drug Administration Modernization Act of 1997 led to the creation of ClinicalTrials.gov, a publicly accessible database maintained by the National Library\n\n| of Medicine (NLM), which launched in 2000 [28].",
    "is_useful": true,
    "question": "What are the advantages of registering clinical trials prospectively in relation to open science?"
  },
  {
    "text": "They underpin health policy and regulation, and they inform patient and provider healthcare decision-making. Because many trials are not published [1\u20136], and because many published reports do not include all of the information needed to understand trial methods [7\u201310] and results [11\u201317], decisions based on published evidence alone may not lead to the best balance of benefits and harms for patients [18\u201321].\n\nTo help participants enroll in trials, improve access to information, and reduce bias, authors have long proposed registering all trials prospectively [22\u201327]. The Food and Drug Administration Modernization Act of 1997 led to the creation of ClinicalTrials.gov, a publicly accessible database maintained by the National Library\n\n| of Medicine (NLM), which launched in 2000 [28]. In |  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- |\n| 2004, the International Committee of Medical Journal |  |  |  |  |  |  |\n| Editors | (ICMJE) | announced | that | trials | initiated | from |\n| 2005 would have to be registered to be considered for |  |  |  |  |  |  |\n| publication [29, 30].",
    "is_useful": true,
    "question": "What measures have been proposed to improve transparency and access to information in clinical trials?"
  },
  {
    "text": "To help participants enroll in trials, improve access to information, and reduce bias, authors have long proposed registering all trials prospectively [22\u201327]. The Food and Drug Administration Modernization Act of 1997 led to the creation of ClinicalTrials.gov, a publicly accessible database maintained by the National Library\n\n| of Medicine (NLM), which launched in 2000 [28]. In |  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- |\n| 2004, the International Committee of Medical Journal |  |  |  |  |  |  |\n| Editors | (ICMJE) | announced | that | trials | initiated | from |\n| 2005 would have to be registered to be considered for |  |  |  |  |  |  |\n| publication [29, 30]. Title VIII of the Food and Drug Ad |  |  |  |  |  |  |\n| ministration Amendments Act of 2007 (FDAAA) [31] |  |  |  |  |  |  |\n| required that certain trials of drugs, biologics, and medical |  |  |  |  |  |  |\n| devices be registered and that results for trials of approved |  |  |  |  |  |  |\n| products be posted on ClinicalTrials.gov.",
    "is_useful": true,
    "question": "What measures have been taken to promote transparency and access in clinical trials?"
  },
  {
    "text": "Title VIII of the Food and Drug Ad |  |  |  |  |  |  |\n| ministration Amendments Act of 2007 (FDAAA) [31] |  |  |  |  |  |  |\n| required that certain trials of drugs, biologics, and medical |  |  |  |  |  |  |\n| devices be registered and that results for trials of approved |  |  |  |  |  |  |\n| products be posted on ClinicalTrials.gov. The FDAAA |  |  |  |  |  |  |\n| also authorized the Food and Drug Administration (FDA) |  |  |  |  |  |  |\n| to issue fines for non-compliance, currently up to $11,569 |  |  |  |  |  |  |\n| per trial per day [32]. \"The Final Rule,\", which took effect |  |  |  |  |  |  |\n| on January 18, 2017, clarified and expanded requirements |  |  |  |  |  |  |\n| for registration and reporting (Box 1) [33, 34]; organiza |  |  |  |  |  |  |\n| tions were expected to be in compliance by April 18, |  |  |  |  |  |  |\n| 2017.",
    "is_useful": true,
    "question": "What are the requirements for the registration and reporting of certain clinical trials as mandated by recent legislation?"
  },
  {
    "text": "The FDAAA |  |  |  |  |  |  |\n| also authorized the Food and Drug Administration (FDA) |  |  |  |  |  |  |\n| to issue fines for non-compliance, currently up to $11,569 |  |  |  |  |  |  |\n| per trial per day [32]. \"The Final Rule,\", which took effect |  |  |  |  |  |  |\n| on January 18, 2017, clarified and expanded requirements |  |  |  |  |  |  |\n| for registration and reporting (Box 1) [33, 34]; organiza |  |  |  |  |  |  |\n| tions were expected to be in compliance by April 18, |  |  |  |  |  |  |\n| 2017. In a complementary policy, the National Institutes |  |  |  |  |  |  |\n\nTable 1 Glossary of terms\n\n| Term | Definition |\n| --- | --- |\n| Application programming interface (API) | A set of methods to facilitate communication among software components, as described in Section |\n|  | 10 of the PRS User's Guide (https://prsinfo.clinicaltrials.gov/prs-users-guide.html#section10) |\n| Cancer center | An organization that specializes in the diagnosis and treatment of cancer, including organizations |\n|  | designated by the National Cancer Institute (see \"National Cancer Institute cancer center\") |\n| Clinical trial (\"trial\") | A study in which human participants are assigned prospectively to receive one or more interventions |\n|  | (i.e., diagnostic, therapeutic, or other types) to evaluate the effects of the intervention(s) on |\n|  | health-related outcomes.",
    "is_useful": true,
    "question": "What regulations exist regarding compliance and penalties for clinical trial registration and reporting?"
  },
  {
    "text": "In a complementary policy, the National Institutes |  |  |  |  |  |  |\n\nTable 1 Glossary of terms\n\n| Term | Definition |\n| --- | --- |\n| Application programming interface (API) | A set of methods to facilitate communication among software components, as described in Section |\n|  | 10 of the PRS User's Guide (https://prsinfo.clinicaltrials.gov/prs-users-guide.html#section10) |\n| Cancer center | An organization that specializes in the diagnosis and treatment of cancer, including organizations |\n|  | designated by the National Cancer Institute (see \"National Cancer Institute cancer center\") |\n| Clinical trial (\"trial\") | A study in which human participants are assigned prospectively to receive one or more interventions |\n|  | (i.e., diagnostic, therapeutic, or other types) to evaluate the effects of the intervention(s) on |\n|  | health-related outcomes. For example, see [34, 36] |\n| Clinical and Translational Science Awards | Awards funded by the National Center for Advancing Translational Sciences (NCATS), a part of the |\n| (CTSA) | National Institutes of Health (NIH), to support a consortium of 64 medical research organizations |\n|  | (https://ncats.nih.gov/ctsa) |\n| Food and Drug Administration | US Public Law 110-85, which established clinical trial registration and reporting requirements |\n| Amendments Act of 2007 (FDAAA) | (section 801) [31].",
    "is_useful": true,
    "question": "What role do application programming interfaces (APIs) play in facilitating communication among software components in the context of open science?"
  },
  {
    "text": "For example, see [34, 36] |\n| Clinical and Translational Science Awards | Awards funded by the National Center for Advancing Translational Sciences (NCATS), a part of the |\n| (CTSA) | National Institutes of Health (NIH), to support a consortium of 64 medical research organizations |\n|  | (https://ncats.nih.gov/ctsa) |\n| Food and Drug Administration | US Public Law 110-85, which established clinical trial registration and reporting requirements |\n| Amendments Act of 2007 (FDAAA) | (section 801) [31]. |\n| Institutional review board (IRB) | A group of persons with responsibility for ensuring the protection of human subjects involved in |\n|  | research. For example, see [58\u201360] |\n| Investigator | A researcher involved in a clinical trial [34, 36]. |\n| National Cancer Institute cancer center | One of 69 organizations designated by the National Cancer Institute (NCI) that specialize in the |\n| (NCI cancer center) | diagnosis and treatment of cancer (https://www.cancer.gov/research/nci-role/cancer-centers) |\n| Protocol Registration and Results System | A web-based data entry system used to register studies on ClinicalTrials.gov and to submit results for |\n| (PRS) | registered studies |\n| PRS organization account (\"account\") | An account assigned to an organization and used to enter information about clinical trials in the Protocol |\n|  | Registration and Results System.",
    "is_useful": true,
    "question": "What is the purpose of the Clinical and Translational Science Awards funded by the National Center for Advancing Translational Sciences?"
  },
  {
    "text": "|\n| Institutional review board (IRB) | A group of persons with responsibility for ensuring the protection of human subjects involved in |\n|  | research. For example, see [58\u201360] |\n| Investigator | A researcher involved in a clinical trial [34, 36]. |\n| National Cancer Institute cancer center | One of 69 organizations designated by the National Cancer Institute (NCI) that specialize in the |\n| (NCI cancer center) | diagnosis and treatment of cancer (https://www.cancer.gov/research/nci-role/cancer-centers) |\n| Protocol Registration and Results System | A web-based data entry system used to register studies on ClinicalTrials.gov and to submit results for |\n| (PRS) | registered studies |\n| PRS organization account (\"account\") | An account assigned to an organization and used to enter information about clinical trials in the Protocol |\n|  | Registration and Results System. An organization account may be managed by one or more administrators |\n|  | and may include trials conducted by multiple investigators |\n| PRS administrator (\"administrator\") | A person who manages an organization account in the Protocol Registration and Results System. |\n|  | Administrators are able to create accounts for individual investigators, review trial information, modify |\n|  | permissions for editing trial information, and check for problems |\n| Trial registration (registration) | The process of entering a minimum dataset about a clinical trial in registry that is accessible to the |\n|  | public (e.g., ClinicalTrials.gov) [34, 36].",
    "is_useful": true,
    "question": "What is the significance of trial registration in clinical research?"
  },
  {
    "text": "An organization account may be managed by one or more administrators |\n|  | and may include trials conducted by multiple investigators |\n| PRS administrator (\"administrator\") | A person who manages an organization account in the Protocol Registration and Results System. |\n|  | Administrators are able to create accounts for individual investigators, review trial information, modify |\n|  | permissions for editing trial information, and check for problems |\n| Trial registration (registration) | The process of entering a minimum dataset about a clinical trial in registry that is accessible to the |\n|  | public (e.g., ClinicalTrials.gov) [34, 36]. |\n| Responsible party | The person or entity responsible for submitting information about a clinical study to ClinicalTrials.gov |\n|  | and updating that information [34, 36]. |\n| Results | Summary information about intervention effects, including participant flow, outcome measures, and |\n|  | adverse events [34, 36]. |\n| Sponsor | The person or organization who oversees a clinical trial and is responsible for study data [34, 36]. |\n| The Final Rule (42 CFR 11) | A federal regulation that implements Section 801 of the Food and Drug Administration Amendments |\n|  | Act of 2007 (FDAAA) and expands requirements for trial registration and results reporting. The |\n|  | effective date is January 18, 2017 and the compliance date is April 18, 2017 [34].",
    "is_useful": true,
    "question": "What are some key responsibilities and processes related to the management and registration of clinical trials?"
  },
  {
    "text": "|\n| Responsible party | The person or entity responsible for submitting information about a clinical study to ClinicalTrials.gov |\n|  | and updating that information [34, 36]. |\n| Results | Summary information about intervention effects, including participant flow, outcome measures, and |\n|  | adverse events [34, 36]. |\n| Sponsor | The person or organization who oversees a clinical trial and is responsible for study data [34, 36]. |\n| The Final Rule (42 CFR 11) | A federal regulation that implements Section 801 of the Food and Drug Administration Amendments |\n|  | Act of 2007 (FDAAA) and expands requirements for trial registration and results reporting. The |\n|  | effective date is January 18, 2017 and the compliance date is April 18, 2017 [34]. |\n| University/organization | A \"type of organization\" used to classify PRS organization accounts by www.ClinicalTrials.gov. |\n\n#### Box 1: Registration and reporting requirements for clinical trials\n\nInternational Committee of Medical Journal Editors (ICMJE)\n\n- To be considered for publication, clinical trials must be registered in public registry before enrolling participants [29, 30].\n- Reports of clinical trials must include a data sharing statement [57].",
    "is_useful": true,
    "question": "What are the requirements for clinical trials to be considered for publication according to the International Committee of Medical Journal Editors?"
  },
  {
    "text": "|\n| The Final Rule (42 CFR 11) | A federal regulation that implements Section 801 of the Food and Drug Administration Amendments |\n|  | Act of 2007 (FDAAA) and expands requirements for trial registration and results reporting. The |\n|  | effective date is January 18, 2017 and the compliance date is April 18, 2017 [34]. |\n| University/organization | A \"type of organization\" used to classify PRS organization accounts by www.ClinicalTrials.gov. |\n\n#### Box 1: Registration and reporting requirements for clinical trials\n\nInternational Committee of Medical Journal Editors (ICMJE)\n\n- To be considered for publication, clinical trials must be registered in public registry before enrolling participants [29, 30].\n- Reports of clinical trials must include a data sharing statement [57].\n\nFood and Drug Administration Amendments Act of 2007 (FDAAA) and The Final Rule [31, 34]\n\n- Applicable clinical trials must be registered on ClinicalTrials.gov within 21 days of enrolling the first participant\n- Trial registrations must include the primary and secondary outcomes, including the specific measures and time-points that will be used to assess trial outcomes\n- Results must be reported within 12 months of the final data collection in support of the primary outcome. (The Final Rule expanded this requirement to include both approved and unapproved products.)",
    "is_useful": true,
    "question": "What are the key requirements for clinical trials regarding registration and results reporting according to the Final Rule and the FDAAA?"
  },
  {
    "text": "|\n\n#### Box 1: Registration and reporting requirements for clinical trials\n\nInternational Committee of Medical Journal Editors (ICMJE)\n\n- To be considered for publication, clinical trials must be registered in public registry before enrolling participants [29, 30].\n- Reports of clinical trials must include a data sharing statement [57].\n\nFood and Drug Administration Amendments Act of 2007 (FDAAA) and The Final Rule [31, 34]\n\n- Applicable clinical trials must be registered on ClinicalTrials.gov within 21 days of enrolling the first participant\n- Trial registrations must include the primary and secondary outcomes, including the specific measures and time-points that will be used to assess trial outcomes\n- Results must be reported within 12 months of the final data collection in support of the primary outcome. (The Final Rule expanded this requirement to include both approved and unapproved products.)\n- Results must include the primary and secondary outcomes, all serious adverse events, all-cause mortality, and adverse events occurring in 5% of participants\n- Results must include baseline information on age and gender and, if collected, for race or ethnic group\n- Records must be updatedas follows: within 15 days of changes to approval or clearance status; within 30 days of reaching the primary completion date; and within 30 days of changes to trial recruitment status, human subjects protection review board status, or responsible party National Institutes of Health (NIH) [36]\n\t- Requirements for NIH-funded clinical trials mirror the requirements for applicable trials under FDAAA\n\t- All clinical trials funded by NIH (in whole or in part) must be registered, and their results must be reported, on ClinicalTrials.gov\n\nof Health (NIH) issued broader requirements that apply to all trials funded by the NIH, including early trials and trials of behavioral interventions [35, 36].",
    "is_useful": true,
    "question": "What are the registration and reporting requirements for clinical trials under open science principles?"
  },
  {
    "text": "- Results must include the primary and secondary outcomes, all serious adverse events, all-cause mortality, and adverse events occurring in 5% of participants\n- Results must include baseline information on age and gender and, if collected, for race or ethnic group\n- Records must be updatedas follows: within 15 days of changes to approval or clearance status; within 30 days of reaching the primary completion date; and within 30 days of changes to trial recruitment status, human subjects protection review board status, or responsible party National Institutes of Health (NIH) [36]\n\t- Requirements for NIH-funded clinical trials mirror the requirements for applicable trials under FDAAA\n\t- All clinical trials funded by NIH (in whole or in part) must be registered, and their results must be reported, on ClinicalTrials.gov\n\nof Health (NIH) issued broader requirements that apply to all trials funded by the NIH, including early trials and trials of behavioral interventions [35, 36].\n\nThere is little evidence about how academic organizations support trial registration and reporting, but some evidence suggests that they are unprepared to meet these requirements. For example, academic organizations have performed worse than industry in registering trials prospectively [37, 38] and reporting results [39\u201346].\n\n#### Methods\n\nBetween November 21, 2016 and March 1, 2017, we conducted an online survey of academic organizations in the USA. We surveyed administrators who are responsible for maintaining organization accounts on ClinicalTrials.gov.",
    "is_useful": true,
    "question": "What are the requirements for results reporting and registration for NIH-funded clinical trials?"
  },
  {
    "text": "There is little evidence about how academic organizations support trial registration and reporting, but some evidence suggests that they are unprepared to meet these requirements. For example, academic organizations have performed worse than industry in registering trials prospectively [37, 38] and reporting results [39\u201346].\n\n#### Methods\n\nBetween November 21, 2016 and March 1, 2017, we conducted an online survey of academic organizations in the USA. We surveyed administrators who are responsible for maintaining organization accounts on ClinicalTrials.gov. For each eligible ClinicalTrials.gov account, we asked one administrator to describe the policies and procedures and the available resources to support trial registration and reporting at their organization (Box 2).\n\n#### Identifying eligible PRS accounts\n\nThe online system used to enter information in the ClinicalTrials.gov database is called the Protocol Registration and Results System (PRS). Each study registered on ClinicalTrials.gov is associated with a \"record\" of that study, and each record is assigned to one PRS organization account. A record may or may not include study results. A single organization, such as a university or health system, might register trials using one or many accounts. For example, \"Yale University\" is one account; by comparison, \"Harvard Medical School\" and \"Harvard School of Dental Medicine\" are each separate accounts.\n\nWe used the PRS account as the unit of analysis because accounts related to the same organization often\n\n#### Box 2: Survey topics\n\nPolicies and procedures\n\n- Does the organization have a policy that requires investigators to register their trials? A results reporting policy?",
    "is_useful": true,
    "question": "What challenges do academic organizations face in supporting trial registration and reporting compared to industry?"
  },
  {
    "text": "#### Identifying eligible PRS accounts\n\nThe online system used to enter information in the ClinicalTrials.gov database is called the Protocol Registration and Results System (PRS). Each study registered on ClinicalTrials.gov is associated with a \"record\" of that study, and each record is assigned to one PRS organization account. A record may or may not include study results. A single organization, such as a university or health system, might register trials using one or many accounts. For example, \"Yale University\" is one account; by comparison, \"Harvard Medical School\" and \"Harvard School of Dental Medicine\" are each separate accounts.\n\nWe used the PRS account as the unit of analysis because accounts related to the same organization often\n\n#### Box 2: Survey topics\n\nPolicies and procedures\n\n- Does the organization have a policy that requires investigators to register their trials? A results reporting policy?\n- Which trials are covered by these policies?\n- When did these policies come into effect?\n- Do these policies describe processes for investigators joining and leaving the organization?\n- Are there penalties for investigators who do not register their trials or report their results?\n\nStaffing and support\n\n- Which functions do staff members perform (e.g., entering results, checking records, educating investigators)?\n- How many staff members are assigned to support trial registration and results reporting? How much time do they spend on these activities?\n- Are there plans to hire more staff in the future? Monitoring systems\n\t- Does the organization have a system for monitoring trial registration and results reporting? For notifying investigators when results are due?",
    "is_useful": true,
    "question": "What are some key topics organizations need to consider regarding their policies and procedures for trial registration and results reporting?"
  },
  {
    "text": "We used the PRS account as the unit of analysis because accounts related to the same organization often\n\n#### Box 2: Survey topics\n\nPolicies and procedures\n\n- Does the organization have a policy that requires investigators to register their trials? A results reporting policy?\n- Which trials are covered by these policies?\n- When did these policies come into effect?\n- Do these policies describe processes for investigators joining and leaving the organization?\n- Are there penalties for investigators who do not register their trials or report their results?\n\nStaffing and support\n\n- Which functions do staff members perform (e.g., entering results, checking records, educating investigators)?\n- How many staff members are assigned to support trial registration and results reporting? How much time do they spend on these activities?\n- Are there plans to hire more staff in the future? Monitoring systems\n\t- Does the organization have a system for monitoring trial registration and results reporting? For notifying investigators when results are due?\n- Does an IRB check whether trials are registered and reported?\n\nrepresent schools or departments that have separate policies and procedures related to trial registration and reporting. Furthermore, we are not aware of a reliable method to associate individual accounts with organization. For example, the \"Johns Hopkins University\" account includes mostly records from the Johns Hopkins University School of Medicine. Investigators at Johns Hopkins University also register trials using the accounts \"Johns Hopkins Bloomberg School of Public Health,\" \"Johns Hopkins Children's Hospital,\" and \"Sidney Kimmel Comprehensive Cancer Center.\"",
    "is_useful": true,
    "question": "What are some key topics addressed in organizational policies and procedures regarding trial registration and results reporting?"
  },
  {
    "text": "- How many staff members are assigned to support trial registration and results reporting? How much time do they spend on these activities?\n- Are there plans to hire more staff in the future? Monitoring systems\n\t- Does the organization have a system for monitoring trial registration and results reporting? For notifying investigators when results are due?\n- Does an IRB check whether trials are registered and reported?\n\nrepresent schools or departments that have separate policies and procedures related to trial registration and reporting. Furthermore, we are not aware of a reliable method to associate individual accounts with organization. For example, the \"Johns Hopkins University\" account includes mostly records from the Johns Hopkins University School of Medicine. Investigators at Johns Hopkins University also register trials using the accounts \"Johns Hopkins Bloomberg School of Public Health,\" \"Johns Hopkins Children's Hospital,\" and \"Sidney Kimmel Comprehensive Cancer Center.\" Schools and hospitals related to Johns Hopkins University have distinct policies, faculties, administrative staff, and institutional review boards (IRBs).\n\nWe included all \"active\" accounts categorized by Clinical-Trials.gov as a \"University/Organization\" in the USA. We received a spreadsheet from the NLM with the number of records in each eligible account on August 4, 2016, and we received PRS administrator contact information from the NLM on September 28, 2016 and December 12, 2016.\n\n#### Survey design\n\nWe developed a draft survey based on investigators' content knowledge and evidence from studies that were known to us at the time.",
    "is_useful": true,
    "question": "What are some key aspects that organizations must consider regarding trial registration and results reporting?"
  },
  {
    "text": "For example, the \"Johns Hopkins University\" account includes mostly records from the Johns Hopkins University School of Medicine. Investigators at Johns Hopkins University also register trials using the accounts \"Johns Hopkins Bloomberg School of Public Health,\" \"Johns Hopkins Children's Hospital,\" and \"Sidney Kimmel Comprehensive Cancer Center.\" Schools and hospitals related to Johns Hopkins University have distinct policies, faculties, administrative staff, and institutional review boards (IRBs).\n\nWe included all \"active\" accounts categorized by Clinical-Trials.gov as a \"University/Organization\" in the USA. We received a spreadsheet from the NLM with the number of records in each eligible account on August 4, 2016, and we received PRS administrator contact information from the NLM on September 28, 2016 and December 12, 2016.\n\n#### Survey design\n\nWe developed a draft survey based on investigators' content knowledge and evidence from studies that were known to us at the time. We organized questions into three domains: (1) organization characteristics, (2) registration and results policies and practices, and (3) staff and resources. We also invited participants to describe any compliance efforts that our questions did not cover. We then piloted the survey among 14 members of the National Clinical Trials Registration and Results Reporting Taskforce. The final survey used skip logic so that participants saw only those questions that were relevant based on their previous answers. Responses were saved automatically, and participants could return to the survey at any time; this allowed participants to discuss their answers with organizational colleagues before submitting.",
    "is_useful": true,
    "question": "What are some key considerations in designing a survey for investigating registration and results policies in clinical trials?"
  },
  {
    "text": "#### Survey design\n\nWe developed a draft survey based on investigators' content knowledge and evidence from studies that were known to us at the time. We organized questions into three domains: (1) organization characteristics, (2) registration and results policies and practices, and (3) staff and resources. We also invited participants to describe any compliance efforts that our questions did not cover. We then piloted the survey among 14 members of the National Clinical Trials Registration and Results Reporting Taskforce. The final survey used skip logic so that participants saw only those questions that were relevant based on their previous answers. Responses were saved automatically, and participants could return to the survey at any time; this allowed participants to discuss their answers with organizational colleagues before submitting. We conducted the survey using Qualtrics software (www.qualtrics.com/); a copy is available as a Word document (Additional file 1) and on the Qualtrics website (http://bit.ly/2tCSqyl).\n\n#### Participant recruitment\n\nOne or more persons, called \"PRS administrators\" by ClinicalTrials.gov, may add or modify records in each account. Some PRS administrators are employed specifically to work on ClinicalTrials.gov, but many PRS administrators have little or no time budgeted by their organizations to work on ClinicalTrials.gov.\n\nFor each eligible account, we created a unique internet address (URL) which we emailed in an invitation letter to one administrator.",
    "is_useful": true,
    "question": "What are the key components considered in the survey designed to assess open science practices?"
  },
  {
    "text": "For continuous data, we calculated the median and interquartile range (IQR) depending on the distribution of responses.\n\nWe conducted subgroup analyses to determine whether organization characteristics might be related to policies and resources. We compared:\n\n- 1. Accounts affiliated with a Clinical and Translational Science Award (CTSA) versus other accounts\n- 2. Accounts affiliated with a cancer center versus other accounts\n- 3. Accounts with < 20 records, 20\u201399 records, and \u2265 100 records\n\nWe conducted a sensitivity analysis to determine whether the results might be sensitive to non-response bias by comparing accounts that responded before the effective date for The Final Rule (January 18, 2017) with accounts that responded on or after The Final Rule took effect.\n\n#### Results\n\n#### Characteristics of eligible accounts\n\nWe identified 783 eligible accounts (Additional file 2), which had 47,701 records by August 2016. The median number of records per account was 7 (IQR = 3\u201336), ranging from 1 (two accounts) to 1563 (mean = 61, standard deviation (SD) = 155). A minority of accounts are responsible for most records; 113/783 (14%) accounts had \u2265 100 records by August 2016, and these accounts were responsible for 38,311/47,701 (80%) records.\n\nThe median number of administrators per account was 1 (IQR = 1\u20133), and one organization had 182 registered administrators.",
    "is_useful": true,
    "question": "What statistical measures were used to analyze the data from eligible accounts in the study?"
  },
  {
    "text": "#### Results\n\n#### Characteristics of eligible accounts\n\nWe identified 783 eligible accounts (Additional file 2), which had 47,701 records by August 2016. The median number of records per account was 7 (IQR = 3\u201336), ranging from 1 (two accounts) to 1563 (mean = 61, standard deviation (SD) = 155). A minority of accounts are responsible for most records; 113/783 (14%) accounts had \u2265 100 records by August 2016, and these accounts were responsible for 38,311/47,701 (80%) records.\n\nThe median number of administrators per account was 1 (IQR = 1\u20133), and one organization had 182 registered administrators.\n\n#### Survey participation\n\nOf 783 eligible accounts, we found no contact details for 16 (2%) and attempted to contact 767 (98%). In four cases (< 1%), we were unable to identify a usable email address. Of eligible accounts, 10/783 (1%) emailed us to decline, 306/783 (39%) did not participate in the survey, and 81/783 (10%) did not provide sufficient information to be included in the analysis (Fig. 1). Two accounts reported that they had multiple policies related to the same account; we asked them to complete questions about their account characteristics but not to complete questions about their specific policies and resources.\n\nIncluded accounts were responsible for 40,351/47,701 (85%) records registered by eligible accounts.",
    "is_useful": true,
    "question": "What percentage of eligible accounts are responsible for the majority of records in an open science context?"
  },
  {
    "text": "1). Two accounts reported that they had multiple policies related to the same account; we asked them to complete questions about their account characteristics but not to complete questions about their specific policies and resources.\n\nIncluded accounts were responsible for 40,351/47,701 (85%) records registered by eligible accounts. We received a partial (43) or complete (323) survey for 366/783 (47%) eligible accounts (Additional file 3).\n\nThe first account completed the survey on November 21, 2016, and the last account completed the survey on March 21, 2017; 31/366 (9%) accounts completed the survey after January 17, 2017. Because of skip logic and because some accounts did not answer all possible questions, accounts answered between 6 and 42 questions (median 19, IQR 17\u201329).\n\n#### Policies and practices\n\nOf 366 accounts, 156 (43%) reported that they have a registration policy and 129 (35%) have a results reporting policy (Table 2). Policies came into effect between 2000 and 2016 (median = 2013, IQR 2010\u20132015; mode = 2016).\n\nAmong those accounts with policies, most policies require registration of trials applicable under FDAAA (118/140, 84%) and funded by the NIH (72/140, 51%) (Additional file 4).",
    "is_useful": true,
    "question": "What percentage of accounts that responded reported having a registration policy?"
  },
  {
    "text": "Because of skip logic and because some accounts did not answer all possible questions, accounts answered between 6 and 42 questions (median 19, IQR 17\u201329).\n\n#### Policies and practices\n\nOf 366 accounts, 156 (43%) reported that they have a registration policy and 129 (35%) have a results reporting policy (Table 2). Policies came into effect between 2000 and 2016 (median = 2013, IQR 2010\u20132015; mode = 2016).\n\nAmong those accounts with policies, most policies require registration of trials applicable under FDAAA (118/140, 84%) and funded by the NIH (72/140, 51%) (Additional file 4). Polices include different requirements for time of registration (Table 3); most require that trials be registered before IRB approval is granted (15/156; 11%), before enrollment begins (49/156; 35%), or within 21 days of beginning enrollment (31/156; 22%). A minority of policies address handling trials associated with investigators joining (57/156; 37%) and leaving organizations (38/156; 24%).\n\n![](_page_4_Figure_9.jpeg)\n\nTable 2 Clinical trial registration and results reporting policies\n\n| QUESTION | No. | Percentage |\n| --- | --- | --- |\n| (number of participants who viewed the question) |  |  |\n| Trial registration policies |  |  |\n| Does the organization have a registration policy?",
    "is_useful": true,
    "question": "What percentage of accounts reported having a registration policy related to clinical trials?"
  },
  {
    "text": "Polices include different requirements for time of registration (Table 3); most require that trials be registered before IRB approval is granted (15/156; 11%), before enrollment begins (49/156; 35%), or within 21 days of beginning enrollment (31/156; 22%). A minority of policies address handling trials associated with investigators joining (57/156; 37%) and leaving organizations (38/156; 24%).\n\n![](_page_4_Figure_9.jpeg)\n\nTable 2 Clinical trial registration and results reporting policies\n\n| QUESTION | No. | Percentage |\n| --- | --- | --- |\n| (number of participants who viewed the question) |  |  |\n| Trial registration policies |  |  |\n| Does the organization have a registration policy? |  |  |\n| (N = 366)a |  |  |\n| Yes | 156 | 43% |\n| No | 173 | 47% |\n| Don't know | 37 | 10% |\n| Does the policy cover investigators joining the organization? (N = 156)b |  |  |\n| Yes | 57 | 37% |\n| No | 76 | 49% |\n| Don't know | 23 | 15% |\n| Skipped (did not answer) | 0 | 0% |\n| Does the policy cover investigators leaving the organization?",
    "is_useful": true,
    "question": "What are the common timeframes required for clinical trial registration according to various policies?"
  },
  {
    "text": "| Percentage |\n| --- | --- | --- |\n| (number of participants who viewed the question) |  |  |\n| Trial registration policies |  |  |\n| Does the organization have a registration policy? |  |  |\n| (N = 366)a |  |  |\n| Yes | 156 | 43% |\n| No | 173 | 47% |\n| Don't know | 37 | 10% |\n| Does the policy cover investigators joining the organization? (N = 156)b |  |  |\n| Yes | 57 | 37% |\n| No | 76 | 49% |\n| Don't know | 23 | 15% |\n| Skipped (did not answer) | 0 | 0% |\n| Does the policy cover investigators leaving the organization? (N = 156)b |  |  |\n| Yes | 38 | 24% |\n| No | 87 | 56% |\n| Don't know | 31 | 20% |\n| Skipped (did not answer) | 0 | 0% |\n| According to the policy, when must trials be registered?",
    "is_useful": true,
    "question": "What are the findings regarding trial registration policies in organizations?"
  },
  {
    "text": "| Percentage |\n| --- | --- | --- |\n| (number of participants who viewed the question) |  |  |\n| Results reporting policies |  |  |\n| Does the organization have a results reporting policy? (N = 366)a |  |  |\n| Yes | 129 | 35% |\n| No | 193 | 53% |\n| Don't know | 44 | 12% |\n| According to the policy, who is responsible for monitoring if results are reported on time? (N = 115)b,c |  |  |\n| Principal investigator | 54 | 47% |\n| Institutional review board | 5 | 4% |\n| PRS administrator | 68 | 59% |\n| Other | 12 | 10% |\n| This responsibility is not assigned in the policy | 18 | 16% |\n| Don't know | 1 | 1% |\n| Skipped (did not answer) | 0 | 0% |\n| According to the policy, can investigators be penalized by the = 114)e organization for failing to report a trial (N |  |  |\n| Yes | 21 | 18% |\n| No | 75 | 66% |\n| Don't know | 18 | 16% |\n| Skipped (did not answer) | 0 | 0% |\n\na An answer to this question was required for an account to be included in the analysis; accounts that did not see or skipped this question were excluded from all analyses\n\nb The number of possible responses (i.e., the denominator) includes the accounts with a relevant policy that viewed this question.",
    "is_useful": true,
    "question": "What percentage of organizations have a results reporting policy according to the survey?"
  },
  {
    "text": "The number of accounts that viewed each question is less than the total number of accounts in the study because (1) participants did not see all questions because of skip logic, and (2) some participants discontinued the survey before viewing all questions\n\nc Because participants could \"check all that apply,\" the sum of all categories exceeds the number of participants who responded (i.e., some participants selected multiple responses)\n\nd Because 50 (36%) selected \"Don't know,\" 89 accounts are included in the analysis\n\ne Of 111 accounts who viewed either question about penalties for (1) failing to register or (2) failing to report a trial, 17 (15%) responded \"Yes\" to both questions, and 31 (28%) responded \"Yes\" to one or both questions\n\nResponsibility for registering trials is most often assigned to principal investigators (72/129; 56%). Responsibility for monitoring whether results are reported on time is most often assigned to principal investigators (54/115, 47%) and administrators (68/115, 59%).\n\nSome policies allow organizations to penalize investigators who fail to register trials (27/115; 18%) or fail to report results (21/114; 18%). One account (< 1%) reported that their organization had penalized an investigator for non-compliance.\n\n#### Resources\n\nFew accounts use computer software to manage their records (68/366; 19%). Of those that use computer\n\n#### Table 3 Resources to support clinical trial registration and results reporting\n\n| QUESTION (number of participants who viewed question) | No.",
    "is_useful": true,
    "question": "What factors can affect the number of accounts that view questions in a study involving trial registration and reporting?"
  },
  {
    "text": "Responsibility for monitoring whether results are reported on time is most often assigned to principal investigators (54/115, 47%) and administrators (68/115, 59%).\n\nSome policies allow organizations to penalize investigators who fail to register trials (27/115; 18%) or fail to report results (21/114; 18%). One account (< 1%) reported that their organization had penalized an investigator for non-compliance.\n\n#### Resources\n\nFew accounts use computer software to manage their records (68/366; 19%). Of those that use computer\n\n#### Table 3 Resources to support clinical trial registration and results reporting\n\n| QUESTION (number of participants who viewed question) | No. | Percentage |\n| --- | --- | --- |\n| Does the organization have an electronic system for managing trial registration or results reporting?",
    "is_useful": true,
    "question": "What are some responsibilities and consequences associated with clinical trial registration and results reporting within organizations?"
  },
  {
    "text": "Responsibility for monitoring whether results are reported on time is most often assigned to principal investigators (54/115, 47%) and administrators (68/115, 59%).\n\nSome policies allow organizations to penalize investigators who fail to register trials (27/115; 18%) or fail to report results (21/114; 18%). One account (< 1%) reported that their organization had penalized an investigator for non-compliance.\n\n#### Resources\n\nFew accounts use computer software to manage their records (68/366; 19%). Of those that use computer\n\n#### Table 3 Resources to support clinical trial registration and results reporting\n\n| QUESTION (number of participants who viewed question) | No. | Percentage |\n| --- | --- | --- |\n| Does the organization have an electronic system for managing trial registration or results reporting? (N | = 366)a |  |\n| Yesb | 68 | 19% |\n| No | 272 | 74% |\n| Don't know | 26 | 7% |\n| Which functions do staff who support registration and results reporting perform (N | = 342)c |  |\n| Group training (e.g., classroom style) | 61 | 18% |\n| Individual training | 151 | 44% |\n| Enter data for principal investigators (PIs) | 174 | 51% |\n| Maintain an educational website | 57 | 17% |\n| Notify PIs about problems or sanctions | 241 | 70% |\n| Assistance with analysis | 58 | 17% |\n| Respond to questions | 241 | 70% |\n| Review problem records | 262 | 77% |\n| Other | 28 | 8% |\n| Don't know | 22 | 6% |\n| Skipped (did not answer) | 0 | 0% |\n| What is the highest qualification of any staff member?",
    "is_useful": true,
    "question": "What percentage of organizations penalize investigators for failing to report trial results or register trials?"
  },
  {
    "text": "The number of accounts that viewed each question is less than the total number of accounts in the study because (1) participants did not see all questions because of skip logic, and (2) some participants discontinued the survey before viewing all questions\n\ne Higher degrees include JD (N = 21, 7%), PhD (N = 69, 22%), and MD (N = 32, 10%); 13 accounts selected 2 higher degrees (8 both PhD and JD, 5 both PhD and MD)\n\nf The number of possible responses was limited to the accounts that reported monitoring compliance with their results reporting policy\n\ng Of the 11 accounts reporting that IRBs monitor trial registration, 4 indicated that the IRB requires registration for approval for some (N = 3) or all trials (N = 1) h Results are the median and interquartile range. We also calculated mean = 0.3, standard deviation = 0.6\n\nsoftware, two use the application programming interface (API) to connect with ClinicalTrials.gov (Table 3).\n\nAmong the 287/366 (78%) accounts that allocate staff to fulfill ClinicalTrials.gov registration and reporting requirements, the median number of full-time equivalent (FTE) staff is 0.08 (IQR = 0.02\u20130.25).",
    "is_useful": true,
    "question": "What factors may affect the number of accounts that view all questions in a survey related to open science practices?"
  },
  {
    "text": "We also calculated mean = 0.3, standard deviation = 0.6\n\nsoftware, two use the application programming interface (API) to connect with ClinicalTrials.gov (Table 3).\n\nAmong the 287/366 (78%) accounts that allocate staff to fulfill ClinicalTrials.gov registration and reporting requirements, the median number of full-time equivalent (FTE) staff is 0.08 (IQR = 0.02\u20130.25). Among the staff who support ClinicalTrials.gov registration and reporting requirements, the staff member with the highest level of education has a graduate degree (232/411; 75%) more often than a bachelor's degree (68/411; 22%) or a high school diploma (11/411; 3%). At the time of this survey, 34/338 (10%) planned to hire more staff, while 217/338 (64%) and 87/338 (26%) did not plan to hire more staff or did not know, respectively. Among accounts affiliated with a CTSA, 24/109 (22%) receive support for Clinical-Trials.gov compliance from the CTSA.",
    "is_useful": true,
    "question": "What proportion of accounts allocated staff for fulfilling ClinicalTrials.gov registration and reporting requirements plan to hire more staff?"
  },
  {
    "text": "Among the 287/366 (78%) accounts that allocate staff to fulfill ClinicalTrials.gov registration and reporting requirements, the median number of full-time equivalent (FTE) staff is 0.08 (IQR = 0.02\u20130.25). Among the staff who support ClinicalTrials.gov registration and reporting requirements, the staff member with the highest level of education has a graduate degree (232/411; 75%) more often than a bachelor's degree (68/411; 22%) or a high school diploma (11/411; 3%). At the time of this survey, 34/338 (10%) planned to hire more staff, while 217/338 (64%) and 87/338 (26%) did not plan to hire more staff or did not know, respectively. Among accounts affiliated with a CTSA, 24/109 (22%) receive support for Clinical-Trials.gov compliance from the CTSA.\n\nStaff perform various roles, including educating investigators individually (151/342; 44%) and in groups (61/ 42; 18%), entering data for principal investigators (174/ 342; 51%), maintaining educational websites (57/342; 17%), notifying investigators about problems (241/342; 70%), assisting with analysis (58/342; 17%), responding to questions (241/342; 70%), and reviewing problem records (262/342; 77%).",
    "is_useful": true,
    "question": "What are some of the roles that staff perform in relation to ClinicalTrials.gov registration and reporting requirements?"
  },
  {
    "text": "Not cancer center Accounts not affiliated with an NCI or other cancer center. \u2265 100 records Accounts with 100 or more registered studies in the USA for which the organization was listed as the \"lead sponsor.\" 20\u201399 records Accounts with between 20 and 99 registered studies. < 20 records Accounts with fewer than 20 registered studies a\n\nTwo accounts did not report whether they are affiliated with a cancer center; they are not included in this subgroup analysis\n\nb Results are for accounts that responded to this question. In our initial analysis, we found potentially invalid data; for example, some participants entered \"0.5\" rather than \"50%\". This occurred because a software bug prevented us from enforcing a data validation rule in the survey. To verify these results, we emailed administrators who indicated that staff spent \u2264 1% of their time on trial registration and reporting. Post hoc, we excluded two outliers because they appeared to report the total number of staff employed at the organization rather than the number of staff who support trial registration and results reporting\n\nrecords, (2) those affiliated with CTSAs, and (3) those affiliated with cancer centers (Table 4). For example, most cancer centers have a registration policy (61/97; 63%) and a reporting policy (52/97; 54%); a minority of other accounts have a registration policy (94/267; 35%) or a reporting policy (77/267; 28%).",
    "is_useful": true,
    "question": "What are the differences in trial registration and reporting policies between accounts affiliated with cancer centers and those that are not?"
  },
  {
    "text": "In our initial analysis, we found potentially invalid data; for example, some participants entered \"0.5\" rather than \"50%\". This occurred because a software bug prevented us from enforcing a data validation rule in the survey. To verify these results, we emailed administrators who indicated that staff spent \u2264 1% of their time on trial registration and reporting. Post hoc, we excluded two outliers because they appeared to report the total number of staff employed at the organization rather than the number of staff who support trial registration and results reporting\n\nrecords, (2) those affiliated with CTSAs, and (3) those affiliated with cancer centers (Table 4). For example, most cancer centers have a registration policy (61/97; 63%) and a reporting policy (52/97; 54%); a minority of other accounts have a registration policy (94/267; 35%) or a reporting policy (77/267; 28%).\n\n#### Non-response bias\n\nWe found direct and indirect evidence of non-response bias, which suggests that our results might overestimate the amount of support available at academic organizations. For example, one administrator who declined to participate replied that their organization \"does not have any central staff managing clinicaltrials.gov and does not utilize an institutional account.\"\n\n#### Table 5 Characteristics of participants\n\nQUESTION (number of participants who viewed the question) No.",
    "is_useful": true,
    "question": "What issues can arise from data entry errors and non-response bias in the context of trial registration and reporting in academic organizations?"
  },
  {
    "text": "Of those accounts we invited to complete the survey that included < 20 records, 171/532 (32%) participated. By comparison, 98/113 (87%) accounts with \u2265 100 records participated.\n\nParticipation might have been related to organization resources. Nearly all CTSAs (62/64; 97%) and most National Cancer Institute (NCI) cancer centers (55/69; 80%) participated in the survey (Table 5), including 48 accounts affiliated with both a cancer center and a CTSA. Furthermore, some included accounts were related; for example, 107 accounts were affiliated with one of the 62 CTSAs.\n\nIn a sensitivity analysis (Additional file 5), we found no clear differences in policies and computer software when comparing early and late responders. Most participants completed the survey before the effective date, so late responders included only 31/366 (8%) accounts.\n\n#### Discussion\n\n#### Summary of findings\n\nTo our knowledge, this is the largest and most comprehensive survey of organizations that register and report clinical trials on ClinicalTrials.gov. We had a high participation rate, and accounts that completed the survey conduct the overwhelming majority of clinical trials registered by academic organizations in the USA. We found that some organizations were prepared to meet trial registration and reporting requirements before The Final Rule took effect, but there is wide variation in practice. Most organizations do not have policies for trial registration and reporting. Most existing policies are consistent with FDAAA; however, most are not consistent with the ICMJE registration policy.",
    "is_useful": true,
    "question": "What did the survey reveal about the participation rates of organizations involved in clinical trials registration and reporting?"
  },
  {
    "text": "In a sensitivity analysis (Additional file 5), we found no clear differences in policies and computer software when comparing early and late responders. Most participants completed the survey before the effective date, so late responders included only 31/366 (8%) accounts.\n\n#### Discussion\n\n#### Summary of findings\n\nTo our knowledge, this is the largest and most comprehensive survey of organizations that register and report clinical trials on ClinicalTrials.gov. We had a high participation rate, and accounts that completed the survey conduct the overwhelming majority of clinical trials registered by academic organizations in the USA. We found that some organizations were prepared to meet trial registration and reporting requirements before The Final Rule took effect, but there is wide variation in practice. Most organizations do not have policies for trial registration and reporting. Most existing policies are consistent with FDAAA; however, most are not consistent with the ICMJE registration policy. Nearly half of existing policies do not require registration of all NIH-funded trials, though organizations could adapt their polices in response to the new NIH requirements. Few policies include penalties for investigators who do not register or report their trials. Although some organizations use computer software to monitor trial registration and reporting, only two have systems that connect directly with ClinicalTrials.gov (i.e., using API). Most staff who support trial registration and reporting have other responsibilities, and most organizations do not plan to hire more staff to support trial registration and reporting.\n\n#### Implications\n\nOur results suggest that most organizations assign responsibility for trial registration and reporting to individual investigators and provide little oversight.",
    "is_useful": true,
    "question": "What were the findings regarding the policies and practices of organizations related to clinical trial registration and reporting according to a comprehensive survey?"
  },
  {
    "text": "Most organizations do not have policies for trial registration and reporting. Most existing policies are consistent with FDAAA; however, most are not consistent with the ICMJE registration policy. Nearly half of existing policies do not require registration of all NIH-funded trials, though organizations could adapt their polices in response to the new NIH requirements. Few policies include penalties for investigators who do not register or report their trials. Although some organizations use computer software to monitor trial registration and reporting, only two have systems that connect directly with ClinicalTrials.gov (i.e., using API). Most staff who support trial registration and reporting have other responsibilities, and most organizations do not plan to hire more staff to support trial registration and reporting.\n\n#### Implications\n\nOur results suggest that most organizations assign responsibility for trial registration and reporting to individual investigators and provide little oversight. Previous studies indicate that senior investigators often delegate this responsibility to their junior colleagues [47].\n\nTo our knowledge, the FDA has never assessed a civil monetary penalty for failing to register or report a trial, and the NIH has never penalized an organization for failing to meet their requirements. The ICMJE policy is not applied uniformly [48], and many published trials are still not registered prospectively and completely [37, 49\u201352]. Organizations may be more likely to comply with these requirements if they are held accountable for doing so by journals, FDA, and funders (see, e.g., http://www.who.int/ ictrp/results/jointstatement/en).\n\nImproving research transparency in the long term will require changes in norms and culture.",
    "is_useful": true,
    "question": "What challenges do organizations face regarding trial registration and reporting in the context of improving research transparency?"
  },
  {
    "text": "#### Implications\n\nOur results suggest that most organizations assign responsibility for trial registration and reporting to individual investigators and provide little oversight. Previous studies indicate that senior investigators often delegate this responsibility to their junior colleagues [47].\n\nTo our knowledge, the FDA has never assessed a civil monetary penalty for failing to register or report a trial, and the NIH has never penalized an organization for failing to meet their requirements. The ICMJE policy is not applied uniformly [48], and many published trials are still not registered prospectively and completely [37, 49\u201352]. Organizations may be more likely to comply with these requirements if they are held accountable for doing so by journals, FDA, and funders (see, e.g., http://www.who.int/ ictrp/results/jointstatement/en).\n\nImproving research transparency in the long term will require changes in norms and culture. Organizations could take four immediate steps to improve trial registration and reporting. First, organizations could offer education to help investigators understand these requirements. Second, organizations could implement policies and procedures to support trial registration and reporting. For example, organizations could require that investigators answer questions on IRB applications to identify clinical trials that require registration. Organizations could also require that investigators provide trial registration numbers before allowing trials to commence. Third, organizations could identify trials that do not meet trial registration and reporting requirements and help individual investigators bring those trials into compliance. Notably, software could provide automatic reminders when trial information needs to be updated [53] or when results will be due, and software could help organizations identify problems that require attention from leaders.",
    "is_useful": true,
    "question": "What steps can organizations take to enhance compliance with trial registration and reporting requirements in order to improve research transparency?"
  },
  {
    "text": "Improving research transparency in the long term will require changes in norms and culture. Organizations could take four immediate steps to improve trial registration and reporting. First, organizations could offer education to help investigators understand these requirements. Second, organizations could implement policies and procedures to support trial registration and reporting. For example, organizations could require that investigators answer questions on IRB applications to identify clinical trials that require registration. Organizations could also require that investigators provide trial registration numbers before allowing trials to commence. Third, organizations could identify trials that do not meet trial registration and reporting requirements and help individual investigators bring those trials into compliance. Notably, software could provide automatic reminders when trial information needs to be updated [53] or when results will be due, and software could help organizations identify problems that require attention from leaders. Prospective reminders would allow administrators and investigators to update information before they become non-compliant with reporting requirements. Finally, organizations could ensure there are consequences for investigators who fail to meet trial registration and reporting requirements. For example, organizations could stop enrollment in ongoing trials or stop investigators from obtaining new grants [54].\n\n#### Limitations\n\nAlthough we sent multiple reminders and gave participants months to respond, our results might be influenced by non-response and social desirability. However, such biases would lead us to overestimate support for research trial registration and reporting. Participating accounts conduct more trials than non-participating accounts, and they appear to be most likely to have policies and resources to support transparency.",
    "is_useful": true,
    "question": "What steps can organizations take to improve research transparency in trial registration and reporting?"
  },
  {
    "text": "Notably, software could provide automatic reminders when trial information needs to be updated [53] or when results will be due, and software could help organizations identify problems that require attention from leaders. Prospective reminders would allow administrators and investigators to update information before they become non-compliant with reporting requirements. Finally, organizations could ensure there are consequences for investigators who fail to meet trial registration and reporting requirements. For example, organizations could stop enrollment in ongoing trials or stop investigators from obtaining new grants [54].\n\n#### Limitations\n\nAlthough we sent multiple reminders and gave participants months to respond, our results might be influenced by non-response and social desirability. However, such biases would lead us to overestimate support for research trial registration and reporting. Participating accounts conduct more trials than non-participating accounts, and they appear to be most likely to have policies and resources to support transparency.\n\nBecause we analyzed results by account, our results are not directly comparable with studies that grouped trials using the data fields \"funder\" [39, 40, 43], \"sponsor\" [41, 44], \"collaborator\" [41], or \"affiliation\" [42]. We analyzed results by account because (1) the account should usually represent the \"responsible party,\" which is the person or organization legally responsible for fulfilling trial registration and reporting requirements, and (2) because we were not aware of another method to identify all trials, or even all accounts, associated with each organization.\n\nWe could not always determine which trials were associated with specific organizations, and organizations might not know which accounts their investigators use.",
    "is_useful": true,
    "question": "How can organizations ensure compliance with trial registration and reporting requirements in research?"
  },
  {
    "text": "However, such biases would lead us to overestimate support for research trial registration and reporting. Participating accounts conduct more trials than non-participating accounts, and they appear to be most likely to have policies and resources to support transparency.\n\nBecause we analyzed results by account, our results are not directly comparable with studies that grouped trials using the data fields \"funder\" [39, 40, 43], \"sponsor\" [41, 44], \"collaborator\" [41], or \"affiliation\" [42]. We analyzed results by account because (1) the account should usually represent the \"responsible party,\" which is the person or organization legally responsible for fulfilling trial registration and reporting requirements, and (2) because we were not aware of another method to identify all trials, or even all accounts, associated with each organization.\n\nWe could not always determine which trials were associated with specific organizations, and organizations might not know which accounts their investigators use. Organizations could work with ClincalTrials.gov to identify non-working email addresses, update administrators' contact information, assign and identify an administrator responsible for overseeing each account, and create a one-to-one relationship between each account and organization. For example, ClinicalTrials.gov could identify multiple accounts managed by administrators at the same organization and help organizations move information into a single account. Organizations would need to prepare before centralizing their records; centralized administration could reduce trial registration and reporting if administrators lack the time, training, and resources to manage these tasks effectively.",
    "is_useful": true,
    "question": "What are some strategies organizations can implement to improve trial registration and reporting transparency in research?"
  },
  {
    "text": "We could not always determine which trials were associated with specific organizations, and organizations might not know which accounts their investigators use. Organizations could work with ClincalTrials.gov to identify non-working email addresses, update administrators' contact information, assign and identify an administrator responsible for overseeing each account, and create a one-to-one relationship between each account and organization. For example, ClinicalTrials.gov could identify multiple accounts managed by administrators at the same organization and help organizations move information into a single account. Organizations would need to prepare before centralizing their records; centralized administration could reduce trial registration and reporting if administrators lack the time, training, and resources to manage these tasks effectively.\n\nWe requested information from one administrator at each organization, and administrators might have been unaware of policies and practices that affect other parts of their organizations (e.g., IRBs, grant management). Finally, some organizations were misclassified on ClinicalTrials.gov (e.g., non-US organizations); we do not know how many organizations were inadvertently included or excluded because of misclassification.\n\n#### Future research\n\nFurther research is needed to determine how to support trial registration and reporting at different types of organizations. Some large organizations register several trials each week, while other organizations register a few trials each year. For small organizations, hiring staff to support trial registration and reporting could be prohibitively expensive. Further qualitative research could explore how different types of organizations are responding to these requirements.\n\nFuture surveys could examine predictors of compliance with trial registration and reporting requirements.",
    "is_useful": true,
    "question": "What challenges do organizations face in managing trial registration and reporting effectively?"
  },
  {
    "text": "We requested information from one administrator at each organization, and administrators might have been unaware of policies and practices that affect other parts of their organizations (e.g., IRBs, grant management). Finally, some organizations were misclassified on ClinicalTrials.gov (e.g., non-US organizations); we do not know how many organizations were inadvertently included or excluded because of misclassification.\n\n#### Future research\n\nFurther research is needed to determine how to support trial registration and reporting at different types of organizations. Some large organizations register several trials each week, while other organizations register a few trials each year. For small organizations, hiring staff to support trial registration and reporting could be prohibitively expensive. Further qualitative research could explore how different types of organizations are responding to these requirements.\n\nFuture surveys could examine predictors of compliance with trial registration and reporting requirements. Although there are important variations in policy and practice, additional quantitative analyses would have little immediate value because most organizations have low compliance [37\u201345]. Instead, detailed case studies might be most useful for identifying best practices. For example, Duke Medicine developed a centralized approach [55], and the US Department of Veterans Affairs (VA) described multiple efforts to support transparency, including an \"internal web-based portal system\" [54]. The National Clinical Trials Registration and Results Reporting Taskforce is a network of administrators who meet monthly by teleconference, share resources (e.g., educational materials), and provide informal peer education.",
    "is_useful": true,
    "question": "What factors contribute to the challenges organizations face in complying with trial registration and reporting requirements?"
  },
  {
    "text": "Some large organizations register several trials each week, while other organizations register a few trials each year. For small organizations, hiring staff to support trial registration and reporting could be prohibitively expensive. Further qualitative research could explore how different types of organizations are responding to these requirements.\n\nFuture surveys could examine predictors of compliance with trial registration and reporting requirements. Although there are important variations in policy and practice, additional quantitative analyses would have little immediate value because most organizations have low compliance [37\u201345]. Instead, detailed case studies might be most useful for identifying best practices. For example, Duke Medicine developed a centralized approach [55], and the US Department of Veterans Affairs (VA) described multiple efforts to support transparency, including an \"internal web-based portal system\" [54]. The National Clinical Trials Registration and Results Reporting Taskforce is a network of administrators who meet monthly by teleconference, share resources (e.g., educational materials), and provide informal peer education. As industry appears to be doing better than academia [37, 39\u201344], it might be useful for academic organizations to understand the methods industry uses to monitor and report compliance (see, e.g., [56]).\n\nWe surveyed organizations after the publication of The Final Rule, and most accounts completed the survey before The Final Rule took effect, several months before the compliance date [34]. Our results should be considered a \"baseline\" for future studies investigating whether organizations adopt new policies and procedures, and whether they allocate new resources, to fulfill registration and reporting requirements. The federal government estimates compliance costs for organizations will be $70,287,277 per year [34].",
    "is_useful": true,
    "question": "What challenges do different types of organizations face regarding compliance with trial registration and reporting requirements in the context of open science?"
  },
  {
    "text": "The National Clinical Trials Registration and Results Reporting Taskforce is a network of administrators who meet monthly by teleconference, share resources (e.g., educational materials), and provide informal peer education. As industry appears to be doing better than academia [37, 39\u201344], it might be useful for academic organizations to understand the methods industry uses to monitor and report compliance (see, e.g., [56]).\n\nWe surveyed organizations after the publication of The Final Rule, and most accounts completed the survey before The Final Rule took effect, several months before the compliance date [34]. Our results should be considered a \"baseline\" for future studies investigating whether organizations adopt new policies and procedures, and whether they allocate new resources, to fulfill registration and reporting requirements. The federal government estimates compliance costs for organizations will be $70,287,277 per year [34]. This survey, and future updates, could be used to improve estimates of the costs of compliance.\n\n#### Conclusions\n\nTo support clinical trial registration and results reporting, organizations should strongly consider adopting appropriate policies, allocating resources to implement those policies, and ensuring there are consequences for investigators who do not register and report the results of their research.\n\n#### Additional files\n\nAdditional file 1: Survey instrument. (DOCX 500 kb) Additional file 2: Eligible accounts. (DOCX 452 kb) Additional file 3: Participating accounts. (DOCX 476 kb)\n\nAdditional file 4: Additional survey results. (DOCX 440 kb)\n\nAdditional file 5: Sensitivity analysis.",
    "is_useful": true,
    "question": "What measures should organizations adopt to support clinical trial registration and results reporting?"
  },
  {
    "text": "The federal government estimates compliance costs for organizations will be $70,287,277 per year [34]. This survey, and future updates, could be used to improve estimates of the costs of compliance.\n\n#### Conclusions\n\nTo support clinical trial registration and results reporting, organizations should strongly consider adopting appropriate policies, allocating resources to implement those policies, and ensuring there are consequences for investigators who do not register and report the results of their research.\n\n#### Additional files\n\nAdditional file 1: Survey instrument. (DOCX 500 kb) Additional file 2: Eligible accounts. (DOCX 452 kb) Additional file 3: Participating accounts. (DOCX 476 kb)\n\nAdditional file 4: Additional survey results. (DOCX 440 kb)\n\nAdditional file 5: Sensitivity analysis. (DOCX 436 kb)\n\n#### Abbreviations\n\nAPI: Application programming interface; CTSA: Clinical and Translational Science Award; FDA: US Food and Drug Administration; FDAAA: Food and Drug Administration Amendments Act of 2007; HHS: Health and Human Services; ICMJE: International Committee of Medical Journal Editors; IRB: Institutional review board; NCI: National Cancer Institute; NIH: National Institutes of Health; NLM: National Library of Medicine; PRS: Protocol Registration and Results System\n\n#### Acknowledgements\n\nWe thank the National Clinical Trials Registration and Results Reporting Taskforce for feedback throughout this study, including members of the taskforce who pilot tested the draft survey: Deborah Barnard, Jennifer Swanton Brown, Debora Dowlin, Elizabeth M. Gendel, Cassandra Greene, Cindy Han, Elizabeth Jach, Kristin Kolsch, Linda Mendelson, Patricia Mendoza, Michelle Morgan, Sheila Noone, Elizabeth Piantadosi, and Lauren Robertson.",
    "is_useful": true,
    "question": "What are the estimated annual compliance costs for organizations related to clinical trial registration and results reporting?"
  },
  {
    "text": "#### Funding\n\nJH was supported by a Johns Hopkins Center of Excellence in Regulatory Science and Innovation (JH-CERSI) grant from the US Food and Drug Administration (U01 FD004977-01; Caleb Alexander; PI). NA and AK were supported by a Johns Hopkins Institute for Clinical and Translational Research (JH-ICTR) grant from the National Center for Research Resources and the National Center for Advancing Translational Sciences (UL1TR001079; Daniel E. Ford, PI). EMW was supported by JH-CERSI and JH-ICTR. JR and AO were supported by the Yale Center for Analytical Sciences (YCAS) and the Yale Center for Clinical Investigation (YCCI). Harvard Catalyst | The Harvard Clinical and Translational Science Center (UL1 TR001102) supports monthly teleconferences for the National Clinical Trials Registration and Results Reporting Taskforce.\n\nThe funders were not involved in the design or conduct of the study, manuscript preparation, or the decision to submit the manuscript for publication; its contents are solely the responsibility of the authors and do not necessarily represent the official views of HHS or FDA.\n\n#### Availability of data and materials\n\nThe statistical code for generating these results is available from the authors. We did not obtain consent to identify participants; the corresponding author will share individual-level data for research in which participants will not be identified publicly.\n\n#### Authors' contributions\n\nJH, AK, and EMW conceived and designed the study, wrote the study protocol, and obtained institutional review board (IRB) approval.",
    "is_useful": true,
    "question": "What practices are recommended for supporting open science in research studies?"
  },
  {
    "text": "Harvard Catalyst | The Harvard Clinical and Translational Science Center (UL1 TR001102) supports monthly teleconferences for the National Clinical Trials Registration and Results Reporting Taskforce.\n\nThe funders were not involved in the design or conduct of the study, manuscript preparation, or the decision to submit the manuscript for publication; its contents are solely the responsibility of the authors and do not necessarily represent the official views of HHS or FDA.\n\n#### Availability of data and materials\n\nThe statistical code for generating these results is available from the authors. We did not obtain consent to identify participants; the corresponding author will share individual-level data for research in which participants will not be identified publicly.\n\n#### Authors' contributions\n\nJH, AK, and EMW conceived and designed the study, wrote the study protocol, and obtained institutional review board (IRB) approval. Regarding data acquisition, NA, JH, AK, and EMW drafted the survey; all authors and members of the National Clinical Trials Registration and Results Reporting Taskforce Survey Subcommittee provided comments about the content and wording of the survey. JH and EMW distributed the survey. EMW responded to questions from participants. Regarding analysis and interpretation of data, EMW drafted the table shells with NA, JH, AK, AO, and JR. AO and JR analyzed the data. All authors contributed to interpreting the results. EMW wrote the first draft of the manuscript. All authors reviewed, provided critical revisions, and approved the final manuscript for publication. Evan Mayo-Wilson is the guarantor.",
    "is_useful": true,
    "question": "What are the guidelines for data availability and authorship contributions in research studies?"
  },
  {
    "text": "# Timo B. Roettger* Preregistration in experimental linguistics: applications, challenges, and limitations\n\nhttps://doi.org/10.1515/ling-2019-0048 Received December 30, 2019; accepted December 1, 2020; published online March 24, 2021\n\nAbstract: The current publication system neither incentivizes publishing null results nor direct replication attempts, which biases the scientific record toward novel findings that appear to support presented hypotheses (referred to as \"publication bias\"). Moreover, flexibility in data collection, measurement, and analysis (referred to as \"researcher degrees of freedom\") can lead to overconfident beliefs in the robustness of a statistical relationship. One way to systematically decrease publication bias and researcher degrees of freedom is preregistration. A preregistration is a time-stamped document that specifies how data is to be collected, measured, and analyzed prior to data collection. While preregistration is a powerful tool to reduce bias, it comes with certain challenges and limitations which have to be evaluated for each scientific discipline individually. This paper discusses the application, challenges and limitations of preregistration for experimental linguistic research.\n\nKeywords: confirmatory; exploratory; preregistration; publication bias; registered report; researcher degrees of freedom\n\n# 1 Introduction\n\nIn recent coordinated efforts to replicate published findings, the social sciences have uncovered surprisingly low replication rates (e.g., Camerer et al. 2018; Open Science Collaboration 2015). This discovery has led to what is now referred to as the \"replication crisis\" in science.",
    "is_useful": true,
    "question": "What is a method proposed to reduce publication bias and researcher degrees of freedom in scientific research?"
  },
  {
    "text": "One way to systematically decrease publication bias and researcher degrees of freedom is preregistration. A preregistration is a time-stamped document that specifies how data is to be collected, measured, and analyzed prior to data collection. While preregistration is a powerful tool to reduce bias, it comes with certain challenges and limitations which have to be evaluated for each scientific discipline individually. This paper discusses the application, challenges and limitations of preregistration for experimental linguistic research.\n\nKeywords: confirmatory; exploratory; preregistration; publication bias; registered report; researcher degrees of freedom\n\n# 1 Introduction\n\nIn recent coordinated efforts to replicate published findings, the social sciences have uncovered surprisingly low replication rates (e.g., Camerer et al. 2018; Open Science Collaboration 2015). This discovery has led to what is now referred to as the \"replication crisis\" in science. There are raising concerns that a similar state of affairs is true for the field of experimental linguistics because it shares with other disciplines many research practices that have been identified to decrease the replicability of published findings (e.g., Marsden et al. 2018a; Roettger and Baer-Henney 2019; S\u00f6nning and Werner this issue). Moreover, there is already mounting evidence that published experimental findings cannot be taken at face value (e.g., Chen 2007; Nieuwland et al. 2018; Papesh 2015; Stack et al. 2018; Westbury 2018, among many others).",
    "is_useful": true,
    "question": "What is preregistration and how does it relate to reducing publication bias in scientific research?"
  },
  {
    "text": "2018; Open Science Collaboration 2015). This discovery has led to what is now referred to as the \"replication crisis\" in science. There are raising concerns that a similar state of affairs is true for the field of experimental linguistics because it shares with other disciplines many research practices that have been identified to decrease the replicability of published findings (e.g., Marsden et al. 2018a; Roettger and Baer-Henney 2019; S\u00f6nning and Werner this issue). Moreover, there is already mounting evidence that published experimental findings cannot be taken at face value (e.g., Chen 2007; Nieuwland et al. 2018; Papesh 2015; Stack et al. 2018; Westbury 2018, among many others). The present\n\n<sup>*</sup>Corresponding author: Timo B. Roettger, Department of Linguistics and Scandanavian Studies, University of Oslo, Postboks 1102 Blindern, 0317 Oslo, Norway, E-mail: timo.roettger@iln.uio.no\n\nOpen Access. \u00a9 2021 Timo B. Roettger, published by De Gruyter. This work is licensed under the Creative Commons Attribution 4.0 International License.\n\nspecial issue is a welcome and timely attempt to assess the situation in linguistics and to critically discuss ways to improve linguistic research practices.\n\nThe use of the label \"crisis\" in the expression \"replication crisis\" suggests a time of intense difficulty, trouble, or even danger.",
    "is_useful": true,
    "question": "What term describes the current challenges in replicating scientific findings that are affecting various fields, including experimental linguistics?"
  },
  {
    "text": "2018; Papesh 2015; Stack et al. 2018; Westbury 2018, among many others). The present\n\n<sup>*</sup>Corresponding author: Timo B. Roettger, Department of Linguistics and Scandanavian Studies, University of Oslo, Postboks 1102 Blindern, 0317 Oslo, Norway, E-mail: timo.roettger@iln.uio.no\n\nOpen Access. \u00a9 2021 Timo B. Roettger, published by De Gruyter. This work is licensed under the Creative Commons Attribution 4.0 International License.\n\nspecial issue is a welcome and timely attempt to assess the situation in linguistics and to critically discuss ways to improve linguistic research practices.\n\nThe use of the label \"crisis\" in the expression \"replication crisis\" suggests a time of intense difficulty, trouble, or even danger. It might, however, be more fruitful to think of the current situation as an opportunity. Repeated failures to replicate published findings have led to a fruitful discourse across disciplines. Researchers have identified shortcomings in how science is practiced and suggested promising ways forward to increase the transparency, reproducibility, and replicability of scientific work.1 Even within linguistics, an increasing number of researchers have articulated their concerns about present research practices and, importantly, have offered practical advice to circumvent these problems in the future (e.g., Baayen et al. 2017; Berez-Kroeker et al. 2018; Kirby and Sonderegger 2018; Marsden et al.",
    "is_useful": true,
    "question": "What opportunities have arisen from the replication crisis in scientific research, particularly in linguistics?"
  },
  {
    "text": "special issue is a welcome and timely attempt to assess the situation in linguistics and to critically discuss ways to improve linguistic research practices.\n\nThe use of the label \"crisis\" in the expression \"replication crisis\" suggests a time of intense difficulty, trouble, or even danger. It might, however, be more fruitful to think of the current situation as an opportunity. Repeated failures to replicate published findings have led to a fruitful discourse across disciplines. Researchers have identified shortcomings in how science is practiced and suggested promising ways forward to increase the transparency, reproducibility, and replicability of scientific work.1 Even within linguistics, an increasing number of researchers have articulated their concerns about present research practices and, importantly, have offered practical advice to circumvent these problems in the future (e.g., Baayen et al. 2017; Berez-Kroeker et al. 2018; Kirby and Sonderegger 2018; Marsden et al. 2018a; Roettger 2019; Vasishth et al. 2018; Wieling et al. 2018; Winter 2011). In the same spirit, the present paper discusses a concept that marks a promising way forward in increasing the replicability of experimental linguistic research: preregistration.\n\nA preregistration is a time-stamped document in which researchers specify prior to data collection how they plan to collect their data and/or how they plan to conduct the data analyses. In the following, I will argue that preregistration helps drawing a line between exploratory and confirmatory research.",
    "is_useful": true,
    "question": "What are some promising ways proposed to improve transparency and replicability in scientific research?"
  },
  {
    "text": "2017; Berez-Kroeker et al. 2018; Kirby and Sonderegger 2018; Marsden et al. 2018a; Roettger 2019; Vasishth et al. 2018; Wieling et al. 2018; Winter 2011). In the same spirit, the present paper discusses a concept that marks a promising way forward in increasing the replicability of experimental linguistic research: preregistration.\n\nA preregistration is a time-stamped document in which researchers specify prior to data collection how they plan to collect their data and/or how they plan to conduct the data analyses. In the following, I will argue that preregistration helps drawing a line between exploratory and confirmatory research. It also allows transparently tracking analytical flexibility and counteracting publication bias. Many authors have discussed the concept of preregistration across disciplines before (e.g., Nosek and Lakens 2014; Wagenmakers et al. 2012) and there are relevant discussions within the language sciences for second language research (Marsden et al. 2018b; Morgan\u2010Short et al. 2018) and language acquisition research (Havron et al. 2020). However, I think it is worth to reiterate applications, challenges, and limitations of preregistration for experimental linguistics at large.",
    "is_useful": true,
    "question": "What is the role of preregistration in enhancing the replicability of experimental research?"
  },
  {
    "text": "A preregistration is a time-stamped document in which researchers specify prior to data collection how they plan to collect their data and/or how they plan to conduct the data analyses. In the following, I will argue that preregistration helps drawing a line between exploratory and confirmatory research. It also allows transparently tracking analytical flexibility and counteracting publication bias. Many authors have discussed the concept of preregistration across disciplines before (e.g., Nosek and Lakens 2014; Wagenmakers et al. 2012) and there are relevant discussions within the language sciences for second language research (Marsden et al. 2018b; Morgan\u2010Short et al. 2018) and language acquisition research (Havron et al. 2020). However, I think it is worth to reiterate applications, challenges, and limitations of preregistration for experimental linguistics at large.\n\n## 2 The problem: biases we live by\n\nIn the following, I will give a brief overview of relevant problems that may affect the replicability of published research and discuss how some of these problems can be\n\n<sup>1</sup> The terms reproducible research and replication are used ambiguously in the literature.",
    "is_useful": true,
    "question": "What is the purpose of preregistration in research, particularly in relation to exploratory and confirmatory studies?"
  },
  {
    "text": "It also allows transparently tracking analytical flexibility and counteracting publication bias. Many authors have discussed the concept of preregistration across disciplines before (e.g., Nosek and Lakens 2014; Wagenmakers et al. 2012) and there are relevant discussions within the language sciences for second language research (Marsden et al. 2018b; Morgan\u2010Short et al. 2018) and language acquisition research (Havron et al. 2020). However, I think it is worth to reiterate applications, challenges, and limitations of preregistration for experimental linguistics at large.\n\n## 2 The problem: biases we live by\n\nIn the following, I will give a brief overview of relevant problems that may affect the replicability of published research and discuss how some of these problems can be\n\n<sup>1</sup> The terms reproducible research and replication are used ambiguously in the literature. Here I follow Claerbout and Karrenbach (1991) and refer to reproducible research as research in which \"authors provide all the necessary data and the computer codes to run the analysis again, recreating the results\" and a replication as a \"study that arrives at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.\" See Barba (2018) for a review of the usage of these terms.\n\ntackled by preregistration.",
    "is_useful": true,
    "question": "What role does preregistration play in addressing issues of analytical flexibility and publication bias in research?"
  },
  {
    "text": "However, I think it is worth to reiterate applications, challenges, and limitations of preregistration for experimental linguistics at large.\n\n## 2 The problem: biases we live by\n\nIn the following, I will give a brief overview of relevant problems that may affect the replicability of published research and discuss how some of these problems can be\n\n<sup>1</sup> The terms reproducible research and replication are used ambiguously in the literature. Here I follow Claerbout and Karrenbach (1991) and refer to reproducible research as research in which \"authors provide all the necessary data and the computer codes to run the analysis again, recreating the results\" and a replication as a \"study that arrives at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.\" See Barba (2018) for a review of the usage of these terms.\n\ntackled by preregistration. In what follows I will assume a particular modus of scientific inquiry in which researchers accumulate knowledge about nature and society by formulating falsifiable hypotheses and test these hypotheses on observable data for which the outcome is unknown. This confirmatory mode of scientific inquiry is particularly common in experimental subfields of linguistics. Many other subfields in linguistics are inherently observational (see Grieve this issue) and thus the proposed dichotomy between exploratory and confirmatory research as well as the concept of preregistration might not similarly apply across the language sciences. We will come back to the value of preregistration for observational studies below.",
    "is_useful": true,
    "question": "What is the importance of preregistration in addressing biases and enhancing the replicability of research in experimental linguistics?"
  },
  {
    "text": "See Barba (2018) for a review of the usage of these terms.\n\ntackled by preregistration. In what follows I will assume a particular modus of scientific inquiry in which researchers accumulate knowledge about nature and society by formulating falsifiable hypotheses and test these hypotheses on observable data for which the outcome is unknown. This confirmatory mode of scientific inquiry is particularly common in experimental subfields of linguistics. Many other subfields in linguistics are inherently observational (see Grieve this issue) and thus the proposed dichotomy between exploratory and confirmatory research as well as the concept of preregistration might not similarly apply across the language sciences. We will come back to the value of preregistration for observational studies below.\n\n#### 2.1 Exploratory and confirmatory research\n\nLinguists who work empirically generally collect data (make observations) to understand aspects of human language, such as how it is comprehended, how it is produced, how it is acquired, or how it has evolved. In linguistics, this can involve the analysis of corpora, the analysis of crosslinguistic databases, or the analysis of experiments, and so forth. The observations are then used to formulate empirical models that capture what has been observed (e.g., Lehmann 1990). In most experimental fields, the model development is usually informed by two phases of research: exploratory and confirmatory research (e.g., Box 1976; de Groot 2014 [1956]; Nicenboim et al. 2018b; Roettger et al.",
    "is_useful": true,
    "question": "What are the two phases of research commonly involved in empirical scientific inquiry, especially in experimental fields?"
  },
  {
    "text": "We will come back to the value of preregistration for observational studies below.\n\n#### 2.1 Exploratory and confirmatory research\n\nLinguists who work empirically generally collect data (make observations) to understand aspects of human language, such as how it is comprehended, how it is produced, how it is acquired, or how it has evolved. In linguistics, this can involve the analysis of corpora, the analysis of crosslinguistic databases, or the analysis of experiments, and so forth. The observations are then used to formulate empirical models that capture what has been observed (e.g., Lehmann 1990). In most experimental fields, the model development is usually informed by two phases of research: exploratory and confirmatory research (e.g., Box 1976; de Groot 2014 [1956]; Nicenboim et al. 2018b; Roettger et al. 2019; Tukey 1977): Researchers explore patterns and relationships in their observations. Based on these observed patterns, they reason about plausible processes or mechanisms that could have given rise to these patterns in the data. They then formulate hypotheses as to how nature will behave in certain situations that they have not been observed yet. These hypotheses can then be tested on new data in an attempt to confirm2 the empirical predictions.\n\nExploratory and confirmatory research are both vital components of scientific progress. Exploration has led to many breakthroughs in science.",
    "is_useful": true,
    "question": "What are the two vital components of scientific progress that involve the process of observing data and formulating hypotheses?"
  },
  {
    "text": "In most experimental fields, the model development is usually informed by two phases of research: exploratory and confirmatory research (e.g., Box 1976; de Groot 2014 [1956]; Nicenboim et al. 2018b; Roettger et al. 2019; Tukey 1977): Researchers explore patterns and relationships in their observations. Based on these observed patterns, they reason about plausible processes or mechanisms that could have given rise to these patterns in the data. They then formulate hypotheses as to how nature will behave in certain situations that they have not been observed yet. These hypotheses can then be tested on new data in an attempt to confirm2 the empirical predictions.\n\nExploratory and confirmatory research are both vital components of scientific progress. Exploration has led to many breakthroughs in science. A linguistic example is the McGurk effect, i.e., perceiving a sound that lies in-between an auditorily presented component of one sound and a visually presented component of another one (McGurk and MacDonald 1976). This effect was not predicted\n\n<sup>2</sup> The term \"confirmation\" in this context refers to statements about statistical hypotheses. Following commonly held views in the philosophy of science (e.g., Popper 1963), we cannot confirmscientific hypotheses (we can only falsify them). While statistical hypotheses and empirical models can be corroborated by data, it must also be borne in mind that this interpretation is always conditional on the statistical model, i.e., the validity of the assumptions specified by the inferential procedure.",
    "is_useful": true,
    "question": "What are the two phases of research that inform model development in most experimental fields?"
  },
  {
    "text": "Exploratory and confirmatory research are both vital components of scientific progress. Exploration has led to many breakthroughs in science. A linguistic example is the McGurk effect, i.e., perceiving a sound that lies in-between an auditorily presented component of one sound and a visually presented component of another one (McGurk and MacDonald 1976). This effect was not predicted\n\n<sup>2</sup> The term \"confirmation\" in this context refers to statements about statistical hypotheses. Following commonly held views in the philosophy of science (e.g., Popper 1963), we cannot confirmscientific hypotheses (we can only falsify them). While statistical hypotheses and empirical models can be corroborated by data, it must also be borne in mind that this interpretation is always conditional on the statistical model, i.e., the validity of the assumptions specified by the inferential procedure.\n\na priori, it was accidentally discovered (Massaro and Stork 1998) and led to predictions that since then have been confirmed on new observations (Alsius et al. 2018). Putting hypotheses under targeted scrutiny via confirmatory tests enables the accumulation of evidence in order to challenge, support, and refine scientific models.\n\nThe distinction between confirmatory and exploratory research is tremendously important. Given the complexity of the phenomena investigated in linguistics, every set of observations offers a myriad of ways to look at it and contains spurious relationships and patterns.",
    "is_useful": true,
    "question": "What are the roles of exploratory and confirmatory research in scientific progress?"
  },
  {
    "text": "Following commonly held views in the philosophy of science (e.g., Popper 1963), we cannot confirmscientific hypotheses (we can only falsify them). While statistical hypotheses and empirical models can be corroborated by data, it must also be borne in mind that this interpretation is always conditional on the statistical model, i.e., the validity of the assumptions specified by the inferential procedure.\n\na priori, it was accidentally discovered (Massaro and Stork 1998) and led to predictions that since then have been confirmed on new observations (Alsius et al. 2018). Putting hypotheses under targeted scrutiny via confirmatory tests enables the accumulation of evidence in order to challenge, support, and refine scientific models.\n\nThe distinction between confirmatory and exploratory research is tremendously important. Given the complexity of the phenomena investigated in linguistics, every set of observations offers a myriad of ways to look at it and contains spurious relationships and patterns. Chance and sampling error alone will produce what might look like a meaningful pattern (e.g., Kirby and Sonderegger 2018; Nicenboim et al. 2018a; Winter 2011). If these exploratory observations are treated as they were predicted a priori, one might overconfidently believe in the robustness of a relationship that will not stand the test of time (e.g., Gelman and Loken 2013; Roettger 2019; Simmons et al. 2011).",
    "is_useful": true,
    "question": "What is the importance of distinguishing between confirmatory and exploratory research in the context of scientific hypotheses?"
  },
  {
    "text": "2018). Putting hypotheses under targeted scrutiny via confirmatory tests enables the accumulation of evidence in order to challenge, support, and refine scientific models.\n\nThe distinction between confirmatory and exploratory research is tremendously important. Given the complexity of the phenomena investigated in linguistics, every set of observations offers a myriad of ways to look at it and contains spurious relationships and patterns. Chance and sampling error alone will produce what might look like a meaningful pattern (e.g., Kirby and Sonderegger 2018; Nicenboim et al. 2018a; Winter 2011). If these exploratory observations are treated as they were predicted a priori, one might overconfidently believe in the robustness of a relationship that will not stand the test of time (e.g., Gelman and Loken 2013; Roettger 2019; Simmons et al. 2011).\n\n#### 2.2 To err is human\n\nResearchers are human and humans have evolved to filter the world in irrational ways (e.g., Tversky and Kahneman 1974), blurring the line between exploration and confirmation (Nuzzo 2015). For example, humans see coherent patterns in randomness (Brugger 2001), they convince themselves of the validity of prior expectations (\"I knew it\", Nickerson 1998), and they perceive events as being plausible in hindsight (\"I knew it all along\", Fischhoff 1975).",
    "is_useful": true,
    "question": "What is the significance of distinguishing between confirmatory and exploratory research in scientific investigations?"
  },
  {
    "text": "2018a; Winter 2011). If these exploratory observations are treated as they were predicted a priori, one might overconfidently believe in the robustness of a relationship that will not stand the test of time (e.g., Gelman and Loken 2013; Roettger 2019; Simmons et al. 2011).\n\n#### 2.2 To err is human\n\nResearchers are human and humans have evolved to filter the world in irrational ways (e.g., Tversky and Kahneman 1974), blurring the line between exploration and confirmation (Nuzzo 2015). For example, humans see coherent patterns in randomness (Brugger 2001), they convince themselves of the validity of prior expectations (\"I knew it\", Nickerson 1998), and they perceive events as being plausible in hindsight (\"I knew it all along\", Fischhoff 1975). When hindsight bias comes into play, researchers tend to generate explanations based on observations, and, at the same time, believe that they would have anticipated this very explanation before observing the data. For example, a researcher might predict that focused constituents in a language under investigation are prosodically marked. As is, this is a vague prediction, as prosodic marking can be operationalized in different ways (e.g., Gordon and Roettger 2017). After combing through a data set of speech and measuring several plausible acoustic dimensions, the researcher discovers that the average word duration of focused words is greater than that of unfocused words.",
    "is_useful": true,
    "question": "What cognitive biases can impact the reliability of research interpretations in open science?"
  },
  {
    "text": "After combing through a data set of speech and measuring several plausible acoustic dimensions, the researcher discovers that the average word duration of focused words is greater than that of unfocused words. After this discovery, the researcher identifies duration, out of all acoustic dimensions that have been measured (and that could have been measured), as the one most relevant for testing the prediction. Crucially, the researcher does not mention those dimensions that did not show a systematic relationship (Roettger 2019). The researcher generates and tests predictions on the same data set.\n\nConsider another example: a group of psycholinguists is convinced that a certain syntactic structure leads to processing difficulties. These processing difficulties should be reflected in language users' reading times. The researchers run an eye-tracking experiment but do not find a significant difference in reading times. Convinced that they have overlooked something, they also measure alternative behavioral indices related to different visual fields (foveal, parafoveal, peripheral), related to different fixations of regions (e.g., first, second, third), and the duration of a fixation before exiting/entering a particular region (see von der Malsburg and Angele 2017 for a discussion of these researcher degrees of freedom in eye tracking). After several analyses of the data, a significant effect of one of these measures materializes. In hindsight, it strikes the researchers as particularly obvious that this measure was the one that shows the clearest effect and when writing up the paper, they frame it as if this linking hypothesis had been spelled out prior to data collection.",
    "is_useful": true,
    "question": "What effect does focusing on specific acoustic dimensions have on the analysis of speech data in open science research?"
  },
  {
    "text": "After several analyses of the data, a significant effect of one of these measures materializes. In hindsight, it strikes the researchers as particularly obvious that this measure was the one that shows the clearest effect and when writing up the paper, they frame it as if this linking hypothesis had been spelled out prior to data collection.\n\nThis after-the-fact reasoning is particularly tempting in linguistic research. Some languages such as English are particularly well investigated. Many other languages are either heavily underdocumented or not documented at all. It is all too easy to assume that grammatical functions, linguistic phenomena, or communication patterns that are relevant for the handful of well-investigated languages can be found in other languages, too (e.g., Ameke 2006; Bender 2011; Gil 2001; Goddard and Wierzbicka 2014; Levisen 2018; Wierzbicka 2009). For example, it has been shown that focused constituents are often phonetically longer in English (e.g., Cooper et al. 1985). Researchers' prior belief in how the next language manifests focus might be biased by their preconceptions about the languages (Majid and Levinson 2010) and cultures (Henrich et al. 2010) with which they are most familiar.\n\n#### 2.3 Incentivizing confirmation over exploration\n\nBiases such as confirmation or hindsight bias are further amplified by the academic ecosystem. When it comes to publishing experimental work, exploration and confirmation are not weighted equally.",
    "is_useful": true,
    "question": "What challenges do researchers face in linguistic studies due to biases and the state of language documentation?"
  },
  {
    "text": "For example, it has been shown that focused constituents are often phonetically longer in English (e.g., Cooper et al. 1985). Researchers' prior belief in how the next language manifests focus might be biased by their preconceptions about the languages (Majid and Levinson 2010) and cultures (Henrich et al. 2010) with which they are most familiar.\n\n#### 2.3 Incentivizing confirmation over exploration\n\nBiases such as confirmation or hindsight bias are further amplified by the academic ecosystem. When it comes to publishing experimental work, exploration and confirmation are not weighted equally. Confirmatory analyses have a superior status within the academic incentive system, determining the way funding agencies assess proposals, and shaping how researchers frame their papers (Sterling 1959). In an incentive system in which high impact publications are the dominant currency to secure jobs and funding, the results of what has actually been an exploratory analysis are often presented as if they were the results of a confirmatory analysis (Simmons et al. 2011). Whether done intentionally or not, this reframing of results adds to the publishability of the proposed findings.\n\nWithin the confirmatory framework, findings that statistically support predictions are considered more valuable than null results. The lack of incentives for publishing null results or direct replication attempts biases the scientific record toward novel positive findings. For example, Marsden et al. (2018a) investigated the prevalence of replication studies across second language research. They found a low replication rate, corresponding to only one direct replication in every 400 articles.",
    "is_useful": true,
    "question": "What biases affect the publication and funding processes in academic research, particularly regarding confirmatory versus exploratory analyses?"
  },
  {
    "text": "Confirmatory analyses have a superior status within the academic incentive system, determining the way funding agencies assess proposals, and shaping how researchers frame their papers (Sterling 1959). In an incentive system in which high impact publications are the dominant currency to secure jobs and funding, the results of what has actually been an exploratory analysis are often presented as if they were the results of a confirmatory analysis (Simmons et al. 2011). Whether done intentionally or not, this reframing of results adds to the publishability of the proposed findings.\n\nWithin the confirmatory framework, findings that statistically support predictions are considered more valuable than null results. The lack of incentives for publishing null results or direct replication attempts biases the scientific record toward novel positive findings. For example, Marsden et al. (2018a) investigated the prevalence of replication studies across second language research. They found a low replication rate, corresponding to only one direct replication in every 400 articles. Replication studies were on average conducted after more than six years and over a hundred citations of the original study. Thus, replications are either only performed after the original study had already impacted the field substantially or only then published if the original study was impactful.\n\nThis leads to a pervasive asymmetry with a large number of null results and direct replication attempts not entering the scientific record (\"publication bias\", e.g., Fanelli [2012]; Franco et al. [2014]; Sterling [1959], see also the \"significance filter\", Vasishth et al. [2018]).",
    "is_useful": true,
    "question": "What are the implications of the academic incentive system on the publication of confirmatory versus null results in scientific research?"
  },
  {
    "text": "The lack of incentives for publishing null results or direct replication attempts biases the scientific record toward novel positive findings. For example, Marsden et al. (2018a) investigated the prevalence of replication studies across second language research. They found a low replication rate, corresponding to only one direct replication in every 400 articles. Replication studies were on average conducted after more than six years and over a hundred citations of the original study. Thus, replications are either only performed after the original study had already impacted the field substantially or only then published if the original study was impactful.\n\nThis leads to a pervasive asymmetry with a large number of null results and direct replication attempts not entering the scientific record (\"publication bias\", e.g., Fanelli [2012]; Franco et al. [2014]; Sterling [1959], see also the \"significance filter\", Vasishth et al. [2018]). For example, Fanelli (2012) analyzed over 4,600 papers published across disciplines, estimated the frequency of papers that, having declared to have \"tested\" a hypothesis, reported support for it. On average, 80% of tested hypotheses were found to be confirmed based on conventional statistical standards.\n\nWhat advances experimental researchers' careers and helps them obtain funding are statistically supported predictions, not null results. The prevalent expectation that the main results of a study should be predicted based on a priori grounds is one of the factors that have led to research practices that are inhibiting scientific progress (John et al. 2012). These questionable practices are connected to the statistical tools that are used.",
    "is_useful": true,
    "question": "How do publication biases affect the representation of replication studies and null results in scientific literature?"
  },
  {
    "text": "[2014]; Sterling [1959], see also the \"significance filter\", Vasishth et al. [2018]). For example, Fanelli (2012) analyzed over 4,600 papers published across disciplines, estimated the frequency of papers that, having declared to have \"tested\" a hypothesis, reported support for it. On average, 80% of tested hypotheses were found to be confirmed based on conventional statistical standards.\n\nWhat advances experimental researchers' careers and helps them obtain funding are statistically supported predictions, not null results. The prevalent expectation that the main results of a study should be predicted based on a priori grounds is one of the factors that have led to research practices that are inhibiting scientific progress (John et al. 2012). These questionable practices are connected to the statistical tools that are used. In most scientific papers, statistical inference is drawn by means of null hypothesis significance testing (NHST), a procedure to evaluate prediction and test hypotheses (Gigerenzer et al. 2004; Lindquist 1940). In NHST, the probability of observing a result at least as extreme as a test statistic (e.g., t-value) is computed, assuming that the null hypothesis is true (the p-value). Receiving a p-value below a certain threshold (commonly 0.05) leads to a categorical decision about the incompatibility of the data with the null hypothesis.",
    "is_useful": true,
    "question": "What statistical method is commonly used in scientific research to evaluate predictions and test hypotheses?"
  },
  {
    "text": "The prevalent expectation that the main results of a study should be predicted based on a priori grounds is one of the factors that have led to research practices that are inhibiting scientific progress (John et al. 2012). These questionable practices are connected to the statistical tools that are used. In most scientific papers, statistical inference is drawn by means of null hypothesis significance testing (NHST), a procedure to evaluate prediction and test hypotheses (Gigerenzer et al. 2004; Lindquist 1940). In NHST, the probability of observing a result at least as extreme as a test statistic (e.g., t-value) is computed, assuming that the null hypothesis is true (the p-value). Receiving a p-value below a certain threshold (commonly 0.05) leads to a categorical decision about the incompatibility of the data with the null hypothesis.\n\nWithin the NHST framework, any pattern that yields a p-value below 0.05 is, in practice at least, considered sufficient to reject the null hypothesis and claim that there is an effect. However, if the p-value is 0.05 and the null is actually true, there is a 5% probability that the data accidentally suggests that the null hypothesis can be refuted (a false positive, otherwise known as Type I error). If one only performs one test and follows only one way to conduct that test, then the p-value is diagnostic about its intended probability. However, the diagnostic probability of the p-value changes as soon as one performs more than one analysis (Benjamini and Hochberg 1995).",
    "is_useful": true,
    "question": "What research practices are inhibiting scientific progress in relation to statistical tools used in studies?"
  },
  {
    "text": "Receiving a p-value below a certain threshold (commonly 0.05) leads to a categorical decision about the incompatibility of the data with the null hypothesis.\n\nWithin the NHST framework, any pattern that yields a p-value below 0.05 is, in practice at least, considered sufficient to reject the null hypothesis and claim that there is an effect. However, if the p-value is 0.05 and the null is actually true, there is a 5% probability that the data accidentally suggests that the null hypothesis can be refuted (a false positive, otherwise known as Type I error). If one only performs one test and follows only one way to conduct that test, then the p-value is diagnostic about its intended probability. However, the diagnostic probability of the p-value changes as soon as one performs more than one analysis (Benjamini and Hochberg 1995).\n\nThere are usually many decisions researchers must make when analyzing data. For example, they have to choose how to measure a desired phenomenon and they have to decide whether any observation has to be excluded and what predictors to include in their analysis (see Roettger [2019] for an in-depth discussion for experimental phonetics). These decisions during the analysis procedure have been referred to as \"researcher degrees of freedom\" (Simmons et al. [2011]; see also Gelman and Loken's [2013] garden of forking paths). If data-analytic flexibility is exploited during analysis, that is after observing the data, hindsight and confirmation biases can creep in and affect how researchers make their decisions.",
    "is_useful": true,
    "question": "What statistical issues can arise from the flexibility in data analysis procedures within the framework of hypothesis testing?"
  },
  {
    "text": "However, the diagnostic probability of the p-value changes as soon as one performs more than one analysis (Benjamini and Hochberg 1995).\n\nThere are usually many decisions researchers must make when analyzing data. For example, they have to choose how to measure a desired phenomenon and they have to decide whether any observation has to be excluded and what predictors to include in their analysis (see Roettger [2019] for an in-depth discussion for experimental phonetics). These decisions during the analysis procedure have been referred to as \"researcher degrees of freedom\" (Simmons et al. [2011]; see also Gelman and Loken's [2013] garden of forking paths). If data-analytic flexibility is exploited during analysis, that is after observing the data, hindsight and confirmation biases can creep in and affect how researchers make their decisions. Researchers often explore many ways of analyzing the data, for all of which they have good reasons. However, the diagnostic nature of the p-value changes dramatically when doing so (Gelman and Loken 2013; Simmons et al. 2011). Two often-discussed instances of this problem are HARKing (Hypothesizing After Results are Known, e.g., Kerr 1998) and selective reporting (Simmons et al. 2011, also referred to as p-hacking). Researchers HARK when they present relationships that have been obtained after data collection as if they were hypothesized in advance, i.e., they reframe exploratory indications as confirmatory results.",
    "is_useful": true,
    "question": "What issues can arise in data analysis due to flexible decision-making by researchers?"
  },
  {
    "text": "[2011]; see also Gelman and Loken's [2013] garden of forking paths). If data-analytic flexibility is exploited during analysis, that is after observing the data, hindsight and confirmation biases can creep in and affect how researchers make their decisions. Researchers often explore many ways of analyzing the data, for all of which they have good reasons. However, the diagnostic nature of the p-value changes dramatically when doing so (Gelman and Loken 2013; Simmons et al. 2011). Two often-discussed instances of this problem are HARKing (Hypothesizing After Results are Known, e.g., Kerr 1998) and selective reporting (Simmons et al. 2011, also referred to as p-hacking). Researchers HARK when they present relationships that have been obtained after data collection as if they were hypothesized in advance, i.e., they reframe exploratory indications as confirmatory results. Researchers selectively report when they explore different analytical options until significant results are found, i.e., different possible data analytical decisions are all explored and the one data analytical path that yields the desired outcome is ultimately reported (while the others are not). The consequence of this (often unintentional) behavior is an inflation of false positives in the literature. Left undetected, false positives can lead to theoretical claims that may misguide future research (Smaldino and McElreath 2016).",
    "is_useful": true,
    "question": "What are the potential consequences of exploiting data-analytic flexibility in research?"
  },
  {
    "text": "2011). Two often-discussed instances of this problem are HARKing (Hypothesizing After Results are Known, e.g., Kerr 1998) and selective reporting (Simmons et al. 2011, also referred to as p-hacking). Researchers HARK when they present relationships that have been obtained after data collection as if they were hypothesized in advance, i.e., they reframe exploratory indications as confirmatory results. Researchers selectively report when they explore different analytical options until significant results are found, i.e., different possible data analytical decisions are all explored and the one data analytical path that yields the desired outcome is ultimately reported (while the others are not). The consequence of this (often unintentional) behavior is an inflation of false positives in the literature. Left undetected, false positives can lead to theoretical claims that may misguide future research (Smaldino and McElreath 2016).\n\nIn light of the outlined interaction between cognitive biases, statistical procedures, and the current incentive structure, it is important to cultivate and institutionalize a clear line between exploration and confirmation for experimental research. One tool to achieve this goal is preregistration.\n\n## 3 A solution: preregistration\n\nA preregistration is a time-stamped document in which researchers specify how they plan to collect their data and/or how they plan to conduct their confirmatory analysis (e.g., Nosek and Lakens 2014; Wagenmakers et al. 2012; see Havron et al. 2020; Marsden et al.",
    "is_useful": true,
    "question": "What are common practices in research that can lead to the inflation of false positives in scientific literature?"
  },
  {
    "text": "The consequence of this (often unintentional) behavior is an inflation of false positives in the literature. Left undetected, false positives can lead to theoretical claims that may misguide future research (Smaldino and McElreath 2016).\n\nIn light of the outlined interaction between cognitive biases, statistical procedures, and the current incentive structure, it is important to cultivate and institutionalize a clear line between exploration and confirmation for experimental research. One tool to achieve this goal is preregistration.\n\n## 3 A solution: preregistration\n\nA preregistration is a time-stamped document in which researchers specify how they plan to collect their data and/or how they plan to conduct their confirmatory analysis (e.g., Nosek and Lakens 2014; Wagenmakers et al. 2012; see Havron et al. 2020; Marsden et al. 2018b; Morgan\u2010Short et al. 2018 for language-related discussions).\n\nPreregistrations can differ with regard to how detailed they are, ranging from basic descriptions of the study design to very detailed descriptions of the procedure and statistical analysis. In the most transparent version of a preregistration, all relevant materials, experimental protocols, and statistical procedures are published alongside the preregistration prior to data collection.\n\nPreregistration draws a clear line between exploratory and confirmatory parts of a study. By doing so, it reduces researcher degrees of freedom because the conduct of a study commits to certain decisions prior to observing data.",
    "is_useful": true,
    "question": "What is a tool that researchers can use to distinguish between exploratory and confirmatory research, thereby reducing false positives in scientific literature?"
  },
  {
    "text": "2012; see Havron et al. 2020; Marsden et al. 2018b; Morgan\u2010Short et al. 2018 for language-related discussions).\n\nPreregistrations can differ with regard to how detailed they are, ranging from basic descriptions of the study design to very detailed descriptions of the procedure and statistical analysis. In the most transparent version of a preregistration, all relevant materials, experimental protocols, and statistical procedures are published alongside the preregistration prior to data collection.\n\nPreregistration draws a clear line between exploratory and confirmatory parts of a study. By doing so, it reduces researcher degrees of freedom because the conduct of a study commits to certain decisions prior to observing data. Additionally, public preregistration can help to reduce publication bias, as the number of failed attempts to reject a hypothesis can be tracked transparently.\n\nThe concept of preregistration is not new. A form of preregistration has been mandatory for clinical trials funded by the US government since 2000. Since 2005, preregistration is a precondition for publishing clinical trials in most medical journals (DeAngelis et al. 2005). In a research climate that can be characterized by an increased interest in openness, transparency and reproducibility, preregistration has become more and more common for experimental research outside of the medical field.",
    "is_useful": true,
    "question": "What are the benefits of preregistration in research, particularly regarding transparency and bias reduction?"
  },
  {
    "text": "Preregistration draws a clear line between exploratory and confirmatory parts of a study. By doing so, it reduces researcher degrees of freedom because the conduct of a study commits to certain decisions prior to observing data. Additionally, public preregistration can help to reduce publication bias, as the number of failed attempts to reject a hypothesis can be tracked transparently.\n\nThe concept of preregistration is not new. A form of preregistration has been mandatory for clinical trials funded by the US government since 2000. Since 2005, preregistration is a precondition for publishing clinical trials in most medical journals (DeAngelis et al. 2005). In a research climate that can be characterized by an increased interest in openness, transparency and reproducibility, preregistration has become more and more common for experimental research outside of the medical field. On the Open Science Framework (osf.io), one of the most widely used preregistration platforms, there is an exponential growth of preregistrations (Nosek and Lindsay 2018), a trend that has not yet carried over to the field of experimental linguistics.\n\nWhen writing a preregistration, the researcher should keep a skeptical reader in mind. The goal of the preregistration is to reassure the skeptic that all necessary decisions have been planned in advance. Ideally, the families of questions in Figure 1 should be addressed in sufficiently specific ways (see also Wicherts et al. 2016 for a detailed list of relevant researcher degrees of freedom):\n\nWhat does \"sufficiently specific\" mean?",
    "is_useful": true,
    "question": "What is the purpose of preregistration in research studies?"
  },
  {
    "text": "2005). In a research climate that can be characterized by an increased interest in openness, transparency and reproducibility, preregistration has become more and more common for experimental research outside of the medical field. On the Open Science Framework (osf.io), one of the most widely used preregistration platforms, there is an exponential growth of preregistrations (Nosek and Lindsay 2018), a trend that has not yet carried over to the field of experimental linguistics.\n\nWhen writing a preregistration, the researcher should keep a skeptical reader in mind. The goal of the preregistration is to reassure the skeptic that all necessary decisions have been planned in advance. Ideally, the families of questions in Figure 1 should be addressed in sufficiently specific ways (see also Wicherts et al. 2016 for a detailed list of relevant researcher degrees of freedom):\n\nWhat does \"sufficiently specific\" mean? For example, excluding participants \"because they were distracted\" is not specific enough because it leaves room for interpretation. Instead, one should operationalize what is meant by being \"distracted\", e.g., incorrectly answering more than 40% of prespecified comprehension questions. Claiming to analyze the data with linear mixed effects models is not sufficient either as there are many moving parts that can influence the results.",
    "is_useful": true,
    "question": "What is the purpose of preregistration in experimental research?"
  },
  {
    "text": "When writing a preregistration, the researcher should keep a skeptical reader in mind. The goal of the preregistration is to reassure the skeptic that all necessary decisions have been planned in advance. Ideally, the families of questions in Figure 1 should be addressed in sufficiently specific ways (see also Wicherts et al. 2016 for a detailed list of relevant researcher degrees of freedom):\n\nWhat does \"sufficiently specific\" mean? For example, excluding participants \"because they were distracted\" is not specific enough because it leaves room for interpretation. Instead, one should operationalize what is meant by being \"distracted\", e.g., incorrectly answering more than 40% of prespecified comprehension questions. Claiming to analyze the data with linear mixed effects models is not sufficient either as there are many moving parts that can influence the results. Instead, one should specify the model structure (including at least a description of the model formula), the inferential procedure (e.g., are hypotheses evaluated by model comparison, by null-hypothesis significance testing, by Bayesian parameter estimation), and the inferential criterion (e.g., when are statistical hypotheses claimed to be confirmed?). Ideally, one publishes the analysis script with the preregistration to leave no ambiguity as to the data analysis pipeline (see the discussion in Section 4.2 on dealing with data-contingent decisions during analysis).\n\nThere are several websites that offer services to preregister studies: two of the most discussed platforms are AsPredicted (AsPredicted.org) and the preregistration forms on the Open Science Framework (osf.io).",
    "is_useful": true,
    "question": "What is the purpose of preregistration in research and what should it ideally include?"
  },
  {
    "text": "Claiming to analyze the data with linear mixed effects models is not sufficient either as there are many moving parts that can influence the results. Instead, one should specify the model structure (including at least a description of the model formula), the inferential procedure (e.g., are hypotheses evaluated by model comparison, by null-hypothesis significance testing, by Bayesian parameter estimation), and the inferential criterion (e.g., when are statistical hypotheses claimed to be confirmed?). Ideally, one publishes the analysis script with the preregistration to leave no ambiguity as to the data analysis pipeline (see the discussion in Section 4.2 on dealing with data-contingent decisions during analysis).\n\nThere are several websites that offer services to preregister studies: two of the most discussed platforms are AsPredicted (AsPredicted.org) and the preregistration forms on the Open Science Framework (osf.io). These platforms afford time-logged reports and either make them publicly available or grant anonymous access only to a specific group of people (such as reviewers and editors during the peer-review process). AsPredicted.org is a rather slim version of what is necessary for a preregistration. One author on the research team simply answers nine questions about the\n\n![](_page_8_Picture_2.jpeg)\n\nFigure 1: Questions that should be answered in a preregistration.\n\nplanned project, and a PDF file of the pre-registration is generated. The Open Science Framework template is more detailed and asks for specifics about the study design and data analysis.",
    "is_useful": true,
    "question": "What elements should be included in a preregistration for a study to ensure clarity and transparency in the data analysis process?"
  },
  {
    "text": "There are several websites that offer services to preregister studies: two of the most discussed platforms are AsPredicted (AsPredicted.org) and the preregistration forms on the Open Science Framework (osf.io). These platforms afford time-logged reports and either make them publicly available or grant anonymous access only to a specific group of people (such as reviewers and editors during the peer-review process). AsPredicted.org is a rather slim version of what is necessary for a preregistration. One author on the research team simply answers nine questions about the\n\n![](_page_8_Picture_2.jpeg)\n\nFigure 1: Questions that should be answered in a preregistration.\n\nplanned project, and a PDF file of the pre-registration is generated. The Open Science Framework template is more detailed and asks for specifics about the study design and data analysis. The preregistration can conveniently be associated with an OSF repository and linked to materials, data, analyses scripts, and preprints.\n\nA particularly promising version of preregistration is a peer-reviewed Registered Report (Nosek and Lakens 2014; Nosek et al. 2018). Registered Reports include the theoretical rationale and research question(s) of the study as well as a detailed methodological description that aims to answer those questions. In other words, a Registered Report is a full-fledged manuscript that does not present results. These reports are assessed by peer reviewers, who offer critical feedback on how well the proposed method addresses the research question.",
    "is_useful": true,
    "question": "What are some platforms that provide services for preregistering studies in open science?"
  },
  {
    "text": "[](_page_8_Picture_2.jpeg)\n\nFigure 1: Questions that should be answered in a preregistration.\n\nplanned project, and a PDF file of the pre-registration is generated. The Open Science Framework template is more detailed and asks for specifics about the study design and data analysis. The preregistration can conveniently be associated with an OSF repository and linked to materials, data, analyses scripts, and preprints.\n\nA particularly promising version of preregistration is a peer-reviewed Registered Report (Nosek and Lakens 2014; Nosek et al. 2018). Registered Reports include the theoretical rationale and research question(s) of the study as well as a detailed methodological description that aims to answer those questions. In other words, a Registered Report is a full-fledged manuscript that does not present results. These reports are assessed by peer reviewers, who offer critical feedback on how well the proposed method addresses the research question. This critical feedback helps the authors to refine their methodological design and potentially identify critical flaws. Upon sufficient revision, the study plan might get accepted in-principle, irrespective of whether the results confirm the researchers' predictions or not.\n\nThe idea behind Registered Reports is by no means new either. Similar proposals have been made as early as 1966 by Robert Rosenthal (1966) (cited by Chambers 2017). The first journal to implement this article format was Cortex almost 50 years later, with an increasing number of journals following suit.",
    "is_useful": true,
    "question": "What are Registered Reports and how do they contribute to the preregistration process in research?"
  },
  {
    "text": "2018). Registered Reports include the theoretical rationale and research question(s) of the study as well as a detailed methodological description that aims to answer those questions. In other words, a Registered Report is a full-fledged manuscript that does not present results. These reports are assessed by peer reviewers, who offer critical feedback on how well the proposed method addresses the research question. This critical feedback helps the authors to refine their methodological design and potentially identify critical flaws. Upon sufficient revision, the study plan might get accepted in-principle, irrespective of whether the results confirm the researchers' predictions or not.\n\nThe idea behind Registered Reports is by no means new either. Similar proposals have been made as early as 1966 by Robert Rosenthal (1966) (cited by Chambers 2017). The first journal to implement this article format was Cortex almost 50 years later, with an increasing number of journals following suit. As of time of writing, there are already 286 scientific journals (and counting) that have adopted Registered Reports including linguistic journals like Bilingualism: Language and Cognition, Biolinguistics, Cognitive Linguistics, Discourse Processes, Language Learning, and Language & Speech.\n\nThe Registered Report workflow effectively counteracts the dynamics that lead to publication bias and has already been shown to produce a more realistic amount of null results than regular publication routes. For example, Allen and Mehler (2019) showed that out of 113 analyzed Registered Reports that explicitly declared to have tested a hypothesis, only 40% found confirmatory evidence. Similarly, Scheel et al.",
    "is_useful": true,
    "question": "What is the purpose of Registered Reports in the context of open science?"
  },
  {
    "text": "Similar proposals have been made as early as 1966 by Robert Rosenthal (1966) (cited by Chambers 2017). The first journal to implement this article format was Cortex almost 50 years later, with an increasing number of journals following suit. As of time of writing, there are already 286 scientific journals (and counting) that have adopted Registered Reports including linguistic journals like Bilingualism: Language and Cognition, Biolinguistics, Cognitive Linguistics, Discourse Processes, Language Learning, and Language & Speech.\n\nThe Registered Report workflow effectively counteracts the dynamics that lead to publication bias and has already been shown to produce a more realistic amount of null results than regular publication routes. For example, Allen and Mehler (2019) showed that out of 113 analyzed Registered Reports that explicitly declared to have tested a hypothesis, only 40% found confirmatory evidence. Similarly, Scheel et al. (Scheel et al. 2021) found 44% confirmed hypothesis in a sample of 71 Registered Reports. While certainly more of these analyses are needed to come to a firmer conclusion about the extent to which Registered Reports reduce publication bias, these numbers stand in stark contrast to those presented by Fanelli (2012) who showed an average of 80% confirmed findings in published papers across disciplines.\n\n# 4 Applications, challenges, and limitations\n\nAt first sight, there are many challenges that come with preregistering linguistic studies (see Nosek et al. 2018 for a general discussion; see also Marsden et al. 2018a).",
    "is_useful": true,
    "question": "What is the significance of Registered Reports in scientific publishing, particularly in relation to publication bias?"
  },
  {
    "text": "For example, Allen and Mehler (2019) showed that out of 113 analyzed Registered Reports that explicitly declared to have tested a hypothesis, only 40% found confirmatory evidence. Similarly, Scheel et al. (Scheel et al. 2021) found 44% confirmed hypothesis in a sample of 71 Registered Reports. While certainly more of these analyses are needed to come to a firmer conclusion about the extent to which Registered Reports reduce publication bias, these numbers stand in stark contrast to those presented by Fanelli (2012) who showed an average of 80% confirmed findings in published papers across disciplines.\n\n# 4 Applications, challenges, and limitations\n\nAt first sight, there are many challenges that come with preregistering linguistic studies (see Nosek et al. 2018 for a general discussion; see also Marsden et al. 2018a). In this section, I will discuss some illustrative examples from experimental linguistics (psycholinguistics and phonetics) that differ in their data collection procedure, the accessibility of data, the time of analysis (before or after data collection), the type of analysis, and so forth.\n\n#### 4.1 Using pre-existing data\n\nConsider the following scenario: A group of researchers (henceforth research group A) is interested in whether the predictability of a word affects its pronunciation. They plan to use an already existing data set: the HCRC Map Task Corpus (Anderson et al. 1991).",
    "is_useful": true,
    "question": "What percentage of Registered Reports found confirmatory evidence for tested hypotheses, according to recent analyses?"
  },
  {
    "text": "# 4 Applications, challenges, and limitations\n\nAt first sight, there are many challenges that come with preregistering linguistic studies (see Nosek et al. 2018 for a general discussion; see also Marsden et al. 2018a). In this section, I will discuss some illustrative examples from experimental linguistics (psycholinguistics and phonetics) that differ in their data collection procedure, the accessibility of data, the time of analysis (before or after data collection), the type of analysis, and so forth.\n\n#### 4.1 Using pre-existing data\n\nConsider the following scenario: A group of researchers (henceforth research group A) is interested in whether the predictability of a word affects its pronunciation. They plan to use an already existing data set: the HCRC Map Task Corpus (Anderson et al. 1991). They want to assess the predictability of words and they plan to extract fundamental frequency ( f0) 3 as the relevant acoustic dimension. The HCRC Corpus had already been collected when they formulated their research hypothesis. One may object, therefore, that preregistrations cannot be applied in such studies.\n\nWhile testing hypotheses on pre-existing data is not ideal, preregistration of the analyses can still be performed. Ideally, of course, researchers want to limit researcher degrees of freedom prior to having seen the data. However, a large amount of recent advancements in our understanding of language resulted from secondary data analyses based on already existing corpora.",
    "is_useful": true,
    "question": "What are some challenges and considerations when preregistering studies that utilize pre-existing data in experimental linguistics?"
  },
  {
    "text": "They plan to use an already existing data set: the HCRC Map Task Corpus (Anderson et al. 1991). They want to assess the predictability of words and they plan to extract fundamental frequency ( f0) 3 as the relevant acoustic dimension. The HCRC Corpus had already been collected when they formulated their research hypothesis. One may object, therefore, that preregistrations cannot be applied in such studies.\n\nWhile testing hypotheses on pre-existing data is not ideal, preregistration of the analyses can still be performed. Ideally, of course, researchers want to limit researcher degrees of freedom prior to having seen the data. However, a large amount of recent advancements in our understanding of language resulted from secondary data analyses based on already existing corpora. Nevertheless, researchers can (and should) preregister analyses after having seen pilot data, parts of the study, or even whole corpora. When researchers generate a hypothesis which they intend to confirm with preexisting data, they can preregister analysis plans and commit to how evidence will be interpreted before analyzing the data. For example, research group A can preregister the exact way they are going to measure predictability (e.g., Do they consider monogram, bigram, trigram probabilities? Are these indices treated as separate predictors or combined into one predictability index? If they are combined, how?",
    "is_useful": true,
    "question": "What is the role of preregistration in research that utilizes pre-existing data sets?"
  },
  {
    "text": "While testing hypotheses on pre-existing data is not ideal, preregistration of the analyses can still be performed. Ideally, of course, researchers want to limit researcher degrees of freedom prior to having seen the data. However, a large amount of recent advancements in our understanding of language resulted from secondary data analyses based on already existing corpora. Nevertheless, researchers can (and should) preregister analyses after having seen pilot data, parts of the study, or even whole corpora. When researchers generate a hypothesis which they intend to confirm with preexisting data, they can preregister analysis plans and commit to how evidence will be interpreted before analyzing the data. For example, research group A can preregister the exact way they are going to measure predictability (e.g., Do they consider monogram, bigram, trigram probabilities? Are these indices treated as separate predictors or combined into one predictability index? If they are combined, how?), they can preregister possible control factors that might affect the dependent variable (e.g., dimensions that are known to affect f0, such as speaker sex and illocutionary force), they can a priori define which data will be excluded (e.g., Are they looking at content words only? Are they only looking at words that fall into a certain range of lexical frequency?), etc.\n\nA challenge for preregistering preexisting data analyses is how much the analyst knows about the data set.",
    "is_useful": true,
    "question": "What is the importance of preregistration in the context of analyzing preexisting data in research studies?"
  },
  {
    "text": "For example, research group A can preregister the exact way they are going to measure predictability (e.g., Do they consider monogram, bigram, trigram probabilities? Are these indices treated as separate predictors or combined into one predictability index? If they are combined, how?), they can preregister possible control factors that might affect the dependent variable (e.g., dimensions that are known to affect f0, such as speaker sex and illocutionary force), they can a priori define which data will be excluded (e.g., Are they looking at content words only? Are they only looking at words that fall into a certain range of lexical frequency?), etc.\n\nA challenge for preregistering preexisting data analyses is how much the analyst knows about the data set. The researchers might have read the seminal paper by Aylett and Turk (2004) who analyzed the same data set, the HCRC Map Task Corpus, to answer a related research question. Aylett and Turk looked at the relationship between word duration and the predictability of the word. In situations like this, it is important to record who has observed the data before the analysis and what observations and summary reports are publicly available and potentially known to the authors. If the authors are blind to already published investigations on a data set, the authors could still test 'novel' predictions. However, if the data set has already been queried with respect to the specific research question, the authors may wish to apply a different analysis, in which case they could pre-register the rationale and analysis plan of said analysis.",
    "is_useful": true,
    "question": "What are some considerations researchers should take into account when preregistering their analysis of data in open science?"
  },
  {
    "text": "), etc.\n\nA challenge for preregistering preexisting data analyses is how much the analyst knows about the data set. The researchers might have read the seminal paper by Aylett and Turk (2004) who analyzed the same data set, the HCRC Map Task Corpus, to answer a related research question. Aylett and Turk looked at the relationship between word duration and the predictability of the word. In situations like this, it is important to record who has observed the data before the analysis and what observations and summary reports are publicly available and potentially known to the authors. If the authors are blind to already published investigations on a data set, the authors could still test 'novel' predictions. However, if the data set has already been queried with respect to the specific research question, the authors may wish to apply a different analysis, in which case they could pre-register the rationale and analysis plan of said analysis.\n\nRegardless of prior knowledge about the data, possible biases in statistical inference can still be minimized by being transparent about what was known prior\n\n<sup>3</sup> Fundamental frequency is the lowest frequency of a periodic waveform. In the context of speech, fundamental frequency closely corresponds to what we perceive as pitch.\n\nto analysis and preregistering the analysis plan. This procedure still reduces researcher degrees of freedom (see Weston et al. 2019 for a collection of resources for secondary data analysis including a preregistration template).",
    "is_useful": true,
    "question": "What is the importance of preregistering analysis plans in research involving preexisting data sets?"
  },
  {
    "text": "If the authors are blind to already published investigations on a data set, the authors could still test 'novel' predictions. However, if the data set has already been queried with respect to the specific research question, the authors may wish to apply a different analysis, in which case they could pre-register the rationale and analysis plan of said analysis.\n\nRegardless of prior knowledge about the data, possible biases in statistical inference can still be minimized by being transparent about what was known prior\n\n<sup>3</sup> Fundamental frequency is the lowest frequency of a periodic waveform. In the context of speech, fundamental frequency closely corresponds to what we perceive as pitch.\n\nto analysis and preregistering the analysis plan. This procedure still reduces researcher degrees of freedom (see Weston et al. 2019 for a collection of resources for secondary data analysis including a preregistration template).\n\n#### 4.2 Changing the preregistration\n\nDeviations from a data collection and analysis plan are common, especially in research that deals with less accessible populations, clinical populations, or populations spanning certain age groups. Researchers also often lack relevant knowledge about the sample, the data collection or analysis. Consider the following scenario: A team of researchers (henceforth research group B) is interested in processing consequences of focus in German children. They hypothesize that focused words are accessed more quickly than words that are not in focus. In their planned experiment, three- to seven-year-olds react to sentences with target words being either in focus or not in a visual world paradigm. The children's eye movements are recorded during comprehension.",
    "is_useful": true,
    "question": "How can researchers minimize biases in statistical inference when analyzing previously queried data sets?"
  },
  {
    "text": "to analysis and preregistering the analysis plan. This procedure still reduces researcher degrees of freedom (see Weston et al. 2019 for a collection of resources for secondary data analysis including a preregistration template).\n\n#### 4.2 Changing the preregistration\n\nDeviations from a data collection and analysis plan are common, especially in research that deals with less accessible populations, clinical populations, or populations spanning certain age groups. Researchers also often lack relevant knowledge about the sample, the data collection or analysis. Consider the following scenario: A team of researchers (henceforth research group B) is interested in processing consequences of focus in German children. They hypothesize that focused words are accessed more quickly than words that are not in focus. In their planned experiment, three- to seven-year-olds react to sentences with target words being either in focus or not in a visual world paradigm. The children's eye movements are recorded during comprehension. The researchers planned to test 70 children but 12 of the 70 children fell asleep during the experiment, a state of affairs that renders their data useless. The sleepiness of children was not anticipated and therefore not mentioned as a data exclusion criterion in the preregistration.\n\nIn this scenario, it is possible to change the preregistration and document these changes alongside the reasons as to why and when changes were made. This procedure still provides substantially lower risk of cognitive biases impacting the conclusions compared to a situation without any preregistration. It also makes these changes to the analysis transparent and detectable.",
    "is_useful": true,
    "question": "What are the benefits of preregistration in research, particularly regarding analysis transparency and the management of changes during a study?"
  },
  {
    "text": "They hypothesize that focused words are accessed more quickly than words that are not in focus. In their planned experiment, three- to seven-year-olds react to sentences with target words being either in focus or not in a visual world paradigm. The children's eye movements are recorded during comprehension. The researchers planned to test 70 children but 12 of the 70 children fell asleep during the experiment, a state of affairs that renders their data useless. The sleepiness of children was not anticipated and therefore not mentioned as a data exclusion criterion in the preregistration.\n\nIn this scenario, it is possible to change the preregistration and document these changes alongside the reasons as to why and when changes were made. This procedure still provides substantially lower risk of cognitive biases impacting the conclusions compared to a situation without any preregistration. It also makes these changes to the analysis transparent and detectable.\n\nAnother important challenge when preregistering a study is specifying appropriate statistical models in advance. Preregistering data analyses necessitates knowledge about the nature of the data. For example, research group B might preregister an analysis assuming that the residuals of the model are normally distributed. After collecting their data, they realize that the data has heavy right tails, calling for a log-transformation or a statistical model without the assumption that residuals are normally distributed. The preregistered analysis is not appropriate. One solution to this challenge is to define data analytical procedures in advance that allow them to evaluate distributional aspects of the data and potential data transformations irrespective of the research question.",
    "is_useful": true,
    "question": "What are the benefits of preregistration in research studies, particularly regarding data analysis and potential biases?"
  },
  {
    "text": "This procedure still provides substantially lower risk of cognitive biases impacting the conclusions compared to a situation without any preregistration. It also makes these changes to the analysis transparent and detectable.\n\nAnother important challenge when preregistering a study is specifying appropriate statistical models in advance. Preregistering data analyses necessitates knowledge about the nature of the data. For example, research group B might preregister an analysis assuming that the residuals of the model are normally distributed. After collecting their data, they realize that the data has heavy right tails, calling for a log-transformation or a statistical model without the assumption that residuals are normally distributed. The preregistered analysis is not appropriate. One solution to this challenge is to define data analytical procedures in advance that allow them to evaluate distributional aspects of the data and potential data transformations irrespective of the research question. Alternatively, one could preregister a decision tree. This may be particularly useful for people using linear mixed-effects models, which are known to occasionally fail to converge. Convergence failures indicate that the iterative optimization procedure fails to reach a stable solution, which renders the model results uninterpretable. In order to remedy such convergence issues, a common strategy is to remove complex random effect terms incrementally from the model, a strategy which often comes with other risks such as inflated Type-I error rates (Barr et al. 2013; Matuschek et al. 2017). Since one cannot anticipate whether a model will converge or not, a plan of how to reduce model complexity can be preregistered in advance.",
    "is_useful": true,
    "question": "What are some of the challenges and solutions associated with preregistering statistical analyses in research studies?"
  },
  {
    "text": "The preregistered analysis is not appropriate. One solution to this challenge is to define data analytical procedures in advance that allow them to evaluate distributional aspects of the data and potential data transformations irrespective of the research question. Alternatively, one could preregister a decision tree. This may be particularly useful for people using linear mixed-effects models, which are known to occasionally fail to converge. Convergence failures indicate that the iterative optimization procedure fails to reach a stable solution, which renders the model results uninterpretable. In order to remedy such convergence issues, a common strategy is to remove complex random effect terms incrementally from the model, a strategy which often comes with other risks such as inflated Type-I error rates (Barr et al. 2013; Matuschek et al. 2017). Since one cannot anticipate whether a model will converge or not, a plan of how to reduce model complexity can be preregistered in advance.\n\nOn a related point, research group A working on the HCRC corpus (see 4.1 above) may realize that they did not anticipate an important set of covariates in their statistical model. Words in phrase-final position often exhibit a rising f0 contour. The last word in an utterance is also often the most predictable, so phrase positions might be confounding predictability. Ideally, then, the analysis should control for phrase position.",
    "is_useful": true,
    "question": "What strategies can researchers use to address model convergence issues in statistical analyses?"
  },
  {
    "text": "In order to remedy such convergence issues, a common strategy is to remove complex random effect terms incrementally from the model, a strategy which often comes with other risks such as inflated Type-I error rates (Barr et al. 2013; Matuschek et al. 2017). Since one cannot anticipate whether a model will converge or not, a plan of how to reduce model complexity can be preregistered in advance.\n\nOn a related point, research group A working on the HCRC corpus (see 4.1 above) may realize that they did not anticipate an important set of covariates in their statistical model. Words in phrase-final position often exhibit a rising f0 contour. The last word in an utterance is also often the most predictable, so phrase positions might be confounding predictability. Ideally, then, the analysis should control for phrase position. In large multivariate data sets, it is important to not only consider a large number of possible covariates but also to consider the many ways how to handle possible collinearity between these covariates (Tomaschek et al. 2018). All of these decisions influence the final results (Roettger 2019; Simmons et al. 2011) and should be anticipated as much as possible. One helpful way to anticipate data-analytical decisions is to examine data from publicly accessible studies (an underappreciated benefit of making data publicly available).",
    "is_useful": true,
    "question": "What are the benefits of making data publicly available in research?"
  },
  {
    "text": "Words in phrase-final position often exhibit a rising f0 contour. The last word in an utterance is also often the most predictable, so phrase positions might be confounding predictability. Ideally, then, the analysis should control for phrase position. In large multivariate data sets, it is important to not only consider a large number of possible covariates but also to consider the many ways how to handle possible collinearity between these covariates (Tomaschek et al. 2018). All of these decisions influence the final results (Roettger 2019; Simmons et al. 2011) and should be anticipated as much as possible. One helpful way to anticipate data-analytical decisions is to examine data from publicly accessible studies (an underappreciated benefit of making data publicly available). Another way to deal with these complex decisions is splitting the data set into two parts (a process called cross-validation, see Stone 1974). One may use the first part to evaluate the feasibility of an analysis and preregister a confirmatory analysis for the second part (Fafchamps and Labonne 2017).\n\nRegardless of how hard one tries \u2013 certain details of the data collection or analysis sometimes cannot be fully anticipated. But as long as one is transparent about the necessary changes to a preregistration, researcher degrees of freedom are reduced. Alternatively \u2013 and orthogonal to increasing transparency \u2013 researchers could run both the preregistered analysis and the changed analysis to evaluate the robustness of a finding.",
    "is_useful": true,
    "question": "What are some benefits of making data publicly available in the context of data analysis?"
  },
  {
    "text": "2011) and should be anticipated as much as possible. One helpful way to anticipate data-analytical decisions is to examine data from publicly accessible studies (an underappreciated benefit of making data publicly available). Another way to deal with these complex decisions is splitting the data set into two parts (a process called cross-validation, see Stone 1974). One may use the first part to evaluate the feasibility of an analysis and preregister a confirmatory analysis for the second part (Fafchamps and Labonne 2017).\n\nRegardless of how hard one tries \u2013 certain details of the data collection or analysis sometimes cannot be fully anticipated. But as long as one is transparent about the necessary changes to a preregistration, researcher degrees of freedom are reduced. Alternatively \u2013 and orthogonal to increasing transparency \u2013 researchers could run both the preregistered analysis and the changed analysis to evaluate the robustness of a finding. This enables researchers to either show that decisions after having seen the data do not influence the results or to transparently communicate possible divergences in their results that are due to critical decisions.\n\n#### 4.3 Exploration beyond preregistered protocol\n\nAfter following the preregistered protocol, research team B, who is interested in focus processing, may end up with a null result. They could not find any relationship between focus and comprehension times. However, they would like to explore their results further and look at different measures related to eye movements. They may think that further exploration is prohibited because they preregistered only analyses related to comprehension times.",
    "is_useful": true,
    "question": "What are some benefits and practices of making research data publicly available in the context of open science?"
  },
  {
    "text": "But as long as one is transparent about the necessary changes to a preregistration, researcher degrees of freedom are reduced. Alternatively \u2013 and orthogonal to increasing transparency \u2013 researchers could run both the preregistered analysis and the changed analysis to evaluate the robustness of a finding. This enables researchers to either show that decisions after having seen the data do not influence the results or to transparently communicate possible divergences in their results that are due to critical decisions.\n\n#### 4.3 Exploration beyond preregistered protocol\n\nAfter following the preregistered protocol, research team B, who is interested in focus processing, may end up with a null result. They could not find any relationship between focus and comprehension times. However, they would like to explore their results further and look at different measures related to eye movements. They may think that further exploration is prohibited because they preregistered only analyses related to comprehension times. Finding themselves in a similar situation, research group B has stuck to the preregistered protocol and reports their results at a conference, where they receive feedback and suggestions for further exploration. Like research team B, however, they may feel imprisoned by the preregistered analysis plan.\n\nPerceiving preregistration as too rigid is a commonly articulated concern (e.g., Goldin-Meadow 2016). It is, however, unwarranted. Preregistration only constraints the confirmatory part of an analysis and does not impact exploration at all.",
    "is_useful": true,
    "question": "How does preregistration impact researchers' ability to explore findings beyond the original protocol?"
  },
  {
    "text": "They could not find any relationship between focus and comprehension times. However, they would like to explore their results further and look at different measures related to eye movements. They may think that further exploration is prohibited because they preregistered only analyses related to comprehension times. Finding themselves in a similar situation, research group B has stuck to the preregistered protocol and reports their results at a conference, where they receive feedback and suggestions for further exploration. Like research team B, however, they may feel imprisoned by the preregistered analysis plan.\n\nPerceiving preregistration as too rigid is a commonly articulated concern (e.g., Goldin-Meadow 2016). It is, however, unwarranted. Preregistration only constraints the confirmatory part of an analysis and does not impact exploration at all. After testing their predictions, researchers are free to explore their data sets, which is, as argued above, an essential component of the scientific discovery process (Nosek et al. 2019; Wagenmakers et al. 2012) and in fact an integral aspect of linguistic research in general (Grieve, this issue). Preregistration simply draws a line between confirmation and exploration, which we can and should clearly flag in our manuscripts (see APA style guidelines, https://apastyle.apa. org/jars/quant-table-1.pdf). That means, when exploring, researchers generate (rather than test) new hypotheses, which would then need to be substantiated by subsequent empirical investigation.",
    "is_useful": true,
    "question": "What concerns do researchers commonly express about preregistration in the context of open science and how does it affect the exploration of data sets?"
  },
  {
    "text": "It is, however, unwarranted. Preregistration only constraints the confirmatory part of an analysis and does not impact exploration at all. After testing their predictions, researchers are free to explore their data sets, which is, as argued above, an essential component of the scientific discovery process (Nosek et al. 2019; Wagenmakers et al. 2012) and in fact an integral aspect of linguistic research in general (Grieve, this issue). Preregistration simply draws a line between confirmation and exploration, which we can and should clearly flag in our manuscripts (see APA style guidelines, https://apastyle.apa. org/jars/quant-table-1.pdf). That means, when exploring, researchers generate (rather than test) new hypotheses, which would then need to be substantiated by subsequent empirical investigation. This calls for a certain modesty when reporting the findings of the exploration as the generalizability of these findings needs to be considered with caution.\n\n#### 4.4 No time for preregistration\n\nSome research is based on student projects or grants with quick turnarounds. Researchers sometimes need to deliver academic currency within a short time scale. This time pressure might make preregistrations not feasible. While this is a legitimate concern for Registered Reports (the peer-reviewed version of preregistration), it does not necessarily apply to preregistrations in general. Writing up the methodological plan prior to data collection and analysis might strike one as additional work, but it is arguably either just shifting the work load or saving time in the long run.",
    "is_useful": true,
    "question": "What role does preregistration play in distinguishing between confirmatory analysis and exploratory research in scientific studies?"
  },
  {
    "text": "org/jars/quant-table-1.pdf). That means, when exploring, researchers generate (rather than test) new hypotheses, which would then need to be substantiated by subsequent empirical investigation. This calls for a certain modesty when reporting the findings of the exploration as the generalizability of these findings needs to be considered with caution.\n\n#### 4.4 No time for preregistration\n\nSome research is based on student projects or grants with quick turnarounds. Researchers sometimes need to deliver academic currency within a short time scale. This time pressure might make preregistrations not feasible. While this is a legitimate concern for Registered Reports (the peer-reviewed version of preregistration), it does not necessarily apply to preregistrations in general. Writing up the methodological plan prior to data collection and analysis might strike one as additional work, but it is arguably either just shifting the work load or saving time in the long run. On the one hand, the method section of any paper needs to be written up eventually, so preregistering a study merely shifts that part of the process to an earlier point in time. The preregistration platforms mentioned in Section 3 make this easy, so there is not even any learning curve to consider. Moreover, writing up the preregistration leads to a more critical assessment of the method in advance and might lead to elimination of critical flaws in the design. Fixing these issues at an early stage saves time and resources.",
    "is_useful": true,
    "question": "What are the potential benefits of preregistration in research, particularly in the context of time constraints and methodological rigor?"
  },
  {
    "text": "This time pressure might make preregistrations not feasible. While this is a legitimate concern for Registered Reports (the peer-reviewed version of preregistration), it does not necessarily apply to preregistrations in general. Writing up the methodological plan prior to data collection and analysis might strike one as additional work, but it is arguably either just shifting the work load or saving time in the long run. On the one hand, the method section of any paper needs to be written up eventually, so preregistering a study merely shifts that part of the process to an earlier point in time. The preregistration platforms mentioned in Section 3 make this easy, so there is not even any learning curve to consider. Moreover, writing up the preregistration leads to a more critical assessment of the method in advance and might lead to elimination of critical flaws in the design. Fixing these issues at an early stage saves time and resources. When it comes to Registered Reports, the eventual findings cannot be CARKed (Criticized After Results are Known; see Nosek and Lakens 2014) and subsequently rejected by the journal contingent on whether the results corroborate or contradict the researchers' predictions or established views. Editors, reviewers, and authors thus save valuable time and resources in the long run.\n\n#### 4.5 No a priori predictions\n\nResearch group A might be at the beginning of a new research program and does not have concrete predictions as to what aspects of the acoustic signal to measure or how to operationalize predictability.",
    "is_useful": true,
    "question": "What are some potential advantages of preregistration in research, especially in terms of methodology and resource efficiency?"
  },
  {
    "text": "The preregistration platforms mentioned in Section 3 make this easy, so there is not even any learning curve to consider. Moreover, writing up the preregistration leads to a more critical assessment of the method in advance and might lead to elimination of critical flaws in the design. Fixing these issues at an early stage saves time and resources. When it comes to Registered Reports, the eventual findings cannot be CARKed (Criticized After Results are Known; see Nosek and Lakens 2014) and subsequently rejected by the journal contingent on whether the results corroborate or contradict the researchers' predictions or established views. Editors, reviewers, and authors thus save valuable time and resources in the long run.\n\n#### 4.5 No a priori predictions\n\nResearch group A might be at the beginning of a new research program and does not have concrete predictions as to what aspects of the acoustic signal to measure or how to operationalize predictability. At the beginning of a research program, researchers rarely have very concrete hypotheses about a system under investigation. In these cases, it is highly appropriate to explore available data and generate new hypotheses. The researchers might explore several acoustic parameters and different predictability indices in a first stage during the discovery process. Research group B might collect some pilot data to identify potential challenges with their preplanned statistical analysis. This is fine, and for this stage of the research project, preregistering a study is not necessarily the best workflow. However, these exploratory studies are often written up in a way that recasts them as hypothesis-testing (Kerr 1998).",
    "is_useful": true,
    "question": "What are the benefits of preregistration and Registered Reports in research?"
  },
  {
    "text": "#### 4.5 No a priori predictions\n\nResearch group A might be at the beginning of a new research program and does not have concrete predictions as to what aspects of the acoustic signal to measure or how to operationalize predictability. At the beginning of a research program, researchers rarely have very concrete hypotheses about a system under investigation. In these cases, it is highly appropriate to explore available data and generate new hypotheses. The researchers might explore several acoustic parameters and different predictability indices in a first stage during the discovery process. Research group B might collect some pilot data to identify potential challenges with their preplanned statistical analysis. This is fine, and for this stage of the research project, preregistering a study is not necessarily the best workflow. However, these exploratory studies are often written up in a way that recasts them as hypothesis-testing (Kerr 1998). This is, again, not necessarily an intentional process. Cognitive biases in tandem with the academic incentive system are often hidden driving forces of such decisions. The nature of statistical procedures (and common misconceptions about them) further facilitate this process. For example, using null hypothesis significance testing, a p-value has only known diagnosticity of false positive rates when one tests prespecified hypotheses and corrects for the number of hypotheses tested. In exploratory analyses, the false positive rate is unknown. Preregistration can still be a valuable tool in these exploratory settings as it constrains researcher degrees of freedom at the analysis stage and makes them transparent.",
    "is_useful": true,
    "question": "What are the implications of not having a priori predictions in research, particularly in the context of exploratory studies and preregistration?"
  },
  {
    "text": "This is fine, and for this stage of the research project, preregistering a study is not necessarily the best workflow. However, these exploratory studies are often written up in a way that recasts them as hypothesis-testing (Kerr 1998). This is, again, not necessarily an intentional process. Cognitive biases in tandem with the academic incentive system are often hidden driving forces of such decisions. The nature of statistical procedures (and common misconceptions about them) further facilitate this process. For example, using null hypothesis significance testing, a p-value has only known diagnosticity of false positive rates when one tests prespecified hypotheses and corrects for the number of hypotheses tested. In exploratory analyses, the false positive rate is unknown. Preregistration can still be a valuable tool in these exploratory settings as it constrains researcher degrees of freedom at the analysis stage and makes them transparent. It is also conceivable to first run an unregistered exploration and then formulate concrete predictions that are tested on a novel data set following preregistered protocol. However, preregistration is more useful for confirmatory research.\n\n#### 4.6 Remaining limitations and observational research\n\nPreregistration is not a panacea for all challenges to empirical sciences. I have already mentioned several limitations of preregistering studies. For example, researchers often face practical limitations as to how they can collect certain data types and how flexible they are with regard to methodological choices in culturally diverse settings or working with different populations.",
    "is_useful": true,
    "question": "What role does preregistration play in exploratory and confirmatory research within the context of open science?"
  },
  {
    "text": "In exploratory analyses, the false positive rate is unknown. Preregistration can still be a valuable tool in these exploratory settings as it constrains researcher degrees of freedom at the analysis stage and makes them transparent. It is also conceivable to first run an unregistered exploration and then formulate concrete predictions that are tested on a novel data set following preregistered protocol. However, preregistration is more useful for confirmatory research.\n\n#### 4.6 Remaining limitations and observational research\n\nPreregistration is not a panacea for all challenges to empirical sciences. I have already mentioned several limitations of preregistering studies. For example, researchers often face practical limitations as to how they can collect certain data types and how flexible they are with regard to methodological choices in culturally diverse settings or working with different populations. Researchers might also be constrained by limited resources and time, making collecting pilot data not a feasible option. Moreover, preregistration is a work flow designed for confirmatory research, mostly found in experimental fields. Thus, preregistration does not fit all forms of linguistic inquiry. It is clear that a large proportion of linguistic subdisciplines is observational in nature (Grieve this issue), including much of corpus linguistics, discourse analysis, field linguistics, historical linguistics, and typology. Studies in these fields are usually not testing specific hypotheses. Instead, they are exploratory in nature. For example, if researchers are interested in how a given phonological contrast is phonetically manifested, they do not necessarily test hypotheses, but explore possible relationships between the signal and lexical forms.",
    "is_useful": true,
    "question": "What role does preregistration play in exploratory and confirmatory research, and what limitations are associated with its use in empirical sciences?"
  },
  {
    "text": "Researchers might also be constrained by limited resources and time, making collecting pilot data not a feasible option. Moreover, preregistration is a work flow designed for confirmatory research, mostly found in experimental fields. Thus, preregistration does not fit all forms of linguistic inquiry. It is clear that a large proportion of linguistic subdisciplines is observational in nature (Grieve this issue), including much of corpus linguistics, discourse analysis, field linguistics, historical linguistics, and typology. Studies in these fields are usually not testing specific hypotheses. Instead, they are exploratory in nature. For example, if researchers are interested in how a given phonological contrast is phonetically manifested, they do not necessarily test hypotheses, but explore possible relationships between the signal and lexical forms. A corpus linguist who is interested in finding relationships between different semantic fields in the lexicon might also not test specific a priori hypotheses. In these cases, preregistration might not be applicable.\n\nWithin experimental linguistics, purely exploratory studies, i.e., studies that explicitly \"only\" generate new hypothesis without testing them, are still difficult to publish and could prohibit or at least substantially slow down publication. One solution would be to explore first and then confirm those exploratory findings on a new data set (Nicenboim et al. 2018b). However, this work flow might not be feasible for certain linguistic projects. The lack of publication value of exploratory analyses in confirmatory fields is deeply rooted in the publication system and must be tackled as a (separate) community-wide effort.",
    "is_useful": true,
    "question": "What challenges do researchers face in publishing exploratory studies within the field of linguistics?"
  },
  {
    "text": "A corpus linguist who is interested in finding relationships between different semantic fields in the lexicon might also not test specific a priori hypotheses. In these cases, preregistration might not be applicable.\n\nWithin experimental linguistics, purely exploratory studies, i.e., studies that explicitly \"only\" generate new hypothesis without testing them, are still difficult to publish and could prohibit or at least substantially slow down publication. One solution would be to explore first and then confirm those exploratory findings on a new data set (Nicenboim et al. 2018b). However, this work flow might not be feasible for certain linguistic projects. The lack of publication value of exploratory analyses in confirmatory fields is deeply rooted in the publication system and must be tackled as a (separate) community-wide effort. A promising way forward relates to changing the incentive structure. Linguistic journals that publish experimental work could explicitly reward exploratory studies by creating respective article types and encourage exploratory analyses (for an example at Cortex, see McIntosh 2017). It is important to stress that it is not desirable to make procedures that are used to ensure robustness and generalizability in strictly confirmatory settings obligatory to all types of linguistic studies. This would likely lead to devaluing or marginalizing those studies for which the preregistration procedures do not fit. Instead, linguists should embrace different paths of discovery and value them equally within their journals.",
    "is_useful": true,
    "question": "What challenges do exploratory studies face in the publication process within experimental linguistics, and how can the publication system be improved to support them?"
  },
  {
    "text": "2018b). However, this work flow might not be feasible for certain linguistic projects. The lack of publication value of exploratory analyses in confirmatory fields is deeply rooted in the publication system and must be tackled as a (separate) community-wide effort. A promising way forward relates to changing the incentive structure. Linguistic journals that publish experimental work could explicitly reward exploratory studies by creating respective article types and encourage exploratory analyses (for an example at Cortex, see McIntosh 2017). It is important to stress that it is not desirable to make procedures that are used to ensure robustness and generalizability in strictly confirmatory settings obligatory to all types of linguistic studies. This would likely lead to devaluing or marginalizing those studies for which the preregistration procedures do not fit. Instead, linguists should embrace different paths of discovery and value them equally within their journals.\n\n## 5 Summary\n\nPreregistration, and especially its peer-reviewed version as a Registered Report, is a powerful tool to reduce publication bias (the tendency to predominantly publish confirming and \"significant\" findings) and it constrains researcher degrees of freedom. With that, preregistration can substantially reduce false discoveries in the publication record, which themselves can have far-reaching consequences, often leading to theoretical claims that may misguide future research (Smaldino and McElreath 2016). Preregistration can increase the robustness of published findings. In turn, a more robust publication record allows experimental linguists to more effectively accumulate knowledge and advance their understanding of human language.",
    "is_useful": true,
    "question": "How can the publication system be improved to support exploratory analyses in linguistic research?"
  },
  {
    "text": "This would likely lead to devaluing or marginalizing those studies for which the preregistration procedures do not fit. Instead, linguists should embrace different paths of discovery and value them equally within their journals.\n\n## 5 Summary\n\nPreregistration, and especially its peer-reviewed version as a Registered Report, is a powerful tool to reduce publication bias (the tendency to predominantly publish confirming and \"significant\" findings) and it constrains researcher degrees of freedom. With that, preregistration can substantially reduce false discoveries in the publication record, which themselves can have far-reaching consequences, often leading to theoretical claims that may misguide future research (Smaldino and McElreath 2016). Preregistration can increase the robustness of published findings. In turn, a more robust publication record allows experimental linguists to more effectively accumulate knowledge and advance their understanding of human language.\n\nAs opposed to commonly articulated concerns, preregistration is possible even if the data is already available (e.g., for corpus analyses). It also must not be considered as restricting researchers, since studies can diverge from a preregistered protocol if they are transparent about the reasons for these changes. Preregistration does not prohibit exploration. It mainly draws a visible line between confirmation and exploration. It is not an additional burden in terms of time or effort, but arguably saves time and resources in the long run. And if the reader is not convinced of preregistration yet, preregistration offers many advantages to you, the individual researcher:\n\n- Preregistration allows others to critically assess your research methods.",
    "is_useful": true,
    "question": "What is the significance of preregistration in research, particularly concerning publication bias and the robustness of findings?"
  },
  {
    "text": "Preregistration can increase the robustness of published findings. In turn, a more robust publication record allows experimental linguists to more effectively accumulate knowledge and advance their understanding of human language.\n\nAs opposed to commonly articulated concerns, preregistration is possible even if the data is already available (e.g., for corpus analyses). It also must not be considered as restricting researchers, since studies can diverge from a preregistered protocol if they are transparent about the reasons for these changes. Preregistration does not prohibit exploration. It mainly draws a visible line between confirmation and exploration. It is not an additional burden in terms of time or effort, but arguably saves time and resources in the long run. And if the reader is not convinced of preregistration yet, preregistration offers many advantages to you, the individual researcher:\n\n- Preregistration allows others to critically assess your research methods. Shortcomings in your proposed study can be detected beforehand. There might be issues with your design, your choice of sample size, your statistical model specifications, or even its translation into code. All these things can be detected not only before publication, but before data collection. This arguably avoids wasting time and resources on suboptimal empirical endeavors and leads to better attempts to empirically challenge scientific models.\n- Preregistration signals confidence. You are not afraid to submit your models to a rigorous test and you are willing to tackle possible sources of bias in a transparent way.\n- Registered Reports can protect you from CARKing (Critiquing After the Results are Known, Nosek and Lakens 2014).",
    "is_useful": true,
    "question": "What are the benefits of preregistration in research?"
  },
  {
    "text": "And if the reader is not convinced of preregistration yet, preregistration offers many advantages to you, the individual researcher:\n\n- Preregistration allows others to critically assess your research methods. Shortcomings in your proposed study can be detected beforehand. There might be issues with your design, your choice of sample size, your statistical model specifications, or even its translation into code. All these things can be detected not only before publication, but before data collection. This arguably avoids wasting time and resources on suboptimal empirical endeavors and leads to better attempts to empirically challenge scientific models.\n- Preregistration signals confidence. You are not afraid to submit your models to a rigorous test and you are willing to tackle possible sources of bias in a transparent way.\n- Registered Reports can protect you from CARKing (Critiquing After the Results are Known, Nosek and Lakens 2014). Reviewers can articulate many reasons for why the results are different from what they expected. However, if reviewers have considered the Registered Report a valid attempt to answer the research question during peer review, criticism of the method after results are known are constrained.\n- An \"in-principle\" accepted Registered Report is academic currency. It signals that your research has already gone through the quality control of peer review and has been found of sufficient quality to be published. Given the discussed biases in the publication system, an accepted Registered Report can be an easier route to publication for early career researchers than the traditional path, especially when the research scrutinizes established views.\n\nPreregistration, however, is not a panacea for all problems.",
    "is_useful": true,
    "question": "What are the benefits of preregistration for researchers in the context of improving research quality and publication outcomes?"
  },
  {
    "text": "- Registered Reports can protect you from CARKing (Critiquing After the Results are Known, Nosek and Lakens 2014). Reviewers can articulate many reasons for why the results are different from what they expected. However, if reviewers have considered the Registered Report a valid attempt to answer the research question during peer review, criticism of the method after results are known are constrained.\n- An \"in-principle\" accepted Registered Report is academic currency. It signals that your research has already gone through the quality control of peer review and has been found of sufficient quality to be published. Given the discussed biases in the publication system, an accepted Registered Report can be an easier route to publication for early career researchers than the traditional path, especially when the research scrutinizes established views.\n\nPreregistration, however, is not a panacea for all problems. There are other important practices that lead to a more robust and replicable scientific record (e.g., Chambers 2017), including incentivizing openness and transparency of the discovery process (e.g., Munaf\u00f2 et al. 2017) including data sharing (e.g., Berez-Kroeker et al. 2018), incentivizing the publication of null results (e.g., Nosek et al. 2012), direct replications (e.g., Zwaan et al. 2018), and exploratory reports (e.g., McIntosh 2017). All of these developments operate on the community level and their instantiation is arguably slow. Preregistration, however, is a practice that we can integrate into our work flow right away.",
    "is_useful": true,
    "question": "What are some practices that contribute to a more robust and replicable scientific record, apart from preregistration?"
  },
  {
    "text": "Preregistration, however, is not a panacea for all problems. There are other important practices that lead to a more robust and replicable scientific record (e.g., Chambers 2017), including incentivizing openness and transparency of the discovery process (e.g., Munaf\u00f2 et al. 2017) including data sharing (e.g., Berez-Kroeker et al. 2018), incentivizing the publication of null results (e.g., Nosek et al. 2012), direct replications (e.g., Zwaan et al. 2018), and exploratory reports (e.g., McIntosh 2017). All of these developments operate on the community level and their instantiation is arguably slow. Preregistration, however, is a practice that we can integrate into our work flow right away. There are practical advantages for individual researchers, and the pay-off for the field of linguistics is considerable.\n\nAcknowledgments: I would like to thank Jack Grieve, Melissa Kline, Lukas S\u00f6nning, Mathias Stoeber, Valentin Werner, and an anonymous reviewer for their insightful comments on earlier versions of this paper. All remaining errors are my own.\n\n## References\n\n- Allen, Chris & David M. A. Mehler. 2019. Open science challenges, benefits and tips in early career and beyond. PLoS Biology 17(5). e3000246.\n- Alsius, Agn\u00e8s, Martin Par\u00e9 & Kevin G. Munhall. 2018.",
    "is_useful": true,
    "question": "What are some important practices that contribute to a more robust and replicable scientific record besides preregistration?"
  },
  {
    "text": "All of these developments operate on the community level and their instantiation is arguably slow. Preregistration, however, is a practice that we can integrate into our work flow right away. There are practical advantages for individual researchers, and the pay-off for the field of linguistics is considerable.\n\nAcknowledgments: I would like to thank Jack Grieve, Melissa Kline, Lukas S\u00f6nning, Mathias Stoeber, Valentin Werner, and an anonymous reviewer for their insightful comments on earlier versions of this paper. All remaining errors are my own.\n\n## References\n\n- Allen, Chris & David M. A. Mehler. 2019. Open science challenges, benefits and tips in early career and beyond. PLoS Biology 17(5). e3000246.\n- Alsius, Agn\u00e8s, Martin Par\u00e9 & Kevin G. Munhall. 2018. Forty years after hearing lips and seeing voices: The McGurk effect revisited. Multisensory Research 31(1/2). 111\u2013144.\n- Ameka, Felix. 2006. Real descriptions: Reflections on native speaker and non-native speaker descriptions of a language. In Felix Ameka, Alan Dench & Nicholas Evans (eds.), Catching language: The standing challenge of grammar writing, 69\u2013112. Berlin & New York: Mouton de Gruyter.",
    "is_useful": true,
    "question": "What practice can be immediately integrated into research workflows that offers practical advantages for individual researchers and benefits the field of linguistics?"
  },
  {
    "text": "# **Identifying Participants in the Personal Genome Project by Name**\n\nLatanya Sweeney, Akua Abu, Julia Winn\n\nHarvard College\n\nCambridge, Massachusetts latanya@fas.harvard.edu, aabu@college.harvard.edu, jwinn@post.harvard.edu\n\n*We linked names and contact information to publicly available profiles in the Personal Genome Project. These profiles contain medical and genomic information, including details about medications, procedures and diseases, and demographic information, such as date of birth, gender, and postal code. By linking demographics to public records such as voter lists, and mining for names hidden in attached documents, we correctly identified 84 to 97 percent of the profiles for which we provided names.*  Our *ability to learn their names is based on their demographics, not their DNA, thereby revisiting an old vulnerability that could be easily thwarted with minimal loss of research value. So, we propose technical remedies for people to learn about their demographics to make better decisions.*\n\n#### **INTRODUCTION**\n\nThe freedom to decide with whom to share one's own medical and genomic information seems critical to protecting personal privacy in today's datarich networked society. Individuals are often in the best position to make decisions about sharing extensive amounts of personal information for many worthy purposes like research. A person can weigh harms and benefits as relevant to her own life. In comparison, decisions by policy makers and committees do not usually allow fine-grained personal distinctions, but instead dictate sweeping actions that apply the same to everyone. But how good are the decisions individuals will make?",
    "is_useful": true,
    "question": "What concerns arise regarding personal privacy when sharing medical and genomic information in the context of open science?"
  },
  {
    "text": "*  Our *ability to learn their names is based on their demographics, not their DNA, thereby revisiting an old vulnerability that could be easily thwarted with minimal loss of research value. So, we propose technical remedies for people to learn about their demographics to make better decisions.*\n\n#### **INTRODUCTION**\n\nThe freedom to decide with whom to share one's own medical and genomic information seems critical to protecting personal privacy in today's datarich networked society. Individuals are often in the best position to make decisions about sharing extensive amounts of personal information for many worthy purposes like research. A person can weigh harms and benefits as relevant to her own life. In comparison, decisions by policy makers and committees do not usually allow fine-grained personal distinctions, but instead dictate sweeping actions that apply the same to everyone. But how good are the decisions individuals will make? A person may have far less expertise than vetted committee members or veteran policy makers. And potential harms and vulnerabilities may be hidden; if so, an individual may not be able to make good decisions.\n\nFor example, sharing information about sexual abuse, abortions, or depression medication may be liberating for one person yet harmful for another. Further, if the information is shared without the explicit appearance of name or address, a person may be more likely to share the information publicly because of a false belief she is anonymous.\n\nIt is important to help people make good data sharing decisions. If people share data widely\n\nand thousands of people get subsequently harmed doing so, policy makers may respond and take away the freedom to make personal data sharing decisions, thereby depriving society of individual choice.",
    "is_useful": true,
    "question": "What is a critical aspect of open science related to sharing personal medical and genomic information?"
  },
  {
    "text": "But how good are the decisions individuals will make? A person may have far less expertise than vetted committee members or veteran policy makers. And potential harms and vulnerabilities may be hidden; if so, an individual may not be able to make good decisions.\n\nFor example, sharing information about sexual abuse, abortions, or depression medication may be liberating for one person yet harmful for another. Further, if the information is shared without the explicit appearance of name or address, a person may be more likely to share the information publicly because of a false belief she is anonymous.\n\nIt is important to help people make good data sharing decisions. If people share data widely\n\nand thousands of people get subsequently harmed doing so, policy makers may respond and take away the freedom to make personal data sharing decisions, thereby depriving society of individual choice. To make smarter decisions, people need an understanding of actual risks and ways technology can help.\n\n#### **BACKGROUND**\n\nLaunched in 2006, the Personal Genome Project (PGP) aims to sequence the genotypic and phenotypic information of 100,000 informed volunteers and display it publicly online in an extensive public database [1]. Information provided in the PGP includes DNA information, behavioral traits, medial conditions, physical characteristics, and environmental factors. A general argument for the disclosure of such information is its utility. The PGP founders believe this information will aid researchers in establishing correlations between certain traits and conducting research in personalized medicine. They also foresee its use as a tool for individuals to learn about their own genetic risk profiles for disease, uncover ancestral data, and examine biological characteristics [2].",
    "is_useful": true,
    "question": "What is the significance of individual choices in data sharing in the context of open science?"
  },
  {
    "text": "To make smarter decisions, people need an understanding of actual risks and ways technology can help.\n\n#### **BACKGROUND**\n\nLaunched in 2006, the Personal Genome Project (PGP) aims to sequence the genotypic and phenotypic information of 100,000 informed volunteers and display it publicly online in an extensive public database [1]. Information provided in the PGP includes DNA information, behavioral traits, medial conditions, physical characteristics, and environmental factors. A general argument for the disclosure of such information is its utility. The PGP founders believe this information will aid researchers in establishing correlations between certain traits and conducting research in personalized medicine. They also foresee its use as a tool for individuals to learn about their own genetic risk profiles for disease, uncover ancestral data, and examine biological characteristics [2]. According to the project's principal founder, Harvard geneticist George Church, the only real utility of this type of information is as data reflecting physical and genomic characteristics [3]. Along with Steven Pinker and Esther Dyson, he volunteered his information publicly as one of the first ten participants in the project. Currently, 2,593 individuals disclose their information publicly at the PGP website.\n\nThe PGP operates under a privacy protocol it terms \"open consent\"[4]. Individual volunteers freely choose to disclose as much personal data as they want, often including identifying demographic data, such as date of birth, gender, and postal code (ZIP). Online, the profiles appear in a \"de-identified state,\" being void of the direct appearance of the participant's name or address.",
    "is_useful": true,
    "question": "What is the purpose of the Personal Genome Project and how does it facilitate open science?"
  },
  {
    "text": "They also foresee its use as a tool for individuals to learn about their own genetic risk profiles for disease, uncover ancestral data, and examine biological characteristics [2]. According to the project's principal founder, Harvard geneticist George Church, the only real utility of this type of information is as data reflecting physical and genomic characteristics [3]. Along with Steven Pinker and Esther Dyson, he volunteered his information publicly as one of the first ten participants in the project. Currently, 2,593 individuals disclose their information publicly at the PGP website.\n\nThe PGP operates under a privacy protocol it terms \"open consent\"[4]. Individual volunteers freely choose to disclose as much personal data as they want, often including identifying demographic data, such as date of birth, gender, and postal code (ZIP). Online, the profiles appear in a \"de-identified state,\" being void of the direct appearance of the participant's name or address. The result provides volunteers with seeming anonymity and a participant is assigned an identification number as the reference to his profile. Participants may upload information directly from external DNA sequencing services\n\n(e.g., from 23andMe), but these services often provide documents having additional personal information including the participant name.\n\nPGP participants are required to sign a range of consent forms and pass an entrance exam. The consent form does not in any way guarantee participants a degree of privacy.",
    "is_useful": true,
    "question": "What are the privacy protocols and consent practices related to the sharing of personal genetic information in open science initiatives?"
  },
  {
    "text": "Currently, 2,593 individuals disclose their information publicly at the PGP website.\n\nThe PGP operates under a privacy protocol it terms \"open consent\"[4]. Individual volunteers freely choose to disclose as much personal data as they want, often including identifying demographic data, such as date of birth, gender, and postal code (ZIP). Online, the profiles appear in a \"de-identified state,\" being void of the direct appearance of the participant's name or address. The result provides volunteers with seeming anonymity and a participant is assigned an identification number as the reference to his profile. Participants may upload information directly from external DNA sequencing services\n\n(e.g., from 23andMe), but these services often provide documents having additional personal information including the participant name.\n\nPGP participants are required to sign a range of consent forms and pass an entrance exam. The consent form does not in any way guarantee participants a degree of privacy. To the contrary, the form explicitly states that participation may even reveal other non-disclosed information about the participant:\n\n\"If you have previously made available or intend to make available genetic or other medical or trait information in a confidential setting, for example in another research study, the data that you provide to the PGP may be used, on its own or in combination with your previously shared data, to identify you as a participant in otherwise private and/or confidential research. This means that any data or other information you may have shared pursuant to a promise of confidentiality or privacy may become public despite your intent that they be kept private and confidential.",
    "is_useful": true,
    "question": "What is the process for individuals who wish to disclose personal data in a public research project focusing on genetics?"
  },
  {
    "text": "Participants may upload information directly from external DNA sequencing services\n\n(e.g., from 23andMe), but these services often provide documents having additional personal information including the participant name.\n\nPGP participants are required to sign a range of consent forms and pass an entrance exam. The consent form does not in any way guarantee participants a degree of privacy. To the contrary, the form explicitly states that participation may even reveal other non-disclosed information about the participant:\n\n\"If you have previously made available or intend to make available genetic or other medical or trait information in a confidential setting, for example in another research study, the data that you provide to the PGP may be used, on its own or in combination with your previously shared data, to identify you as a participant in otherwise private and/or confidential research. This means that any data or other information you may have shared pursuant to a promise of confidentiality or privacy may become public despite your intent that they be kept private and confidential. This could result in certain adverse effects for you, including ones not considered or anticipated by this consent form\" [5]\n\nRisks mentioned by the form include public disclosure and identification and the use of personal genomic information for non-medical purposes including cloning provided cell lines. It is emphasized that all risk lies with the individual.\n\nOnce a participant uploads information to his online profile, the PGP offers almost no means to amend or modify information. Participants basically display all the contents of the profile or none at all unless they know how to edit files directly. Some of these files use complicated and unusual formats (e.g., a continuity of care report that holds the participant's personal health record).",
    "is_useful": true,
    "question": "What are the potential privacy risks associated with participating in open science genetic research? "
  },
  {
    "text": "This means that any data or other information you may have shared pursuant to a promise of confidentiality or privacy may become public despite your intent that they be kept private and confidential. This could result in certain adverse effects for you, including ones not considered or anticipated by this consent form\" [5]\n\nRisks mentioned by the form include public disclosure and identification and the use of personal genomic information for non-medical purposes including cloning provided cell lines. It is emphasized that all risk lies with the individual.\n\nOnce a participant uploads information to his online profile, the PGP offers almost no means to amend or modify information. Participants basically display all the contents of the profile or none at all unless they know how to edit files directly. Some of these files use complicated and unusual formats (e.g., a continuity of care report that holds the participant's personal health record).\n\n# **Medical Privacy Regulation: HIPAA**\n\nThe Health Information Portability and Accountability Act (HIPAA) in the United States is the federal regulation that dictates sharing of medical information beyond the immediate care of the patient, prescribing to whom and how physicians, hospitals, and insurers may share a patient's medical information broadly. Patients are exempt. A patient may share his own information with whomever and wherever he chooses, and others have already posted detailed medical and genomic information \u2013 abortions, medications, and histories of abuse and mental illness \u2013 freely and publicly online at the PGP.",
    "is_useful": true,
    "question": "What are the potential risks associated with sharing personal genomic information in an open science context?"
  },
  {
    "text": "Once a participant uploads information to his online profile, the PGP offers almost no means to amend or modify information. Participants basically display all the contents of the profile or none at all unless they know how to edit files directly. Some of these files use complicated and unusual formats (e.g., a continuity of care report that holds the participant's personal health record).\n\n# **Medical Privacy Regulation: HIPAA**\n\nThe Health Information Portability and Accountability Act (HIPAA) in the United States is the federal regulation that dictates sharing of medical information beyond the immediate care of the patient, prescribing to whom and how physicians, hospitals, and insurers may share a patient's medical information broadly. Patients are exempt. A patient may share his own information with whomever and wherever he chooses, and others have already posted detailed medical and genomic information \u2013 abortions, medications, and histories of abuse and mental illness \u2013 freely and publicly online at the PGP.\n\nFor medical data covered under HIPAA to be shared publicly beyond the control of the patient, dates would only contain the year, and the ZIP code would only have the first 2 digits if the population in the ZIP code is less than 20,000. No explicit identifiers, such as name, Social Security numbers, or addresses can appear.1\n\nIn sharp comparison, participants in the PGP often share their full date of birth (month, day and year) and the full 5 digit ZIP code, regardless of where they reside.",
    "is_useful": true,
    "question": "What are the key differences in the sharing of medical information between individual patients and participants in open science platforms like the PGP?"
  },
  {
    "text": "No explicit identifiers, such as name, Social Security numbers, or addresses can appear.1\n\nIn sharp comparison, participants in the PGP often share their full date of birth (month, day and year) and the full 5 digit ZIP code, regardless of where they reside.\n\nIn 1997, Sweeney showed how demographics appearing in medical data that did not have the names of patients could be linked to registries of people (e.g., voter lists) to restore name and contact information to the medical data [6]. Her earliest example was identifying the medical information of William Weld, former governor of Massachusetts, using only his date of birth, gender, ZIP appearing in the medical data and a voter list [7]. Sweeney also used populations reported in the U.S. Census to predict that at most 87 percent of the U.S. population had unique combinations of date of birth, gender, and ZIP [6]. Recently, others have challenged whether there really is any vulnerability to being reidentified by date of birth, gender and ZIP, citing a lack of documented examples and being confused about whether Weld was re-identified because he was targeted or because his demographics were unique [8]; begging the question to be revisited. Can people be re-identified by date of birth, gender, and 5-digit ZIP?\n\n# **METHODS**\n\nThe database we used was 1130 public profiles copied from the PGP website [1] on September 1, 2011, which were all the public profiles available at the PGP at that time.",
    "is_useful": true,
    "question": "What are the potential risks associated with the re-identification of individuals from anonymized medical data?"
  },
  {
    "text": "Sweeney also used populations reported in the U.S. Census to predict that at most 87 percent of the U.S. population had unique combinations of date of birth, gender, and ZIP [6]. Recently, others have challenged whether there really is any vulnerability to being reidentified by date of birth, gender and ZIP, citing a lack of documented examples and being confused about whether Weld was re-identified because he was targeted or because his demographics were unique [8]; begging the question to be revisited. Can people be re-identified by date of birth, gender, and 5-digit ZIP?\n\n# **METHODS**\n\nThe database we used was 1130 public profiles copied from the PGP website [1] on September 1, 2011, which were all the public profiles available at the PGP at that time. About half of the profiles, 579 of 1130 or 51 percent, had date of birth, gender and 5-digit ZIP. Unless otherwise noted or is obvious from context, these 579 profiles comprise the base dataset for analysis (\"Dataset\").\n\n# **Experiment: Linking Demographics**\n\nWe conducted an experiment to determine how many of the profiles in Dataset we could reidentify by name using public records. Our sources for public records was a national sample of voter registrations (\"Voter Data\") and online access to a public records website (\"Public Records\"). The voter data was purchased from a third-party data broker and contained a sample of voter registrations for the 5-digit ZIP codes listed in Dataset.",
    "is_useful": true,
    "question": "What demographic factors are suggested to potentially allow for the re-identification of individuals in public datasets?"
  },
  {
    "text": "# **METHODS**\n\nThe database we used was 1130 public profiles copied from the PGP website [1] on September 1, 2011, which were all the public profiles available at the PGP at that time. About half of the profiles, 579 of 1130 or 51 percent, had date of birth, gender and 5-digit ZIP. Unless otherwise noted or is obvious from context, these 579 profiles comprise the base dataset for analysis (\"Dataset\").\n\n# **Experiment: Linking Demographics**\n\nWe conducted an experiment to determine how many of the profiles in Dataset we could reidentify by name using public records. Our sources for public records was a national sample of voter registrations (\"Voter Data\") and online access to a public records website (\"Public Records\"). The voter data was purchased from a third-party data broker and contained a sample of voter registrations for the 5-digit ZIP codes listed in Dataset. We estimate the sampling fraction in the Voter Data to be 72% based on a comparison of totals reported by state officials in one state (Massachusetts).\n\n<sup>1 45</sup> CFR 164.514(b)(2) (2002).\n\nThe approach involved matching demographics (date of birth, gender, ZIP) appearing in the PGP profiles of Dataset to those appearing in Voter Data or Public Records, and recording how many of the matches yielded just one name. Figure 1 shows a graphical depiction of our approach.\n\n![](_page_2_Figure_3.jpeg)\n\n**Figure 1.",
    "is_useful": true,
    "question": "What methodology was used to determine the reidentification of profiles in a public dataset using public records?"
  },
  {
    "text": "Our sources for public records was a national sample of voter registrations (\"Voter Data\") and online access to a public records website (\"Public Records\"). The voter data was purchased from a third-party data broker and contained a sample of voter registrations for the 5-digit ZIP codes listed in Dataset. We estimate the sampling fraction in the Voter Data to be 72% based on a comparison of totals reported by state officials in one state (Massachusetts).\n\n<sup>1 45</sup> CFR 164.514(b)(2) (2002).\n\nThe approach involved matching demographics (date of birth, gender, ZIP) appearing in the PGP profiles of Dataset to those appearing in Voter Data or Public Records, and recording how many of the matches yielded just one name. Figure 1 shows a graphical depiction of our approach.\n\n![](_page_2_Figure_3.jpeg)\n\n**Figure 1.** Linking PGP profile to a voter list using demographics to put names to the medical and genomic content appearing in the PGP profile.\n\n## **Experiment: Extracting Embedded Names**\n\nWe wrote programs in the Python and Java programming languages to download files from the publicly accessible PGP website; these were public profiles and other downloadable files associated with profiles. Some personal names appeared within downloaded documents. Our programs automatically extracted demographic information from the profile and names appearing in the filenames of compressed files that downloaded. From observation, we learned that many of the downloadable DNA files associated with PGP profiles were compressed, but once the file was uncompressed, the resulting file had a filename that included the name of the person as part of the filename.",
    "is_useful": true,
    "question": "What methods can be used to link personal genomic information to public voter registration data in open science research?"
  },
  {
    "text": "From observation, we learned that many of the downloadable DNA files associated with PGP profiles were compressed, but once the file was uncompressed, the resulting file had a filename that included the name of the person as part of the filename. As a made-up example, the profile *hx0157A* may have a downloadable DNA file named *hx0157A_8659862.zip*, which when uncompressed gives the file having the name\n\n*genome_Elaine_Smith_Full_629562.txt*.\n\n## **RESULTS**\n\nNumerous tests were conducted on Dataset to demonstrate the ability to reliably put names to the profiles. Linking demographics in Dataset to those in the Voter Data yielded 130 (130 of 579 or 22 percent) unique matches. Our programs for finding embedded names located 103 (or 18 percent) names. Searches of the demographics from Dataset on a public records website yielded 156 (or 27 percent) unique matches. Combining these results gave a list of 241 (or 42 percent) unique names matching profiles. These were submitted to the PGP staff and scored: **84 percent correctly matched**, being as high as **97 percent if allowing considerations for possible nicknames** (e.g. Jim instead of James).\n\nTable 1 describes the contributions of each strategy. Embedded names contributed 74 names not accounted for by the other strategies, linking to Voter Data contributed 44 distinct names and to Public Records, 65 names that would not otherwise have been known. Embedded names had 12 names in common with Voter Data and 17 with Public Records.",
    "is_useful": true,
    "question": "What methods were used to match names to PGP profiles in a dataset study?"
  },
  {
    "text": "** Discrimination of strategies. Values report the number of names specific to the strategy (e.g., embedded names contributing 74 names not otherwise found) or in common across strategies (e.g., 17 names found in both embedded names and Public Records).\n\n|  | Wrong | Total | Correct% |\n| --- | --- | --- | --- |\n| Name | 19 | 103 | 82% |\n| Voter Data | 9 | 130 | 93% |\n| Public Records | 20 | 156 | 87% |\n\n<sup><</sup>b>Table 2. Correctness of different re-identification strategies. Errors in matching embedded names and other strategies are due primarily to uses of nicknames rather than real names.\n\n#### **DISCUSSION**\n\nThese experiments demonstrate how PGP profiles are vulnerable to re-identification. What's the potential harm? Many participants reveal more than DNA, including seemingly sensitive conditions (e.g., profiles hu342A08, hu6D1115, and hu56B3B6) \u2013abortions, sexual abuse, illegal drug use, alcoholism, clinical depression and more.\n\nPerhaps more alarming are potential economic harms a participant may face. Here is an example. Suppose a hypothetical participant named Bob has a predisposition to a gene-based disease\n\nrelated to his genetic profile online. He applies for life insurance. If Bob is aware of the predisposition and discloses the information, he may be denied coverage or asked to pay a much higher premium.",
    "is_useful": true,
    "question": "What are the risks associated with re-identification of profiles in open science, particularly regarding personal health information?"
  },
  {
    "text": "Correctness of different re-identification strategies. Errors in matching embedded names and other strategies are due primarily to uses of nicknames rather than real names.\n\n#### **DISCUSSION**\n\nThese experiments demonstrate how PGP profiles are vulnerable to re-identification. What's the potential harm? Many participants reveal more than DNA, including seemingly sensitive conditions (e.g., profiles hu342A08, hu6D1115, and hu56B3B6) \u2013abortions, sexual abuse, illegal drug use, alcoholism, clinical depression and more.\n\nPerhaps more alarming are potential economic harms a participant may face. Here is an example. Suppose a hypothetical participant named Bob has a predisposition to a gene-based disease\n\nrelated to his genetic profile online. He applies for life insurance. If Bob is aware of the predisposition and discloses the information, he may be denied coverage or asked to pay a much higher premium. If he does not disclose knowledge of the predisposition or if he is not aware of the predisposition, the insurance company may fail to pay the claim upon his death. While the Genetic Information Non-Discrimination Act of 2008 (GINA) protects against some forms of discrimination (e.g. medical insurance), it does not cover all forms (e.g. life insurance).\n\nGiven the earlier discussion, we might have predicted being able to match unique names to more than 42 percent of the profiles. There are several possible explanations. The first is the temporal mismatch in the data described earlier.",
    "is_useful": true,
    "question": "What are the potential harms of re-identification strategies related to personal profiles in open science contexts?"
  },
  {
    "text": "By making these values less specific, it becomes harder to link his name to the profile. Also, she can remove her name from appearing explicitly in documents she uploads.\n\nTo achieve these precautions required intervention on our part. We provide two technical services to PGP participants.\n\nSweeney constructed a website for a person to determine how unique his demographics may be (and therefore how easy it is to identify him) from his ZIP code, date of birth, and gender.2 Anyone can check his or her demographics, even if not a participant in the PGP.\n\nAs stated earlier, the PGP itself does not support editing of the date of birth field, though participants can modify ZIP codes directly. So, we built a CCR (Continuity of Care Record) editor3 for a participant to change his date of birth to report only year of birth or remove it altogether in the CCR file he uploads to the PGP.\n\nUsing knowledge from this study and associated services, individuals can make better data sharing decisions, or at least be more informed of risks and society can learn that date of birth, gender and 5-digit ZIP codes can be uniquely identifying. 4\n\n## Acknowledgments\n\nThe authors gratefully thank Sean Hooley for help validating and scoring findings, participants in the Personal Genome Project (PGP) for being brave participants, and George Church, Jason Bobe and Madeline Ball for hosting the PGP. Our access to the PGP was limited to publicly available information.",
    "is_useful": true,
    "question": "What precautions can individuals take to protect their privacy when participating in open science initiatives like the Personal Genome Project?"
  },
  {
    "text": "Our access to the PGP was limited to publicly available information. Julia Winn and Akua Abu did original re-identifications as class projects in the CS105 Privacy and Technology course at Harvard, taught by Jim Waldo and Latanya Sweeney. A copy of all data used in this study is available at foreverdata.org and the Dataverse Network. This work has been supported in part by a National Institutes of Health Grant (1R01ES021726).\n\n#### References\n\n- 1. Personal Genome Project.\n- www.personalgenomes.org/community.html 2. Public Genomes. NOVA scienceNOW. August 18, 2009.\n\nwww.pbs.org/wgbh/nova/body/publicgenomes.html\n\n- 3. Goetz T. How the Personal Genome Project could unlock the mysteries of life. Wired 16.08 July 26, 2008.\nhttp://www.wired.com/medtech/stemcells/maga zine/16-08/ff_church?currentPage=all\n\n- 4. Lunshof J, Chadwick R and Church G. From genetic privacy to open consent. Nature Reviews Genetics. April 2008; doi:10.1038/nrg2360\n- 5. Consent Form. Personal Genome Project. http://www.personalgenomes.org/consent/PGP_ Consent_Approved_02212012.pdf\n- 6. Sweeney L. *Simple Demographics Often Identify People Uniquely*.",
    "is_useful": true,
    "question": "What platforms provide access to the data used in the study related to the Personal Genome Project?"
  },
  {
    "text": "I have long since forgotten what a Ramsey Sentence is, and I doubt if fifty people in the world besides Paul and, perhaps, Ramsey himself think the concept is exciting. But Meehl had those students on the edge of their seats, unwilling to leave until they had it whole.\n\nThe present paper is a distillation of the three lectures I have been contributing to Paul's Philosophical Psychology. I offer it here in fond respect for the man who has been my teacher and friend for nearly forty years.\n\nI shall argue the following theses:\n\n- (I) Psychology isn't doing very well as a scientific discipline and something seems to be wrong somewhere.\n- (II) This is due partly to the fact that psychology is simply harder than physics or chemistry, and for a variety of reasons. One interesting reason is that people differ structurally from one another and, to that extent, cannot be understood in terms of the same theory since theories are guesses about structure.\n- (III) But the problems of psychology are also due in part to a defect in our research tradition; our students are carefully taught to behave in the same obfuscating, self-deluding, pettifogging ways that (some of) their teachers have employed.\n\nHaving made this diagnosis, I will suggest some home remedies, some ways in which the next generation could pull up its socks and do better than its predecessors have done. Along the way I shall argue that research is overvalued in the Academy and that graduate students should not permit themselves to be bullied into feeling bad about the fact that most of them will never do any worthwhile research.",
    "is_useful": true,
    "question": "What challenges does psychology face as a scientific discipline according to the author's argument?"
  },
  {
    "text": "I think this attitude is shortsighted. By taking a frank look at ourselves and making an honest assessment of our symptoms and defects, it is possible, I think, to see some of the apparent and correctable reasons for these problems.\n\n# **I. Something Is Wrong with the Research Tradition in Psychology**\n\nIt is instructive to attempt to follow the progress of a research idea from its germination in the mind of a psychological scientist until it finally flowers (if it ever does) within the pages of an archival journal. If the budding idea seems to its parent to be really promising, the almost invariable first step is to write it up in the form of a grant application directed most commonly to one of the federal agencies. Writing grant applications is laborious and time-consuming, and there is no doubt that many research ideas begin to seem less viable during the course of this process and are aborted at this early stage.\n\n# A. *Most Grant Applications Are Bad*\n\nApplications directed to the National Institute of Mental Health are routed to an appropriate Research Review committee consisting of 10 or 12 established investigators with broadly similar interests who meet for several days three times each year to consider submissions and make recommendations for funding. Although all committee members are nominally expected to read the entire set of applications (and a few probably do this), the review committees depend largely on the reports of those two or three members who have been assigned principal responsibility for the given proposal.",
    "is_useful": true,
    "question": "What challenges are associated with the research tradition in psychology as it pertains to the grant application process?"
  },
  {
    "text": "If the budding idea seems to its parent to be really promising, the almost invariable first step is to write it up in the form of a grant application directed most commonly to one of the federal agencies. Writing grant applications is laborious and time-consuming, and there is no doubt that many research ideas begin to seem less viable during the course of this process and are aborted at this early stage.\n\n# A. *Most Grant Applications Are Bad*\n\nApplications directed to the National Institute of Mental Health are routed to an appropriate Research Review committee consisting of 10 or 12 established investigators with broadly similar interests who meet for several days three times each year to consider submissions and make recommendations for funding. Although all committee members are nominally expected to read the entire set of applications (and a few probably do this), the review committees depend largely on the reports of those two or three members who have been assigned principal responsibility for the given proposal. The Institute gets good value from these peer review committees whose members, not wishing to appear foolish or uninformed before their peers at the tri-annual meetings, invest many (uncompensated) hours before each meeting studying their assigned subset of applications and composing well-considered critiques and recommendations. At the meetings, proposals are carefully discussed and evaluated before the committee votes. Of all the applications received by NIMH in a given year, only about 25% are considered promising enough to be actually funded.\n\n### *B. Most Manuscripts Submitted to the Journals Are Bad*\n\nArchival scientific journals also depend upon the peer review system.",
    "is_useful": true,
    "question": "What role does peer review play in the evaluation of grant applications and manuscripts in research funding and publication?"
  },
  {
    "text": "Although all committee members are nominally expected to read the entire set of applications (and a few probably do this), the review committees depend largely on the reports of those two or three members who have been assigned principal responsibility for the given proposal. The Institute gets good value from these peer review committees whose members, not wishing to appear foolish or uninformed before their peers at the tri-annual meetings, invest many (uncompensated) hours before each meeting studying their assigned subset of applications and composing well-considered critiques and recommendations. At the meetings, proposals are carefully discussed and evaluated before the committee votes. Of all the applications received by NIMH in a given year, only about 25% are considered promising enough to be actually funded.\n\n### *B. Most Manuscripts Submitted to the Journals Are Bad*\n\nArchival scientific journals also depend upon the peer review system. The editors of most psychological journals do a preliminary screening, returning at once those manuscripts that are the most obviously unacceptable, and then send out the remainder to two or more referees selected for their expertise on the topic of the given paper. Like most academic psychologists of my advanced age, I have refereed hundreds of papers for some 20 journals over the years and can attest that it is a dispiriting business. My reviews tended to be heavily burdened with sarcasm evoked by the resentment I felt in having to spend several hours of my time explicating the defects of a paper which one could see in the first ten minutes' reading had no hope of contributing to the sum of human knowledge.",
    "is_useful": true,
    "question": "What role does the peer review system play in the evaluation of scientific manuscripts and proposals?"
  },
  {
    "text": "Of all the applications received by NIMH in a given year, only about 25% are considered promising enough to be actually funded.\n\n### *B. Most Manuscripts Submitted to the Journals Are Bad*\n\nArchival scientific journals also depend upon the peer review system. The editors of most psychological journals do a preliminary screening, returning at once those manuscripts that are the most obviously unacceptable, and then send out the remainder to two or more referees selected for their expertise on the topic of the given paper. Like most academic psychologists of my advanced age, I have refereed hundreds of papers for some 20 journals over the years and can attest that it is a dispiriting business. My reviews tended to be heavily burdened with sarcasm evoked by the resentment I felt in having to spend several hours of my time explicating the defects of a paper which one could see in the first ten minutes' reading had no hope of contributing to the sum of human knowledge. I became troubled by the fact that it was possible for me thus to assault the author's *amour propre* from the safety of the traditional anonymity of journal referees, and I began to sign my reviews and have done so unfailingly these past 15 years or so. While I continue to be critical, I find that I am very careful to be sure of the grounds for my comments, knowing that the author will know who is talking.",
    "is_useful": true,
    "question": "What percentage of applications received by the NIMH in a given year are considered promising enough to be funded?"
  },
  {
    "text": "While I continue to be critical, I find that I am very careful to be sure of the grounds for my comments, knowing that the author will know who is talking. It seems to me, in this age of accountability, that authors *ought* to know who has said what about their work and, moreover, that journal readers ought to be able to learn in a footnote which editor or reviewers decided that any given article should have been published.\n\nIn any case, whether the reviews are signed or not, the effect of this peer review process is that from 60 to 90% of articles submitted to journals published by the American Psychological Association are rejected.\n\n#### *C. Most Actually Published Research Is Bad*\n\nIn their 1970 *Annual Review* chapter on Memory and Verbal Learning, Tulving and Madigan reported that they had independently rated each of 540 published articles in terms of its \"contribution to knowledge.\" With \"remarkable agreement,\" they found that they had sorted two-thirds of the articles into a category labeled:\n\n\"utterly inconsequential.\" The primary function these papers serve is to give something to do to people who count papers instead of reading them. Future research and understanding of verbal learning and memory would not be affected at all if none of the papers in this category had seen the light of day. (Tulving & Madigan, 1970, p. 441)\n\nAbout 25 percent of the articles were classified as:\n\n\"run-of-the-mill\" . . . these articles also do not add anything really new to knowledge . . .",
    "is_useful": true,
    "question": "What are the key concerns regarding the quality and accountability in the publication of scholarly articles?"
  },
  {
    "text": "#### *C. Most Actually Published Research Is Bad*\n\nIn their 1970 *Annual Review* chapter on Memory and Verbal Learning, Tulving and Madigan reported that they had independently rated each of 540 published articles in terms of its \"contribution to knowledge.\" With \"remarkable agreement,\" they found that they had sorted two-thirds of the articles into a category labeled:\n\n\"utterly inconsequential.\" The primary function these papers serve is to give something to do to people who count papers instead of reading them. Future research and understanding of verbal learning and memory would not be affected at all if none of the papers in this category had seen the light of day. (Tulving & Madigan, 1970, p. 441)\n\nAbout 25 percent of the articles were classified as:\n\n\"run-of-the-mill\" . . . these articles also do not add anything really new to knowledge . . . [such articles] make one wish that at least some writers, faced with the decision of whether to publish or perish, should have seriously considered the latter alternative, (p. 442)\n\nOnly about 10 percent of the entire set of published papers received the modest compliment of being classified as \"worthwhile.\" Given that memory and verbal learning was then a popular and relatively 'hard' area of psychological research, attracting some of the brightest students, this is a devastating assessment of the end product.",
    "is_useful": true,
    "question": "What percentage of published research articles in the study were deemed to provide a significant contribution to knowledge?"
  },
  {
    "text": "442)\n\nOnly about 10 percent of the entire set of published papers received the modest compliment of being classified as \"worthwhile.\" Given that memory and verbal learning was then a popular and relatively 'hard' area of psychological research, attracting some of the brightest students, this is a devastating assessment of the end product.\n\nHence, of the research ideas generated by these psychologists, who are all card-carrying scientists and who liked these ideas well enough to invest weeks or months of their lives working on them, less than 25% of 40% of 10% = 1% actually appear to make some sort of contribution to the discipline.\n\n## *D. Most Published Articles Are Not Read Anyway*\n\nGarvey and Griffith (1963) found that about half the papers published in APA journals have fewer than 200 readers (not all of whom are edified). Two-thirds of these papers are never cited by another author. Somewhat surprisingly, the same thing is true even in physics: Cole and Cole (1972) found that half the papers in physics journals are never cited. Even articles in *Physical Review,* generally considered one of the most prestigious journals, do not always make much of a splash; 50% are cited once or never during the three years after they appear.\n\nWhen he was at Minnesota years ago, B. F. Skinner used to say that he avoided reading the literature since it only \"poisons the mind.\" In psychology, what other researchers are doing is seldom useful to one's self except perhaps as something to refute or, more rarely, as a bandwagon to climb up on.",
    "is_useful": true,
    "question": "What percentage of published papers are considered worthwhile and contribute to their disciplines?"
  },
  {
    "text": "One does not have to actually read the literature until it is time to start writing one's paper. Lindsey (1978) and Watson (1982) have cited the long publication lags typical of social science journals as evidence that psychologists do not need to know what their colleagues are doing; we do not fear being 'scooped' because it is so unlikely that anyone else would be prospecting in the same area.\n\n## *E. Theories in Psychology Are Like Old Soldiers: They Are Not Refuted or Replaced\u2014They Don't Die \u2014They Only Fade Away*\n\nLike a good scientific theory, this simile of Paul Meehl's has sufficient verisimilitude to continue to be useful. The exciting theoretical developments of my student days\u2014the work of Hull, Spence, and Tolman, to focus just on one thenactive area\u2014have sunk into obscurity. In the hard sciences, each generation stands upon the shoulders of its predecessors, the bones of the Elder Giants become part of the foundation of an ever-growing edifice. The great names of psychology's comparatively recent past are respected mainly as intrepid explorers who came back empty-handed. There is no edifice, just this year's ant hill, most of which will be abandoned and washed away in another season.\n\nIn the 1940s and '50s, there was a torrent of interest and research surrounding the debate between the S-R reinforcement theorists at Yale and Iowa City and the S-S expectancy theorists headquartered at Berkeley.",
    "is_useful": true,
    "question": "How do publication lags in social science journals affect the way researchers view their colleagues' work?"
  },
  {
    "text": "But the range or scope of the field is very great so that there will be a majority of people at every APA convention with whom I share few if any scientific interests.\n\n## *F. Research in Psychology Does Not Tend to Replicate*\n\nCharles Darwin once pointed out that, while false theories do relatively little harm, false facts can seriously retard scientific progress. As Mark Twain put it, somewhere, it is not so much what we don't know that hurts us, as those things we do know that aren't so. Weiner and Wechsler (1958), in a similar vein, remark that \"the results that are the most difficult to explain are the ones that are not true\" (p. ix). Every mature psychologist knows from experience that it is foolish to believe a new result merely on the basis of the first published study, especially if the finding seems unusually important or provocative. Within the narrow circles of our particular fields of interest, many of us learn that there are certain investigators who stand out from the herd because their findings can be trusted.\n\nThere is a lot of talk currently about actual dishonesty in research reporting. We were all quite properly scandalized by the Cyril Burt affair when his official biographer concluded that at least most of the subjects in Burt's widely cited study of monozygotic twins reared apart were as fictitious as the two female collaborators whose names Burt signed to his reports of this alleged research (Hearnshaw, 1979).",
    "is_useful": true,
    "question": "What is the impact of false facts on scientific progress, according to the perspective shared in the context of psychological research?"
  },
  {
    "text": "Weiner and Wechsler (1958), in a similar vein, remark that \"the results that are the most difficult to explain are the ones that are not true\" (p. ix). Every mature psychologist knows from experience that it is foolish to believe a new result merely on the basis of the first published study, especially if the finding seems unusually important or provocative. Within the narrow circles of our particular fields of interest, many of us learn that there are certain investigators who stand out from the herd because their findings can be trusted.\n\nThere is a lot of talk currently about actual dishonesty in research reporting. We were all quite properly scandalized by the Cyril Burt affair when his official biographer concluded that at least most of the subjects in Burt's widely cited study of monozygotic twins reared apart were as fictitious as the two female collaborators whose names Burt signed to his reports of this alleged research (Hearnshaw, 1979). But the problem of the unreplicability of so many findings in the psychological literature involves something more subtle and more difficult to deal with than deliberate chicanery. In amost every study, the investigator will have hoped to find a certain pattern of results, at the very least an orderly, selfconsistent pattern of results. The processes of planning, conducting, and analyzing any psychological experiment are complicated, frequently demanding decisions that are so weakly informed by any ancillary theory or established practice as to seem essentially arbitrary.",
    "is_useful": true,
    "question": "What challenges are associated with the credibility and replicability of research findings in psychology?"
  },
  {
    "text": "There is a lot of talk currently about actual dishonesty in research reporting. We were all quite properly scandalized by the Cyril Burt affair when his official biographer concluded that at least most of the subjects in Burt's widely cited study of monozygotic twins reared apart were as fictitious as the two female collaborators whose names Burt signed to his reports of this alleged research (Hearnshaw, 1979). But the problem of the unreplicability of so many findings in the psychological literature involves something more subtle and more difficult to deal with than deliberate chicanery. In amost every study, the investigator will have hoped to find a certain pattern of results, at the very least an orderly, selfconsistent pattern of results. The processes of planning, conducting, and analyzing any psychological experiment are complicated, frequently demanding decisions that are so weakly informed by any ancillary theory or established practice as to seem essentially arbitrary. As the investigator makes his or her way through this underbrush, there is the ever-beckoning lure of the desired or expected outcome that tends to influence the choices made at each step.\n\nSelective error-checking is perhaps the simplest and most innocent example of the problem. If the worst sin researchers committed was to re-score or recalculate results that come out 'wrong,' while accepting at once those results that fit with expectations, a significant number of unreplicable findings would appear in the journals. To illustrate some of the subtler sources of distortion, let us consider a couple of real-life examples (see also Gould, 1978).",
    "is_useful": true,
    "question": "What challenges are associated with ensuring the replicability of findings in psychological research?"
  },
  {
    "text": "When World War I broke out, the National Research Council appointed a committee to assess the validity of Marston's test as a possible aid in the interrogation of suspected spies. The committee consisted of L. T. Troland of Harvard, H. E. Burtt of Ohio State, and Marston himself. According to Marston (1938), a total of 100 criminal cases were examined in the Boston criminal court and the systolic blood pressure test led to correct determinations in 97 of the 100 cases.\n\nMarston later invented the comic-strip character \"Wonder Woman,\" with her magic lasso that makes men tell the truth. During the 1930s his picture was to be found in full-page magazine advertisements using the lie detector to \"prove\" that Gillette blades shave closer and more comfortably. For these reasons, we might be skeptical of Marston's scientific claims. But Troland and Burtt were respected psychologists, and Father Walter Summers, chair of the Psychology Department at Fordham, was not a man to be suspected of exaggeration. Summers (1939) invented a lie detector based on an entirely different principle and claimed that his method had proved 100% accurate on a long series of criminal cases. But both Marston and Summers were wrong. Neither method has been taken seriously during the last 50 years and both of the \"specific lie responses\" they claimed to have discovered are commonly shown by innocent people while truthfully denying false accusations. It is impossible now to discover how it was that the hopes of these enthusiastic investigators became transmuted into false proofs.",
    "is_useful": true,
    "question": "What challenges does the field of lie detection face in establishing reliable methods for determining truthfulness?"
  },
  {
    "text": ". . Shouldn't we expect by now something more satisfying than [this] welter of diverse and contradictory opinions? . . . Where are indications of cumulative gains of research, converging lines of evidence, and generally accepted definitions, concepts, and formulations? (Jensen, 1987, pp. 193-194)\n\nOne of the central concepts of psychology \u2014the paradigmatic concept of differential psychology \u2014is intelligence, a topic of great theoretical and practical interest and research for more than a century, the only psychological trait that can boast its own exclusive journal. Yet, in 1987, the leading modern student of intelligence finds it necessary to lament the lack of real cumulative progress in that core area.\n\nSuppose that with some magic Time Machine we could transport Linus Pauling back to the day in 1925 when he had his final oral examination for the Ph.D. in Chemistry at Cal Tech. Our Time Machine will restore his youthful vigor but will permit him to retain all the new things that he has learned, through his own research and that of others, in the 60-plus years since he was examined for his doctorate. Imagine the wonders with which he could regale his astonished professors! Many of the most important developments\u2014the quantum theoretical aspects, for example \u2014 would be beyond their understanding. Just a partial description of the technology that is now available in the chemical laboratory would be likely to induce ecstatic seizures in at least some committee members. Those professors of the flapper era would look upon their bright-eyed student as if he were a visitor from some advanced civilization on another planet\u2014as indeed he would be.",
    "is_useful": true,
    "question": "What challenges does the field of psychology face in achieving cumulative progress in research, particularly regarding intelligence?"
  },
  {
    "text": "But it was the professionals who had managed to convince themselves of such odd notions in the first place\u2014their neighbors would have known better. I am sure that each of you could, with some effort, generate a short list of more interesting and solid findings (my own list, not surprisingly, would include some of my own work), but it is a depressing undertaking because one's list compares so sadly with that of any chemist, physicist, or astronomer. Can we blame it on our youth? Think of the long list of astonishing discoveries produced in our coeval, genetics, with just a fraction of our person-power.\n\n## *H. Cargo-Cult Science*\n\nIn his lively autobiography, the late Nobel laureate Richard Feynman (1986) expressed the view that much of psychological research is \"Cargo-cult science\":\n\nIn the South Seas there is a cargo cult of people. During the war, they saw airplanes land with lots of good materials, and they want the same thing to happen now. So they've arranged to make things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit on, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas\u2014he's the controller\u2014and they wait for the airplanes to land. They're doing everything right. The form is perfect. It looks just the way it looked before. But it doesn't work. No airplanes land.",
    "is_useful": true,
    "question": "What term is used to describe the phenomenon where practitioners follow a ritualized approach to research without understanding the underlying scientific principles, as illustrated by comparisons to a cargo cult?"
  },
  {
    "text": "## *H. Cargo-Cult Science*\n\nIn his lively autobiography, the late Nobel laureate Richard Feynman (1986) expressed the view that much of psychological research is \"Cargo-cult science\":\n\nIn the South Seas there is a cargo cult of people. During the war, they saw airplanes land with lots of good materials, and they want the same thing to happen now. So they've arranged to make things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit on, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas\u2014he's the controller\u2014and they wait for the airplanes to land. They're doing everything right. The form is perfect. It looks just the way it looked before. But it doesn't work. No airplanes land. So I call these things cargo cult science, because they follow all the apparent precepts and forms of scientific investigation, but they're missing something essential, because the planes don't land.\n\n## *Summary*\n\nIt is hard to avoid the conclusion that psychology is a kind of shambling, poor relation of the natural sciences. As the example of genetics shows us, we cannot reasonably use our relative youth as an excuse\u2014and at age 100 we are a little long in the tooth to claim that with a straight face anyway. Psychologists in the American Association for the Advancement of Science have been trying recently to get *Science* to publish a psychological article now and then.",
    "is_useful": true,
    "question": "What concept is used to describe scientific practices that closely mimic established methods but lack a fundamental component necessary for true scientific inquiry?"
  },
  {
    "text": "Psychologists in the American Association for the Advancement of Science have been trying recently to get *Science* to publish a psychological article now and then. The editors reply that they get lots of submissions from psychologists but they just are not as interesting as all the good stuff they keep getting from the biochemists, the space scientists, the astronomers, and the geneticists.\n\nMoreover, *Science,* like its British counterpart, *Nature,* is a relatively fastpublication journal where hot, new findings are published, findings that are of general interest and that other workers in the field will want to know about promptly. But psychologists seldom have anything to show and tell that other psychologists need to know about promptly. We are each working in a different part of the forest, we are not worried that someone else will publish first, and we do not need to know what others have found because ours is not a vertical enterprise, building on what has been discovered previously.\n\nMost of us realize that we do not really have to dig into the journals until we are ready to write up our own work for publication and need some citations to make our results seem more relevant and coherent. Our theories have a short half-life and they just die in the larval stage instead of metamorphosing into something better. Worse yet, our experiments do not replicate very well and so it is hard to be sure what to theorize about.\n\n## **II. Why? What Has Gone Wrong with Psychology's Research Tradition?**\n\n## *A. Are Psychologists Dumber Than Physicists or Biologists?",
    "is_useful": true,
    "question": "What challenges does the field of psychology face in gaining recognition and publication in leading scientific journals?"
  },
  {
    "text": "Scientists must be able to idealize and oversimplify, to escape from the particular to the general and, often, to sneak up on a complicated problem by successive approximations. The atomic model of Thompson and Bohr, for example, served physics well for many years and probably made possible the new knowledge and the new concepts which ultimately proved that model to be grossly oversimplified. If Thompson and Bohr had known some of what is now known about leptons and quarks and so on, if they had been required to operate in the murk of all these forbidding complexities, they might never have been able to make any progress. The same thing is true in biology. It was important for nineteenth-century biologists to be able to think of the cell as a simple little basic unit of protoplasm with relatively simple functions and properties. If they had been forced to realize that the average cell does more complicated chemistry every day than the Dupont Corporation, it might have been very inhibiting.\n\nWhen one looks at the heavens on a clear night, it is interesting to contemplate the fact that only a few hundred stars are visible to the naked eye at any given time and place, only about 6,000 in the entire celestial sphere. Moreover, only a few really bright stars are present and they combine in our perception as the great constellations. The constancy of shape of these starry patterns and their regular apparent movement from east to west was the beginning of astronomy.",
    "is_useful": true,
    "question": "How do simplification and idealization play a role in scientific progress and understanding?"
  },
  {
    "text": "Because of differences in temperament and native ability, Bill eschews most vicarious experience in favor of active adventure outdoors; Bob is fascinated by science fiction and later by real science; George is addicted to adventure programs; Paul, who is precocious, discovers pornography. What began as mere parametric differences must often lead to real differences in structure. Since human nature is so obviously complicated, perhaps the most we can reasonably hope for is that the varieties of human software packages will be classifiable into a manageable number of homeomorphic types within each of which some rules hold that do not hold across groups or types. (And it is relevant to note that we now have powerful analytic methods for detecting latent taxa in psychometric data, viz., Meehl & Golden, 1982.)\n\n## **Summary of the Structural vs. Parametric Variation Issue**\n\nAll sciences have as their objects of study collections of entities or systems, and the job of the science is to describe the behavior of these entities and, ultimately, to develop a theory about the structure of the different types of entities so that their behavior can be deduced from the theory of their structure. This job is relatively easier when the entities are all structurally alike, or when they can be sorted into classes or types within which there is structural similarity. Thus, all atoms of a given isotope of any element are structurally alike; thus, one microtheory fits all exemplars of a given isotope and, moreover, one macrotheory contains the features common to all the microtheories.",
    "is_useful": true,
    "question": "How do different approaches in the study of entities within a science facilitate the development of theories about their behavior and structure?"
  },
  {
    "text": "We must simply keep trying and find out how far we can go.\n\n#### *D. Psychology So Far Has Lacked Good Paradigms*\n\nWe talked earlier about the long publication lags in social science journals and the suggestion that we countenance this because we are all digging in separate places on the beach, looking for different things; we do not need to know how anyone else is doing \u2014or what they are doing \u2014and we do not fear that anyone else will scoop us because we know that no one else is hunting where we are or for the same thing \u2014i.e., we lack paradigms. In gold mining, a paradigm consists in the discovery of a deposit, a seam, so that people can get to work, all the technicians who know how to dig, to timber a tunnel, to build sluices for treating the ore, and so on.\n\nHeinrich Schliemann was a paradigm maker; he figured out where to dig for the ruins of the ancient city of Troy. Based on his pathfinding, an army of archaeologists could start doing useful work, had whole careers laid out before them. Many good doctoral dissertations were made possible by Schliemann's essential first steps. It is important to understand that just having the tools for research, for digging, is not enough. You can be smart and well trained, brighteyed and bushy-tailed, but if you do not know where to dig, you may end up in a dry hole or a mud hole. The hot paradigms currently are, of course, in molec-\n\nular biology.",
    "is_useful": true,
    "question": "What is the importance of having paradigms in scientific research?"
  },
  {
    "text": "The hot paradigms currently are, of course, in molec-\n\nular biology. Any psychology graduate student has the option of transferring to one of those areas where one could have almost total certainty of spending one's career doing useful work identifying codon sequences on the ninth chromosome or etc. The paradigms are there, it is just a matter of digging.\n\nParadigm-makers are few and far between in every science. In psychology there have been a few\u2014Freud, Skinner, Pavlov, and Piaget, to list some important examples \u2014and there have also been some pseudo paradigm-makers or false prophets \u2014Jung, Hull, and Kohler, for example, able and intrepid adventurers who had the bad luck to return empty-handed (and also Freud, Skinner, Pavlov, and Piaget from another point of view, i.e., implicitly or explicitly they claimed too much).\n\n# *E. Too Many Go into Research Who Do Not Have a True Vocation*\n\n## **(1) Fact: Most Meaningful Research Is Done by an Elite Handful**\n\n*a) Price's Inverse Square Law.* In a 1963 book called *Little Science, Big Science,* Derek de Sola Price pointed out that, going back into the nineteenth century, rates of scientific publication have followed, approximately, an \"inverse square law\" in the sense that the number, N, of scientists producing k papers is approximately proportional to 1/k2 .",
    "is_useful": true,
    "question": "What economic principle suggests that the number of scientists producing a certain volume of research papers decreases exponentially as the production volume increases?"
  },
  {
    "text": "# *E. Too Many Go into Research Who Do Not Have a True Vocation*\n\n## **(1) Fact: Most Meaningful Research Is Done by an Elite Handful**\n\n*a) Price's Inverse Square Law.* In a 1963 book called *Little Science, Big Science,* Derek de Sola Price pointed out that, going back into the nineteenth century, rates of scientific publication have followed, approximately, an \"inverse square law\" in the sense that the number, N, of scientists producing k papers is approximately proportional to 1/k2 . This means that for every 100 authors who produce one paper, 25 will produce two, 11 will write three papers, 6 will write four, and so on (1 of the 100 will manage as many as ten papers). This model suggests that about 50% of all scientific papers are produced by about 10% of the scientists and we're including as \"scientists\" not all the graduates or Ph.D.s but only those who have published at least one paper. The modal lifetime number of publications for Ph.D. psychologists is zero.\n\n*b) Publication by Psychologists.* Out of 20,000 first authors in APA journals over a five-year span, Garvey's APA study found that only 5% appear twice in that five years; less than 2% average one appearance per year\u2014i.e., only about 400 authors publish once per year in APA journals. Using a different data set, George Miller found a similar result, namely that most of the lasting work in psychology was done by a core group of about 400 individuals.",
    "is_useful": true,
    "question": "What trend does Derek de Sola Price's research suggest about the distribution of scientific publications among researchers?"
  },
  {
    "text": "This model suggests that about 50% of all scientific papers are produced by about 10% of the scientists and we're including as \"scientists\" not all the graduates or Ph.D.s but only those who have published at least one paper. The modal lifetime number of publications for Ph.D. psychologists is zero.\n\n*b) Publication by Psychologists.* Out of 20,000 first authors in APA journals over a five-year span, Garvey's APA study found that only 5% appear twice in that five years; less than 2% average one appearance per year\u2014i.e., only about 400 authors publish once per year in APA journals. Using a different data set, George Miller found a similar result, namely that most of the lasting work in psychology was done by a core group of about 400 individuals. Myers (1970) found that half of all authors of articles in psychological journals were cited once or less over the next six years.\n\n## (2) **The Ortega Hypothesis**\n\nJose Ortega y Gasset, a Spanish philosopher who died in 1955, described the world of science as a kind of beehive:\n\nFor it is necessary to insist upon this extraordinary but undeniable fact: experimental science had progressed thanks in great part to the work of men astoundingly mediocre, and even less than mediocre. That is to say, modern science, the root and symbol of our actual civilization, finds a place for the intellectually commonplace man and allows him to work therein with success.",
    "is_useful": true,
    "question": "What evidence is there to suggest that a small percentage of authors contribute to the majority of scientific publications?"
  },
  {
    "text": "Using a different data set, George Miller found a similar result, namely that most of the lasting work in psychology was done by a core group of about 400 individuals. Myers (1970) found that half of all authors of articles in psychological journals were cited once or less over the next six years.\n\n## (2) **The Ortega Hypothesis**\n\nJose Ortega y Gasset, a Spanish philosopher who died in 1955, described the world of science as a kind of beehive:\n\nFor it is necessary to insist upon this extraordinary but undeniable fact: experimental science had progressed thanks in great part to the work of men astoundingly mediocre, and even less than mediocre. That is to say, modern science, the root and symbol of our actual civilization, finds a place for the intellectually commonplace man and allows him to work therein with success. In this way the majority of scientists help the general advance of science while shut up in the narrow cell of their laboratory, like the bee in the cell of its hive, or the turnspit at his wheel. (Cole & Cole, 1972)\n\nIn their interesting *Science* paper, the Coles point out that the common view accords with Ortega's, that science is an ant hill or termite colony kind of enterprise, with multitudes of anonymous workers each contributing essential efforts.",
    "is_useful": true,
    "question": "What analogy is used to describe the nature of scientific work, highlighting the contributions of many individuals regardless of their prominence?"
  },
  {
    "text": "That is to say, modern science, the root and symbol of our actual civilization, finds a place for the intellectually commonplace man and allows him to work therein with success. In this way the majority of scientists help the general advance of science while shut up in the narrow cell of their laboratory, like the bee in the cell of its hive, or the turnspit at his wheel. (Cole & Cole, 1972)\n\nIn their interesting *Science* paper, the Coles point out that the common view accords with Ortega's, that science is an ant hill or termite colony kind of enterprise, with multitudes of anonymous workers each contributing essential efforts.\n\nAnother version from Lord Florey, a past-president of the Royal Society:\n\nScience is rarely advanced by what is known in current jargon as a \"breakthrough\"; rather does our increasing knowledge depend on the activity of thousands of our colleagues throughout the world who add small points to what will eventually become the splendid picture, much in the same way the Pointillistes built up their extremely beautiful canvasses.\n\nAny large city works, to the extent that it does work, on this principle of the termite colony. So does the world of business and commerce under the free enterprise system. The postulate of free enterprise economists is that this is the only way that the world of commerce can work at all effectively.\n\nCole & Cole (1972) investigated whether this description actually fits the enterprise of physics by examining the patterns of citations of other researchers in papers published in the physics journals in 1965.",
    "is_useful": true,
    "question": "How does modern science reflect the contributions of many individuals working collectively rather than relying solely on major breakthroughs?"
  },
  {
    "text": "Another version from Lord Florey, a past-president of the Royal Society:\n\nScience is rarely advanced by what is known in current jargon as a \"breakthrough\"; rather does our increasing knowledge depend on the activity of thousands of our colleagues throughout the world who add small points to what will eventually become the splendid picture, much in the same way the Pointillistes built up their extremely beautiful canvasses.\n\nAny large city works, to the extent that it does work, on this principle of the termite colony. So does the world of business and commerce under the free enterprise system. The postulate of free enterprise economists is that this is the only way that the world of commerce can work at all effectively.\n\nCole & Cole (1972) investigated whether this description actually fits the enterprise of physics by examining the patterns of citations of other researchers in papers published in the physics journals in 1965. They discovered that, at least in 1965, 80% of citations were to the work of just 20% of physicists. They took a \"representative\" sample of 84 university physicists, got their \"best\" paper published in 1965, looked at the 385 authors whom these 84 cite. Sixty percent of the cited authors were from the top nine physics departments, 68% had won awards on the order of the Nobel Prize or election to the National Academy of Sciences, 76% were prolific publishers.",
    "is_useful": true,
    "question": "How does the collaborative nature of scientific advancement resemble the activities of a termite colony?"
  },
  {
    "text": "Many more may be perfectly qualified to do good work, useful work, once a paradigm is available.\n\n## **(3) Serendipity Is Emergenic**\n\nIt may be that being a good researcher, in the sense of paradigm maker, is an \"emergenic\" trait (Lykken, 1982; Li, 1987), the result of a particular configuration of independent traits all of which have to be present or present in a certain degree to yield the result. Having a fine singing voice, for example, is an emergenic trait. Being a great violinist or pianist probably requires high scores on several quasi-independent traits; there are lots of people with a good ear or fast reflexes or deep musical insight or good manual dexterity, but one has to have all of these to play at Carnegie Hall. I would guess that successful paradigm-making may be a multiplicative function of brains x energy x creativity x arrogance x daring x ??? and perhaps the relative weighting of the components is different for different fields of study. *Chutzpah* is probably a necessary ingredient in many situations; if you don't sell your ideas, they won't make any waves. Barbara McKlintock is a case in point. Her Nobel Prize was awarded for work done many years earlier which had not been noticed because she did not sell it. Someone else realized, retrospectively, that she had really pioneered in a currently hot area and did the selling, belatedly, for her.\n\nIn fact, I think that what we call genius is probably emergenic.",
    "is_useful": true,
    "question": "What factors may contribute to successful paradigm-making in research?"
  },
  {
    "text": "*a) Meehl's ' 'Seven Sacred Cows ofAcademia.''* Among these (regrettably unpublished) fallacious postulates is the proposition that a good university-level teacher must be a productive scholar. The two activities are competitive more than they are complementary. It takes much the same kind of intelligence, thought, reading, and insight\u2014not to mention clock hours\u2014to prepare a good lecture as to write a good article or plan a good experiment. (A really good researcher is likely to be a good teacher only because he/she has these abilities and is the kind of person who won't do something at all without doing it well.) Think about people like Isaac Asimov and Carl Sagan, Walter Munn and Gardner Lindzey, or the late Kenneth MacCorquodale. Munn and Lindzey wrote outstanding textbooks, Mac-Corquodale was a splendid teacher, Asimov and Sagan have helped millions of people to understand science a little better. All were fine scholars and good communicators, all of them might have made less of a contribution if they had allocated more of their energies trying to do original research.\n\n*b) Teaching and Public Service.* These two avenues through which an academic can justify his or her paycheck are at least as important as research, at least as demanding of very similar abilities. Most research discoveries will be made by someone else if you do not do it; e.g., if Watson and Crick hadn't worked so hard on the double helix of DNA, Linus Pauling would have had it in a few more months.",
    "is_useful": true,
    "question": "What are some important roles of academics in addition to conducting research?"
  },
  {
    "text": "*b) Teaching and Public Service.* These two avenues through which an academic can justify his or her paycheck are at least as important as research, at least as demanding of very similar abilities. Most research discoveries will be made by someone else if you do not do it; e.g., if Watson and Crick hadn't worked so hard on the double helix of DNA, Linus Pauling would have had it in a few more months. Much useful research is not really very brilliant, the only mystery is why one didn't think of it sooner. Yet it must be said that many bright and knowledgeable people never seem to think of these things or, if they do, don't do anything about it, or can't seem to discriminate between the more- and less-promising ideas that they do have and tend to follow up the wrong ones.\n\nIs it better to turn up even a real nugget of new truth (which many would-be researchers never achieve) or to save a marriage, cure a phobia, teach a really stimulating course, influence legislation by persuasive testimony, plant some important ideas in the minds of students, policy-makers, or laypersons?\n\nOver the past ten years or so, I have spent about one-third of my professional time educating the public about the \"lie detector\": One does not need specialized knowledge to see that most of the claims of the lie detector industry are nonsense and sheer wishful thinking. Senator Sam Ervin, untrained in psychology, realized at once that the polygraph test is a form of \"20th Century witchcraft.\" Yet most people, including many psychologists, cannot see it until someone points it out.",
    "is_useful": true,
    "question": "What are the two avenues through which an academic can justify their paycheck, and why are they considered important?"
  },
  {
    "text": "Is it better to turn up even a real nugget of new truth (which many would-be researchers never achieve) or to save a marriage, cure a phobia, teach a really stimulating course, influence legislation by persuasive testimony, plant some important ideas in the minds of students, policy-makers, or laypersons?\n\nOver the past ten years or so, I have spent about one-third of my professional time educating the public about the \"lie detector\": One does not need specialized knowledge to see that most of the claims of the lie detector industry are nonsense and sheer wishful thinking. Senator Sam Ervin, untrained in psychology, realized at once that the polygraph test is a form of \"20th Century witchcraft.\" Yet most people, including many psychologists, cannot see it until someone points it out. Let's say that I am a Grade B researcher: i.e., nothing wholly trivial or flagrantly wrong, some product that is genuinely useful, nothing really great. If I spend about one-third of my time on polygraph-related matters, that means one-third less production of Grade B research. In exchange, however, quite a few innocent persons who might have gone to prison because they failed polygraph tests were found innocent, quite a few bad guys who might have escaped prison because they had passed \"friendly\" polygraph tests are in prison. Where there was virtually no scientific criticism of the lie detector on which legislators, lawyers, and judges could draw, now there is a book and more than 40 articles and editorials and these criticisms have been cited in several state supreme court decisions banning polygraph evidence because of its inaccuracy.",
    "is_useful": true,
    "question": "What is the benefit of engaging in public education about flawed scientific practices, such as the use of lie detectors, compared to producing traditional research?"
  },
  {
    "text": "Where there was virtually no scientific criticism of the lie detector on which legislators, lawyers, and judges could draw, now there is a book and more than 40 articles and editorials and these criticisms have been cited in several state supreme court decisions banning polygraph evidence because of its inaccuracy. Minnesota and Michigan now ban the use of the polygraph on employees; I was the only scientific witness to testify on behalf of both bills. A bill for a similar federal statute was passed by the House of Representatives in 1986, in part because of my testimony, and will likely become law in 1988.\n\nAny Grade B psychologist could have done these things and it demands no great personal sacrifice since it is mostly fun to do; I lay no claim to be either a genius or a saint. The point is that this sort of public service work is more useful and valuable than most Grade B research (and *all* research of Grades C through Z). One suspects that most of you young psychologists would be able to find a way to make a similar pro *bono* use of your abilities and training at some time in your careers. One hopes that more of you will seize the opportunity when it comes along and not be hindered by any silly notion that it is nobler in the mind to publish some dingbat paper instead.\n\n*c) Research Has Visibility.* One reason research is overvalued is that it gets the glory, its fruits are tangible and public \u2014 you can count the books and articles and you know who wrote them. Great teaching or brilliant clinical work goes relatively unrecognized.",
    "is_useful": true,
    "question": "How can the visibility of research contribute to the perception of its value compared to other forms of professional work in the field of psychology?"
  },
  {
    "text": "The examples we shall have space for here are only illustrative; our bad habits are legion and every one that we throw overboard will make us feel and function better.\n\n## A. *Use of Scientistic Jargon*\n\nWhen I was serving my time on an NIMH research review committee and was assigned to be primary reviewer for a dumb proposal, I found that it was usually sufficient just to translate the author's proposal into ordinary language. \"No, is that really what he plans to do? Why that's dumb!\" Graduate students planning dissertation projects could save themselves later grief by following this rule: Using only language understandable by an intelligent layperson, state your hypothesis, the ancillary hypotheses necessary for your experiment to be a test of that hypothesis, and your predictions. If, stripped of jargon, such a prospectus fails to sound sensible and promising, forget it. Many examples of how social scientists, including some of the most eminent, tend to dress up banal ideas in jargon can be found in Andreski's *The Social Sciences as Sorcery.* I take as my moral for this sermon an excellent phrase of Hans Eysenck's: eschew meretricious and obfuscating sesquipedalianism.\n\nPsychologists, and their psychiatric cousins, are susceptible not only to fads of jargon but to fads of methodology, research techniques, experimental designs, even variables chosen less because of their relevance to some important problem than because they are currently in vogue.",
    "is_useful": true,
    "question": "What are some of the negative impacts of using scientific jargon and trendy methodologies in research proposals?"
  },
  {
    "text": "It is the Cargo Cult mentality, when someone cites a \"research finding,\" which leads us to renounce common sense and embrace foolishness. We should throw it overboard.\n\n## *B. Over-Reliance on Significance Testing: The Favorite Ritual*\n\nResearchers often do not know what they are looking for or what will turn up but one goal always beckons, namely, a p-value less than .05, since that is what it takes to get a publication. Pursuit of statistical significance has become the tail that wags the dog.\n\nI once was outside reviewer on a dissertation from a Canadian university, a rather interesting-sounding study of autonomic responses of psychopaths, neurotic offenders, and normals. I found it impossible to determine how the study came out, however, because there were 75 *pages* of ANOVA tables, 4th order interactions, some of them \"significant\" and discussed at wearying length. I suggested that the candidate should be passed since he clearly had been taught to do this by his faculty but that perhaps some of the faculty ought to be defrocked.\n\n#### **(1) The Null Hypothesis Is (Almost) Always False**\n\nA professor at Northwestern spent most of 1967 flipping a coin 300,000 times, finding 50.2% heads, significant at the .01 level. At about the same time, Meehl and I did our unpublished \"Crud Factor\" study. We had available from the University's Student Counseling Bureau computerized responses to an \"After High School, What?\"",
    "is_useful": true,
    "question": "What is a significant issue related to statistical significance in research publication practices?"
  },
  {
    "text": "I think that the only way a psychologist is likely to fail to refute the null hypothesis with really large samples is by using unreliable measures (which, of course, is easy for a psychologist to do!). And if the null hypothesis is always false, then refuting a null hypothesis is a very weak test of a theory and not in itself a justification for publishing a paper.\n\n## **(2) Statistically Significant Findings Are Frequently Misleading**\n\nI once published an article (Lykken, 1968) examining the claim of another author that a \"frog response\" on the Rorschach test is evidence that the responder unconsciously believes in the \"cloacal theory of birth.\" That author reasoned that one who believes impregnation occurs *per os* and parturition *per anus* might see frogs on the Rorschach and also be disposed toward eating disorders. A group of patients who had given frog responses were found to have many more references to eating disorders in their charts than a control group of patients without frog responses. The Chi-square was highly significant. We have already seen why we need not feel the least compulsion to accept this theory on the basis of this outcome, but must we not at least admit that an empirical fact has been demonstrated, viz., this connection between frog responding and eating problems?\n\nRemembering that false facts tend to be more mischievous than false theories, let us ask what is the \"fact\" that this study seems to have demonstrated. The notion of a valid empirical finding is grounded in the idea of replication.",
    "is_useful": true,
    "question": "What is necessary for an empirical finding to be considered valid in the context of psychological research?"
  },
  {
    "text": "Remembering that false facts tend to be more mischievous than false theories, let us ask what is the \"fact\" that this study seems to have demonstrated. The notion of a valid empirical finding is grounded in the idea of replication. Because this author's result achieved the .01 level of significance, we say that, if this experiment were to be repeated exactly hundreds of times, then we should be willing to bet $99 to $1 that the grand mean result will be non-zero and at least in the direction found by the first author. But not even he could repeat the same experiment exactly, not even once. The most we could do, as readers, is to repeat the experiment as the author described it, to follow his *experimental recipe',* I call this process \"operational replication.\" But neither he nor we know whether he has adequately described all the conditions that pertained in his first study and that influenced the outcome. If our operational replication fails, the most likely explanation will be that his experimental recipe was incomplete. And his original significance test provides no quantitative estimate of the likelihood that our operational replication will succeed.\n\nIf an operational replication is successful, we still cannot be certain that \"Rorschach frog responding is associated with eating disorders.\" Such an empirical generalization leaps far ahead of the facts in hand. These facts are that patients of the type he studied, who give what he calls frog responses when the Rorschach is administered the way he did it, are likely to have an excess of eating disorders, defined as he defined them, listed in the ward notes of the nurses who worked in his hospital.",
    "is_useful": true,
    "question": "What is the significance of replication in establishing the validity of empirical findings in scientific research?"
  },
  {
    "text": "In a constructive replication, we deliberately ignore the first author's recipe and focus solely on the generalization in which he and we are interested. We design our own test of that hypothesis, select our own patients, administer the Rorschach as we think it should be given, define \"frog responding\" and \"eating disorders,\" and assess the latter, in whatever way seems sensible to us. Only by constructive replication can we reasonably hope to compel respect for any claim we make of having demonstrated a generalizable empirical difference or relationship.\n\nA significance test is like a license on a car; you have to have one before you drive to the APA convention, but only an idiot would invest in an old wrecker just because it has a valid license plate. R. A. Fisher himself made a similar point to the British Society for Psychical Research (Fisher, 1929); significance testing may make a finding more intriguing but it takes replication (constructive replication) to make it believable.\n\n#### (3) **Ways of Staying Out of \"Significant\" Trouble**\n\n*a) Make Range, Rather Than Merely Directional, Predictions* When we test the null hypothesis, that the difference or correlation is actually zero, against the usual weak, directional hypothesis, that the difference or correlation is, say, positive, then even if our theory is quite wrong our chances of refuting the null hypothesis increase with the size of the sample, approaching p = 0.5; that is, the bigger and more expensive the experiment, the more likely it is to yield a false result, a seeming but undeserved confirmation of the theory.",
    "is_useful": true,
    "question": "What is the significance of constructive replication in validating empirical claims in research?"
  },
  {
    "text": "R. A. Fisher himself made a similar point to the British Society for Psychical Research (Fisher, 1929); significance testing may make a finding more intriguing but it takes replication (constructive replication) to make it believable.\n\n#### (3) **Ways of Staying Out of \"Significant\" Trouble**\n\n*a) Make Range, Rather Than Merely Directional, Predictions* When we test the null hypothesis, that the difference or correlation is actually zero, against the usual weak, directional hypothesis, that the difference or correlation is, say, positive, then even if our theory is quite wrong our chances of refuting the null hypothesis increase with the size of the sample, approaching p = 0.5; that is, the bigger and more expensive the experiment, the more likely it is to yield a false result, a seeming but undeserved confirmation of the theory. If our theory were strong enough to make a point prediction (e.g., the correlation is 0.50), then this situation would be happily reversed. The larger our sample and the more precise our measurements, the more stringent would be the test of our theory. Psychological theories may never be able to make point predictions, but at least, like say the cosmologists, we ought to be able to squeeze out of our theories something more than merely the prediction that A and B are positively correlated.\n\nIf we took our theories seriously and made the effort, we should be able to make rough estimates of parameters sufficient to say, e.g., that the correlation ought to be greater than .40 but not higher than .80.",
    "is_useful": true,
    "question": "What is one way researchers can improve the credibility of their findings in scientific research?"
  },
  {
    "text": "If our theory were strong enough to make a point prediction (e.g., the correlation is 0.50), then this situation would be happily reversed. The larger our sample and the more precise our measurements, the more stringent would be the test of our theory. Psychological theories may never be able to make point predictions, but at least, like say the cosmologists, we ought to be able to squeeze out of our theories something more than merely the prediction that A and B are positively correlated.\n\nIf we took our theories seriously and made the effort, we should be able to make rough estimates of parameters sufficient to say, e.g., that the correlation ought to be greater than .40 but not higher than .80. Then, at least we should be able to claim that the better the experiment the tougher the test of the theory. Suppose that a very large and careful experiment yields a correlation within the predicted range; what are the odds of this happening even if our theory is wholly false? I know of no general way to quantify this problem beyond saying that the odds are substantially less than the customary value of 50:50. There are no firm and heaven-sent criteria, only informed human judgment applied to the particulars of this case. If the theory does logically lead to the given range prediction, using auxiliary hypotheses that seem reasonably robust, and if the experiment was truly a tough test, then we must respect the *theory a posteriori* more than the frog response result compelled us to respect the theory of cloacal birth.\n\n*b) Multiple Corroboration.",
    "is_useful": true,
    "question": "How does the size of a sample and precision of measurements affect the testing of psychological theories in open science?"
  },
  {
    "text": "Then, at least we should be able to claim that the better the experiment the tougher the test of the theory. Suppose that a very large and careful experiment yields a correlation within the predicted range; what are the odds of this happening even if our theory is wholly false? I know of no general way to quantify this problem beyond saying that the odds are substantially less than the customary value of 50:50. There are no firm and heaven-sent criteria, only informed human judgment applied to the particulars of this case. If the theory does logically lead to the given range prediction, using auxiliary hypotheses that seem reasonably robust, and if the experiment was truly a tough test, then we must respect the *theory a posteriori* more than the frog response result compelled us to respect the theory of cloacal birth.\n\n*b) Multiple Corroboration.* Any theory worth thinking about should be rich enough to generate more than one testable prediction. If one makes five reasonably independent predictions and they all are confirmed experimentally, one can claim p less than (0.5)5 or less than about 4 chances in 100 of doing that well accidentally.\n\n*c) Comparing Alternative Models.* As Sir Karl Popper has pointed out, we should not aspire to show that our theory is valid but, rather, that it possesses more \"verisimilitude\" than any current competitor and therefore deserves interim allegiance until something better comes along. That is, for any theory, if our tests are sufficiently searching and stringent, the theory must ultimately fail.",
    "is_useful": true,
    "question": "What is a key requirement for a theory to be considered robust in open science?"
  },
  {
    "text": "*b) Multiple Corroboration.* Any theory worth thinking about should be rich enough to generate more than one testable prediction. If one makes five reasonably independent predictions and they all are confirmed experimentally, one can claim p less than (0.5)5 or less than about 4 chances in 100 of doing that well accidentally.\n\n*c) Comparing Alternative Models.* As Sir Karl Popper has pointed out, we should not aspire to show that our theory is valid but, rather, that it possesses more \"verisimilitude\" than any current competitor and therefore deserves interim allegiance until something better comes along. That is, for any theory, if our tests are sufficiently searching and stringent, the theory must ultimately fail. A more constructive approach, therefore, is to apply equally stringent tests to existing alternative models and to focus subsequent research and development on the model or models that fit the data best. This is the approach of modern biometrical genetics (e.g., Jinks & Fulker, 1970; Eaves, 1982) and of structural-modeling specialists (e.g., Bentler & Bonett, 1980; Cudeck & Browne, 1983).\n\nIn most areas especially of \"soft\" psychology, it is rare for a proponent of a theory to give explicit systematic attention to possible alternative explanations of a data set. Showing that one's theory is compatible with the trend of one's data is, as we have seen, only weak corroboration for the theory.",
    "is_useful": true,
    "question": "What principles should guide the testing and validation of scientific theories in relation to alternative models?"
  },
  {
    "text": "In the initial attack on a new problem, we can use the Two-Phase Experiment. Phase 1 is the discovery phase, the pilot study, in which we find out for ourselves how the land lies. Since we are not trying to impress or convince anyone else, we include only such refinements and controls as we ourselves believe to be necessary to evaluate the hypothesis. If we decide after running three subjects that some aspect of our set-up should be changed, we change it and roll on. If our planned method of analysis of the data yields mostly noise, we feel free to seek a different method that will yield an orderly result. If Phase 1 produces interesting findings and if, in our judgment, we can now design a full-scale experiment that will yield the same findings, then we move on to Phase 2, the proof or verification phase, the elegant experiment designed to convince others (e.g., journal referees) that our findings are valid.\n\nAssuming that our judgment is good, the Phase 2 experiment will always be better designed and more likely to produce useful results because of what we have learned in Phase 1. If Phase 1 does not work out, we will not feel so committed to the project that we will struggle to wring some publishable but unreplicable findings out of it. Muller, Otto, and Benignus (1983) discuss these and other useful strategies in a paper written for psychophysiologists but equally valuable for workers in other research areas.",
    "is_useful": true,
    "question": "What is the significance of the Two-Phase Experiment in the context of research methodology?"
  },
  {
    "text": "If Phase 1 produces interesting findings and if, in our judgment, we can now design a full-scale experiment that will yield the same findings, then we move on to Phase 2, the proof or verification phase, the elegant experiment designed to convince others (e.g., journal referees) that our findings are valid.\n\nAssuming that our judgment is good, the Phase 2 experiment will always be better designed and more likely to produce useful results because of what we have learned in Phase 1. If Phase 1 does not work out, we will not feel so committed to the project that we will struggle to wring some publishable but unreplicable findings out of it. Muller, Otto, and Benignus (1983) discuss these and other useful strategies in a paper written for psychophysiologists but equally valuable for workers in other research areas.\n\nReichenbach's distinction between the *Context of Discovery* (e.g., the pilot study) and the *Context of Verification* (e.g., the Phase 2 study) is a useful one, especially for psychologists. Since we should be honestly humble about how little we know for sure, it behooves us to be open and relatively loose in the context of discovery. Just as there are few hypotheses than we can claim as proven, so are there relatively few that we can reasonably reject out of hand. Extrasensory perception is a good example.",
    "is_useful": true,
    "question": "What are the phases of research that help ensure the validity and replicability of findings in scientific studies?"
  },
  {
    "text": "If Phase 1 does not work out, we will not feel so committed to the project that we will struggle to wring some publishable but unreplicable findings out of it. Muller, Otto, and Benignus (1983) discuss these and other useful strategies in a paper written for psychophysiologists but equally valuable for workers in other research areas.\n\nReichenbach's distinction between the *Context of Discovery* (e.g., the pilot study) and the *Context of Verification* (e.g., the Phase 2 study) is a useful one, especially for psychologists. Since we should be honestly humble about how little we know for sure, it behooves us to be open and relatively loose in the context of discovery. Just as there are few hypotheses than we can claim as proven, so are there relatively few that we can reasonably reject out of hand. Extrasensory perception is a good example. Having worked for years with hundreds of pairs of adult twins, hearing so many anecdotes of apparent telepathic communication between them, which usually occur in moments of stress or crisis, I am inclined to believe in telepathy \u2014as an individual but not as a scientist. That is, I would be happy to invest of my time and the government's money in what I thought was a promising telepathy experiment. But to compensate for this openness in the context of discovery, we must be tough-minded in the context of verification.",
    "is_useful": true,
    "question": "What distinction is important in the research process that highlights the difference between early exploration of ideas and formal testing of hypotheses?"
  },
  {
    "text": "Since we should be honestly humble about how little we know for sure, it behooves us to be open and relatively loose in the context of discovery. Just as there are few hypotheses than we can claim as proven, so are there relatively few that we can reasonably reject out of hand. Extrasensory perception is a good example. Having worked for years with hundreds of pairs of adult twins, hearing so many anecdotes of apparent telepathic communication between them, which usually occur in moments of stress or crisis, I am inclined to believe in telepathy \u2014as an individual but not as a scientist. That is, I would be happy to invest of my time and the government's money in what I thought was a promising telepathy experiment. But to compensate for this openness in the context of discovery, we must be tough-minded in the context of verification. Since no one has yet succeeded in capturing telepathy in the laboratory, in discovering a paradigm that yields consistent, reproducible results, telepathy remains just an intriguing hypothesis which no one should believe in *qua* scientist.\n\n## **(4) The Bottom Line**\n\nThe best single rule may be Feynman's principle of total scientific honesty. Feynman says:\n\nIf you're doing an experiment, you should report everything that you think might make it invalid \u2014not only what you think is right about it [but] other causes that might possibly explain your results. . . . Details that could throw doubt on your interpretation must be given if you know them. .. .",
    "is_useful": true,
    "question": "What principle emphasizes the importance of honesty and transparency in scientific research, particularly when reporting the validity of experiments?"
  },
  {
    "text": "That is, I would be happy to invest of my time and the government's money in what I thought was a promising telepathy experiment. But to compensate for this openness in the context of discovery, we must be tough-minded in the context of verification. Since no one has yet succeeded in capturing telepathy in the laboratory, in discovering a paradigm that yields consistent, reproducible results, telepathy remains just an intriguing hypothesis which no one should believe in *qua* scientist.\n\n## **(4) The Bottom Line**\n\nThe best single rule may be Feynman's principle of total scientific honesty. Feynman says:\n\nIf you're doing an experiment, you should report everything that you think might make it invalid \u2014not only what you think is right about it [but] other causes that might possibly explain your results. . . . Details that could throw doubt on your interpretation must be given if you know them. .. . If you make a theory, for example, you must also put down all the facts that disagree with it ... you want to make sure, when explaining what it fits, that those things it fits are not just the things that gave you the idea for the theory but that the finished theory makes something else come out right, in addition. (Feynman, 1986)\n\nThis is not nearly so easy as it seems since it is natural to become infatuated with one's own ideas, to become an advocate, to be a much gentler critic of one's own work than one is of others'.",
    "is_useful": true,
    "question": "What principle emphasizes the importance of total scientific honesty in research and the need to report all factors that could invalidate results?"
  },
  {
    "text": "Feynman says:\n\nIf you're doing an experiment, you should report everything that you think might make it invalid \u2014not only what you think is right about it [but] other causes that might possibly explain your results. . . . Details that could throw doubt on your interpretation must be given if you know them. .. . If you make a theory, for example, you must also put down all the facts that disagree with it ... you want to make sure, when explaining what it fits, that those things it fits are not just the things that gave you the idea for the theory but that the finished theory makes something else come out right, in addition. (Feynman, 1986)\n\nThis is not nearly so easy as it seems since it is natural to become infatuated with one's own ideas, to become an advocate, to be a much gentler critic of one's own work than one is of others'. Many of us are able to tear other people's research limb from limb while we smile upon our own like an indulgent parent. In fact, I think one *should be* protective at first until the toddler at least can stand erect. But before one lets the little devil out into the neighborhood, one must learn to look at it as critically as others will.",
    "is_useful": true,
    "question": "What is an important principle to uphold when conducting scientific experiments and presenting findings in the context of open science?"
  },
  {
    "text": "If you make a theory, for example, you must also put down all the facts that disagree with it ... you want to make sure, when explaining what it fits, that those things it fits are not just the things that gave you the idea for the theory but that the finished theory makes something else come out right, in addition. (Feynman, 1986)\n\nThis is not nearly so easy as it seems since it is natural to become infatuated with one's own ideas, to become an advocate, to be a much gentler critic of one's own work than one is of others'. Many of us are able to tear other people's research limb from limb while we smile upon our own like an indulgent parent. In fact, I think one *should be* protective at first until the toddler at least can stand erect. But before one lets the little devil out into the neighborhood, one must learn to look at it as critically as others will.\n\n## **Conclusions**\n\nIn my junior year in college, I was led to change my major from Chemical Engineering to Psychology by the brilliant teaching of Kenneth MacCorquodale and Paul Meehl and by my discovery, in W. T. Heron's course in Learning Theory, that I was already at the cutting edge of development of this slow-blooming young science. I have never regretted that decision, for there is nothing I would rather have been\u2014that I could have been\u2014than a psychologist. I am a rough carpenter rather than a finisher or cabinetmaker and there is need yet for rough carpentry in Psychology's edifice.",
    "is_useful": true,
    "question": "What are the challenges researchers face when evaluating their own theories critically in the context of scientific inquiry?"
  },
  {
    "text": "We can take (rather weak) comfort in the fact that, if our discipline were as mature as physics is, then psychology would probably be recognized as more difficult than physics. It is certainly harder to be a psychological researcher now than it was to do research in physics in Faraday's time.\n\nThe brain-computer analogy seems to me to be provocative and genuinely useful, clarifying the relationship among the traditional sub-areas of psychology and illuminating the deep waters of the nomothetic-ideographic problem. It may even be that the new academic Departments of Computer Science will evolve a structure that foreshadows that of future Departments of Psychology.\n\nIt is important that we recognize, acknowledge, and root out the Cargo Cult aspects of our enterprise, the scientistic rituals and related bad habits by means of which we have sought to emulate the form, but not the substance, of the hard sciences. Some of the most pernicious of these bad habits involve rituals of statistical inference. My prescription would be a limited moratorium on directional tests of significance. From now until the Year 2000, let us say that research reports submitted to psychological journals must include either tests of range, rather than mere directional, predictions or else systematic comparisons of alternative hypotheses. I think these latter, more powerful techniques are potentially within our grasp, but they are new and harder than the nearly futile null hypothesis testing to which we have become addicted.",
    "is_useful": true,
    "question": "What challenges does psychological research face in comparison to more established sciences like physics?"
  },
  {
    "text": "*research-article*2019\n\n# Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods\n\n![](_page_0_Picture_3.jpeg)\n\n# Evan C. Carter1, Felix D. Sch\u00f6nbrodt2, Will M. Gervais3, and Joseph Hilgard4\n\n1 Human Research and Engineering Directorate, U.S. Army Research Laboratory, Aberdeen, Maryland; 2 Department of Psychology, Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen; 3 Department of Psychology, University of Kentucky; and 4 Department of Psychology, Illinois State University\n\n# ASSOCIATION FOR *General Article* PSYCHOLOGICAL SCIENCE\n\nhttps://doi.org/10.1177/2515245919847196 Advances in Methods and Practices in Psychological Science 2019, Vol. 2(2) 115\u2013144 \u00a9 The Author(s) 2019 Article reuse guidelines: sagepub.com/journals-permissions DOI: 10.1177/2515245919847196 www.psychologicalscience.org/AMPPS\n\n#### Abstract\n\nPublication bias and questionable research practices in primary research can lead to badly overestimated effects in metaanalysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology.",
    "is_useful": true,
    "question": "What are the implications of publication bias and questionable research practices in psychological research, and how do statistical methods aim to address these issues?"
  },
  {
    "text": "2(2) 115\u2013144 \u00a9 The Author(s) 2019 Article reuse guidelines: sagepub.com/journals-permissions DOI: 10.1177/2515245919847196 www.psychologicalscience.org/AMPPS\n\n#### Abstract\n\nPublication bias and questionable research practices in primary research can lead to badly overestimated effects in metaanalysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses\u2014that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications.",
    "is_useful": true,
    "question": "What recommendations do methodologists provide to improve the reliability of meta-analytic results in psychology?"
  },
  {
    "text": "Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses\u2014that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shiny apps.org/apps/metaExplorer/.\n\n## Keywords\n\nmeta-analysis, publication bias, p-hacking, questionable research practices, bias correction, open data, open materials\n\nReceived 5/26/17; Revision accepted 3/18/19\n\nStatistical techniques for analyzing the results from a set of studies in aggregate\u2014often called meta-analysis\u2014are popular in psychology and many other scientific disciplines because they provide high-powered tests, the ability to examine moderators across studies, and precise effect-size estimates that are useful for planning future studies and making policy decisions.",
    "is_useful": true,
    "question": "What practices should researchers in psychology focus on to ensure the reliability of meta-analytic results?"
  },
  {
    "text": "Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shiny apps.org/apps/metaExplorer/.\n\n## Keywords\n\nmeta-analysis, publication bias, p-hacking, questionable research practices, bias correction, open data, open materials\n\nReceived 5/26/17; Revision accepted 3/18/19\n\nStatistical techniques for analyzing the results from a set of studies in aggregate\u2014often called meta-analysis\u2014are popular in psychology and many other scientific disciplines because they provide high-powered tests, the ability to examine moderators across studies, and precise effect-size estimates that are useful for planning future studies and making policy decisions. However, just as bias can make the results from individual studies completely misleading (e.g., Simmons, Nelson, & Simonsohn, 2011), it can do the same to meta-analytic results. To address this problem, researchers have developed statistical techniques designed to identify and correct for bias. In this article, we present a neutral comparison (Boulesteix, Wilson, & Hapfelmeier, 2017) of how several promising methods perform when applied to simulated data that could have plausibly been produced by research in psychology. Our goal is to help researchers in psychology know what to expect from different methods when conducting meta-analyses in the face of bias.",
    "is_useful": true,
    "question": "What practices are recommended for researchers in psychology to improve the quality and reliability of meta-analytic results?"
  },
  {
    "text": "However, just as bias can make the results from individual studies completely misleading (e.g., Simmons, Nelson, & Simonsohn, 2011), it can do the same to meta-analytic results. To address this problem, researchers have developed statistical techniques designed to identify and correct for bias. In this article, we present a neutral comparison (Boulesteix, Wilson, & Hapfelmeier, 2017) of how several promising methods perform when applied to simulated data that could have plausibly been produced by research in psychology. Our goal is to help researchers in psychology know what to expect from different methods when conducting meta-analyses in the face of bias.\n\n## Meta-Analysis\n\nMeta-analytic techniques involve synthesizing a set of results from studies investigating the same empirical\n\n#### Corresponding Author:\n\nEvan C. Carter, Human Research and Engineering Directorate, U.S. Army Research Laboratory, Aberdeen Proving Ground, Aberdeen, MD 21005\n\nE-mail: evan.c.carter@gmail.com\n\n#### Box 1. Glossary\n\n\u03b4. Under the fixed-effect model, \u03b4 is the hypothetical true underlying effect estimated by each study. Under the random-effects model, \u03b4 is the mean of the distribution of hypothetical true underlying effects.\n\ndi , vi . The observed effect size (d) and its associated variance (v) for the i th study. We calculate d as M M S 1 2 \u2212 , where M1 and\n\nM2 are the means of the two groups and S is the pooled standard error of the two groups.",
    "is_useful": true,
    "question": "What are the implications of bias in research studies and how do researchers attempt to address it in the context of meta-analysis?"
  },
  {
    "text": "Such a case would yield estimates that are accurate on average, but any individual estimate could be quite far from the truth.\n\nRoot mean square error (RMSE). RMSE incorporates information about the average error as well as the variance (i.e., the efficiency) in the estimates. Low RMSE is possible even when a method produces estimates that are consistently biased in one direction. For example, if a very narrow distribution of estimates is centered a bit above the true value, the estimates are too high, on average, but the variability of these estimates is low. Thus, both mean error and RMSE must be considered when a method's estimation performance is evaluated. For both mean error and RMSE, values as close to zero as possible are desirable. 95% coverage probability. The percentage of 95% confidence intervals that include the true value of \u03b4. Optimally, a method's coverage probability is at the nominal level of 95%. Low coverage is problematic, as it means that most confidence intervals do not contain the true value. Coverage rates higher than 95% may indicate exceedingly wide confidence intervals.\n\nphenomenon (Borenstein, Hedges, Higgins, & Rothstein, 2011). Most often, the results from the individual studies take the form of effect-size estimates, and because meta-analyses are usually applied to studies with dependent variables measured on different scales, effect-size estimates are typically standardized. The typical goal of a meta-analysis is to produce a single summary estimate of the hypothetical true underlying effect, \u03b4, estimated by each effect size in the data set.",
    "is_useful": true,
    "question": "What statistical measures are important for evaluating the accuracy and reliability of estimation methods in open science research?"
  },
  {
    "text": "Most often, the results from the individual studies take the form of effect-size estimates, and because meta-analyses are usually applied to studies with dependent variables measured on different scales, effect-size estimates are typically standardized. The typical goal of a meta-analysis is to produce a single summary estimate of the hypothetical true underlying effect, \u03b4, estimated by each effect size in the data set. This approach is usually called fixed-effect meta-analysis (Cooper, Hedges, & Valentine, 2009) and can be modeled as di = \u03b4 + ei , where di is the observed effect size for study i, which differs from the true underlying effect, \u03b4, by some amount of sampling error, ei , which is normally distributed with a mean of 0 and a variance of vi . (See Box 1 for a glossary of the statistical symbols and terms used in this article.)\n\nAnother common model, known as random-effects (RE) meta-analysis (Cooper et al., 2009), holds that each study provides an estimate, di , of a different, related true effect, Ti \u2014that is, di = Ti + ei . This approach allows for the possibility that researchers attempting to study the same phenomenon may nonetheless be studying different underlying effects that vary as a function of, for example, the operationalization of the independent variable or the population sampled in the particular study.",
    "is_useful": true,
    "question": "What are the typical goals and models of meta-analysis in research?"
  },
  {
    "text": "Applying the RE model to an observed set of studies provides an estimate of the average true underlying effect, \u03b4, and the amount of between-study heterogeneity, \u03c42 . In this article, we use RE meta-analysis as our baseline for \"uncorrected\" meta-analysis. It should be noted, however, that determining which uncorrected estimator for the average true underlying effect to use is an active area of study itself (Baker & Jackson, 2013; T. Rice, Higgins, & Lumley, 2017; Schmid, 2017; T. D. Stanley & Doucouliagos, 2015; Veroniki et al., 2016).\n\n## Bias\n\nThe effects being estimated by meta-analysis can be systematically over- or underestimated in the face of bias, which is caused by factors that affect the analysis and reporting of the individual studies in the metaanalytic data set. We considered two primary sources of meta-analytic bias in our simulation study: *publication bias* and *questionable research practices* (QRPs).\n\nPublication bias occurs when the probability of results entering the published record is affected by the results themselves (Rothstein, Sutton, & Borenstein, 2006).",
    "is_useful": true,
    "question": "What are the two primary sources of bias in meta-analysis as discussed in relation to open science?"
  },
  {
    "text": "It should be noted, however, that determining which uncorrected estimator for the average true underlying effect to use is an active area of study itself (Baker & Jackson, 2013; T. Rice, Higgins, & Lumley, 2017; Schmid, 2017; T. D. Stanley & Doucouliagos, 2015; Veroniki et al., 2016).\n\n## Bias\n\nThe effects being estimated by meta-analysis can be systematically over- or underestimated in the face of bias, which is caused by factors that affect the analysis and reporting of the individual studies in the metaanalytic data set. We considered two primary sources of meta-analytic bias in our simulation study: *publication bias* and *questionable research practices* (QRPs).\n\nPublication bias occurs when the probability of results entering the published record is affected by the results themselves (Rothstein, Sutton, & Borenstein, 2006). For example, if researchers strongly believe that an effect is real and positive, reports of statistically nonsignificant or negative estimates of that effect may never be submitted for publication or may be rejected by reviewers and editors (Ferguson & Heene, 2012; Greenwald, 1975; Rothstein et al., 2006; Sterling, Rosenbaum, & Weinkam, 1995). In other words, statistically nonsignificant results, or results that contradict accepted theory, are left in the \"file drawer.\"",
    "is_useful": true,
    "question": "What are the primary sources of bias addressed in meta-analysis?"
  },
  {
    "text": "We considered two primary sources of meta-analytic bias in our simulation study: *publication bias* and *questionable research practices* (QRPs).\n\nPublication bias occurs when the probability of results entering the published record is affected by the results themselves (Rothstein, Sutton, & Borenstein, 2006). For example, if researchers strongly believe that an effect is real and positive, reports of statistically nonsignificant or negative estimates of that effect may never be submitted for publication or may be rejected by reviewers and editors (Ferguson & Heene, 2012; Greenwald, 1975; Rothstein et al., 2006; Sterling, Rosenbaum, & Weinkam, 1995). In other words, statistically nonsignificant results, or results that contradict accepted theory, are left in the \"file drawer.\" Because the data set collected by the meta-analyst depends on the availability of studies on the topic of interest, and published data are much easier to find than nonpublished data, publication bias can result in a meta-analytic sample that overrepresents studies yielding statistically significant, theory-consistent results. This can result in misleading meta-analytic findings, such as inflated estimates of the average true underlying effect. And although we do not focus on heterogeneity here, it is important to note that such bias also affects estimates of heterogeneity in complex, nonlinear ways (e.g., Augusteijn, van Aert, & van Assen, 2019; Jackson, 2007).",
    "is_useful": true,
    "question": "What are the primary sources of meta-analytic bias that can impact the accuracy of research findings?"
  },
  {
    "text": "In other words, statistically nonsignificant results, or results that contradict accepted theory, are left in the \"file drawer.\" Because the data set collected by the meta-analyst depends on the availability of studies on the topic of interest, and published data are much easier to find than nonpublished data, publication bias can result in a meta-analytic sample that overrepresents studies yielding statistically significant, theory-consistent results. This can result in misleading meta-analytic findings, such as inflated estimates of the average true underlying effect. And although we do not focus on heterogeneity here, it is important to note that such bias also affects estimates of heterogeneity in complex, nonlinear ways (e.g., Augusteijn, van Aert, & van Assen, 2019; Jackson, 2007).\n\nA related but independent form of bias is the use of QRPs (also referred to as the undisclosed use of researcher degrees of freedom or p-hacking). QRPs are said to occur when researchers favor a specific analytic approach (e.g., removing outliers or covariates) from the variety of potential approaches on the basis of the results that it yields. Such choices may be justifiable, yet simultaneously arbitrary and motivated (Simonsohn, Simmons, & Nelson, 2016). As is the case with publication bias, QRPs can result in overestimates of the true effect, as analyses that yield significant results are highlighted and analyses that do not yield such results are censored.",
    "is_useful": true,
    "question": "What are the potential biases in research publication that can lead to misleading meta-analytic findings?"
  },
  {
    "text": "And although we do not focus on heterogeneity here, it is important to note that such bias also affects estimates of heterogeneity in complex, nonlinear ways (e.g., Augusteijn, van Aert, & van Assen, 2019; Jackson, 2007).\n\nA related but independent form of bias is the use of QRPs (also referred to as the undisclosed use of researcher degrees of freedom or p-hacking). QRPs are said to occur when researchers favor a specific analytic approach (e.g., removing outliers or covariates) from the variety of potential approaches on the basis of the results that it yields. Such choices may be justifiable, yet simultaneously arbitrary and motivated (Simonsohn, Simmons, & Nelson, 2016). As is the case with publication bias, QRPs can result in overestimates of the true effect, as analyses that yield significant results are highlighted and analyses that do not yield such results are censored. We note that all bias-correcting methods that we applied in our study were designed to address publication bias, not QRPs.",
    "is_useful": true,
    "question": "What are QRPs and how do they impact research findings in the context of open science?"
  },
  {
    "text": "A related but independent form of bias is the use of QRPs (also referred to as the undisclosed use of researcher degrees of freedom or p-hacking). QRPs are said to occur when researchers favor a specific analytic approach (e.g., removing outliers or covariates) from the variety of potential approaches on the basis of the results that it yields. Such choices may be justifiable, yet simultaneously arbitrary and motivated (Simonsohn, Simmons, & Nelson, 2016). As is the case with publication bias, QRPs can result in overestimates of the true effect, as analyses that yield significant results are highlighted and analyses that do not yield such results are censored. We note that all bias-correcting methods that we applied in our study were designed to address publication bias, not QRPs.\n\n## Simulation Studies of Bias Correction in Meta-Analysis\n\nMany simulation studies have been conducted to compare the performance of methods that correct for bias in meta-analysis (e.g., Hedges & Vevea, 1996; McShane, B\u00f6ckenholt, & Hansen, 2016; Moreno et al., 2009; R\u00fccker, Carpenter, & Schwarzer, 2011; Simonsohn, Nelson, & Simmons, 2014; T. D. Stanley, 2017; T. D. Stanley & Doucouliagos, 2014; van Aert, Wicherts, & van Assen, 2016; van Assen, van Aert, & Wicherts, 2015).",
    "is_useful": true,
    "question": "What are QRPs and how do they relate to bias in research findings?"
  },
  {
    "text": "We note that all bias-correcting methods that we applied in our study were designed to address publication bias, not QRPs.\n\n## Simulation Studies of Bias Correction in Meta-Analysis\n\nMany simulation studies have been conducted to compare the performance of methods that correct for bias in meta-analysis (e.g., Hedges & Vevea, 1996; McShane, B\u00f6ckenholt, & Hansen, 2016; Moreno et al., 2009; R\u00fccker, Carpenter, & Schwarzer, 2011; Simonsohn, Nelson, & Simmons, 2014; T. D. Stanley, 2017; T. D. Stanley & Doucouliagos, 2014; van Aert, Wicherts, & van Assen, 2016; van Assen, van Aert, & Wicherts, 2015). However, there is very little overlap among these studies in either the methods they have examined or the simulated conditions they have explored. Different simulation studies have implemented bias differently, have drawn sample sizes from different distributions, and have varied widely in the value and form of the simulated true underlying effects. This lack of overlap is not surprising given that there is an effectively infinite number of possible combinations of different conditions to explore and no way of determining which conditions actually underlie real-world data. In other words, not only is there an inherent dimensionality problem in these simulation studies, but there is also no ground truth.",
    "is_useful": true,
    "question": "What are the challenges associated with simulation studies of bias correction in meta-analysis?"
  },
  {
    "text": "However, there is very little overlap among these studies in either the methods they have examined or the simulated conditions they have explored. Different simulation studies have implemented bias differently, have drawn sample sizes from different distributions, and have varied widely in the value and form of the simulated true underlying effects. This lack of overlap is not surprising given that there is an effectively infinite number of possible combinations of different conditions to explore and no way of determining which conditions actually underlie real-world data. In other words, not only is there an inherent dimensionality problem in these simulation studies, but there is also no ground truth. These problems are often not discussed in reports of simulation studies, and indeed, many of the reports just cited explicitly or implicitly\u2014recommended the use of a single method, despite the fact that each study examined performance of only a handful of correction methods in only a limited subset of possible conditions.\n\nIn this article, we do not identify a single method that we believe should be used in all situations. Instead, we aim to add to the existing literature by (a) exploring a further set of conditions that may plausibly represent real data from research in psychology; (b) comparing a larger set of meta-analytic methods that, to our knowledge, have yet to be directly compared; and (c) discussing how our results can facilitate sensitivity analysis in meta-analysis.1\n\n## Disclosures\n\nR (R Core Team, 2016) scripts for our analyses and simulation are available at the Open Science Framework (https://osf.io/rf3ys).",
    "is_useful": true,
    "question": "What are some challenges associated with simulation studies in the context of open science?"
  },
  {
    "text": "These problems are often not discussed in reports of simulation studies, and indeed, many of the reports just cited explicitly or implicitly\u2014recommended the use of a single method, despite the fact that each study examined performance of only a handful of correction methods in only a limited subset of possible conditions.\n\nIn this article, we do not identify a single method that we believe should be used in all situations. Instead, we aim to add to the existing literature by (a) exploring a further set of conditions that may plausibly represent real data from research in psychology; (b) comparing a larger set of meta-analytic methods that, to our knowledge, have yet to be directly compared; and (c) discussing how our results can facilitate sensitivity analysis in meta-analysis.1\n\n## Disclosures\n\nR (R Core Team, 2016) scripts for our analyses and simulation are available at the Open Science Framework (https://osf.io/rf3ys). Furthermore, we have made available interactive figures and tables that allow a detailed exploration of the results (http://www.shinyapps.org/ apps/metaExplorer/). Supplemental material, which includes a comprehensive presentation of our results, is also available at the Open Science Framework (https://osf.io/rf3ys). We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study.\n\n## Method\n\n## *Simulation*\n\nWe simulated the number of meta-analyzed studies, k, as one of four values (10, 30, 60, 100).",
    "is_useful": true,
    "question": "What are the main objectives of the study in relation to meta-analysis methods?"
  },
  {
    "text": "Furthermore, we have made available interactive figures and tables that allow a detailed exploration of the results (http://www.shinyapps.org/ apps/metaExplorer/). Supplemental material, which includes a comprehensive presentation of our results, is also available at the Open Science Framework (https://osf.io/rf3ys). We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study.\n\n## Method\n\n## *Simulation*\n\nWe simulated the number of meta-analyzed studies, k, as one of four values (10, 30, 60, 100). All simulated individual studies had a two-group experimental design, so all effect sizes took the form of a standardized mean difference, Cohen's d. Notably, there is reason to think that this may be the most commonly used effect-size measure in psychology (see Table S1 in Fanelli, Costas, & Ioannidis, 2017). Cohen's d is an estimate of the true underlying effect, \u03b4, which we chose to simulate as taking one of four values (0, 0.2, 0.5, 0.8), corresponding to the null hypothesis and Cohen's rule-of-thumb values for small, medium, and large effects, respectively.\n\n*Heterogeneity.* As mentioned, variation in the true underlying effect, \u03b4, is described by the heterogeneity\n\n![](_page_3_Figure_1.jpeg)\n\nFig. 1.",
    "is_useful": true,
    "question": "What resources are available for detailed exploration of study results in open science?"
  },
  {
    "text": "The x-axis has been truncated at n = 400 for better visibility. This figure is available at https://osf.io/av285/, under a CC-BY4.0 license.\n\nparameter, \u03c4. We simulated three values for \u03c4 (0, 0.2, 0.4)2 that may plausibly represent research in psychology: In an analysis of 187 meta-analyses that used standardized mean differences and were published in *Psychological Bulletin* from 1990 through 2013 (van Erp, Verhagen, Grasman, & Wagenmakers, 2017), 50% of all estimates of \u03c4 were smaller than 0.2, and 80% were smaller than 0.4.3\n\n*Study-level data.* Independent samples were randomly generated for the control and experimental groups; observations in the control group were drawn from a normal distribution with a mean of 0 and standard deviation of 1, and observations in the experimental group were drawn from a normal distribution with a mean of Ti and standard deviation of 1. Ti was defined as the sum of \u03b4 and ui , where ui was drawn from a normal distribution with a mean of 0 and standard deviation of \u03c4. Note that Ti , therefore, represented a study-specific true effect that varied randomly if \u03c4 was greater than 0. Cohen's d and the associated variance, v, were calculated for each simulated study, and a two-tailed independent-samples t test was applied to generate a t value and a p value.",
    "is_useful": true,
    "question": "What statistical measures and techniques are commonly utilized in analyzing research effects in psychology?"
  },
  {
    "text": "Figure 1 compares the empirical per-group sample-size distribution with the best-fitting curve.\n\n*Publication bias.* For the simulation of publication bias, we used two censoring functions (\"medium publication bias\" and \"high publication bias\") that mapped a probability that a study was published to the study's onetailed p-value. If the effect was in the \"correct\" direction, both functions returned a 100% probability of publication when *pone-tailed* was less than .025. Studies with \"marginally significant\" effects, .025 \u2264 *pone-tailed* < .05, were published with an exponentially decreasing probability that reached 20% in the medium-publication-bias condition and 5% in the strong-publication-bias condition at *pone-tailed* = .05. The probability of publication then remained constant for all p values up to, but not including, .5. If the effect was in the \"wrong\" direction (*pone-tailed* \u2265 .5), the probability of publication was constant at 5% in the mediumpublication-bias condition and 0% in the strong-publicationbias condition for all values of *pone-tailed* up to, but not including, .995. The probability of publication then increased exponentially until, at *pone-tailed* = .9995, it reached a constant level of 50% in the medium-publication-bias condition and 20% in the strong-publication-bias condition (see Fig. 2).",
    "is_useful": true,
    "question": "How does publication bias influence the probability of a study being published based on its p-value?"
  },
  {
    "text": "The probability of publication then remained constant for all p values up to, but not including, .5. If the effect was in the \"wrong\" direction (*pone-tailed* \u2265 .5), the probability of publication was constant at 5% in the mediumpublication-bias condition and 0% in the strong-publicationbias condition for all values of *pone-tailed* up to, but not including, .995. The probability of publication then increased exponentially until, at *pone-tailed* = .9995, it reached a constant level of 50% in the medium-publication-bias condition and 20% in the strong-publication-bias condition (see Fig. 2). In our simulation, a random Bernoulli draw using the probability computed by these censoring functions determined whether a study was published.4 Studies were continually simulated until the target number of k studies had been reached. In the no-publication-bias condition, all studies were included regardless of the value of *pone-tailed*.\n\nTo our knowledge, this specific implementation of publication bias is comparable to, but different from, the implementations used in previous simulation studies (e.g., Bayarri & DeGroot, 1991; Guan & Vandekerckhove, 2016).",
    "is_useful": true,
    "question": "How does publication bias influence the probability of study publication based on different p values?"
  },
  {
    "text": "The probability of publication then increased exponentially until, at *pone-tailed* = .9995, it reached a constant level of 50% in the medium-publication-bias condition and 20% in the strong-publication-bias condition (see Fig. 2). In our simulation, a random Bernoulli draw using the probability computed by these censoring functions determined whether a study was published.4 Studies were continually simulated until the target number of k studies had been reached. In the no-publication-bias condition, all studies were included regardless of the value of *pone-tailed*.\n\nTo our knowledge, this specific implementation of publication bias is comparable to, but different from, the implementations used in previous simulation studies (e.g., Bayarri & DeGroot, 1991; Guan & Vandekerckhove, 2016). Our primary reason for choosing this approach was that we did not want our publication-bias functions to exactly match those assumed by any of the biascorrecting methods being examined (e.g., Iyengar & Greenhouse, 1988; McShane et al., 2016), as this might result in an overly optimistic assessment of those methods' performance (Simonsohn, Simmons, & Nelson, 2017). Furthermore, we wanted to test whether results obtained previously with more straightforward\n\n![](_page_4_Figure_1.jpeg)\n\nFig. 2. Implementation of publication bias in the simulation.",
    "is_useful": true,
    "question": "How does publication bias affect the probability of publication in research studies?"
  },
  {
    "text": "To our knowledge, this specific implementation of publication bias is comparable to, but different from, the implementations used in previous simulation studies (e.g., Bayarri & DeGroot, 1991; Guan & Vandekerckhove, 2016). Our primary reason for choosing this approach was that we did not want our publication-bias functions to exactly match those assumed by any of the biascorrecting methods being examined (e.g., Iyengar & Greenhouse, 1988; McShane et al., 2016), as this might result in an overly optimistic assessment of those methods' performance (Simonsohn, Simmons, & Nelson, 2017). Furthermore, we wanted to test whether results obtained previously with more straightforward\n\n![](_page_4_Figure_1.jpeg)\n\nFig. 2. Implementation of publication bias in the simulation. The graphs show the probability of publication as a function of the one-tailed p value of the simulated results in (a) the no-publication-bias condition, (b) the medium-publication-bias condition, and (c) the strong-publication-bias condition. The x-axes have a logarithmic scale on both sides of *pone-tailed* = .5 to increase the visibility of the function at the high and low ends of the scale. This figure is available at https://osf.io/ f6esc/, under a CC-BY4.0 license.\n\npublication-bias functions would be robust to our more nuanced implementation.\n\n*Questionable research practices.",
    "is_useful": true,
    "question": "What is the purpose of using a specific implementation of publication bias in research studies?"
  },
  {
    "text": "Furthermore, we wanted to test whether results obtained previously with more straightforward\n\n![](_page_4_Figure_1.jpeg)\n\nFig. 2. Implementation of publication bias in the simulation. The graphs show the probability of publication as a function of the one-tailed p value of the simulated results in (a) the no-publication-bias condition, (b) the medium-publication-bias condition, and (c) the strong-publication-bias condition. The x-axes have a logarithmic scale on both sides of *pone-tailed* = .5 to increase the visibility of the function at the high and low ends of the scale. This figure is available at https://osf.io/ f6esc/, under a CC-BY4.0 license.\n\npublication-bias functions would be robust to our more nuanced implementation.\n\n*Questionable research practices.* We studied four forms of QRPs: (a) optional removal of outliers, (b) optional selection between two dependent variables, (c) optional use of moderators, and (d) optional stopping. Each data set that had QRPs applied to it was designed to simulate a study with a 2 (group: experimental vs. control) \u00d7 2 (level of the moderator: Level 1 vs. Level 2) design and two dependent variables. Each dependent variable was measured across n observations.",
    "is_useful": true,
    "question": "What are some forms of questionable research practices that can affect the integrity of scientific studies?"
  },
  {
    "text": "The x-axes have a logarithmic scale on both sides of *pone-tailed* = .5 to increase the visibility of the function at the high and low ends of the scale. This figure is available at https://osf.io/ f6esc/, under a CC-BY4.0 license.\n\npublication-bias functions would be robust to our more nuanced implementation.\n\n*Questionable research practices.* We studied four forms of QRPs: (a) optional removal of outliers, (b) optional selection between two dependent variables, (c) optional use of moderators, and (d) optional stopping. Each data set that had QRPs applied to it was designed to simulate a study with a 2 (group: experimental vs. control) \u00d7 2 (level of the moderator: Level 1 vs. Level 2) design and two dependent variables. Each dependent variable was measured across n observations. The moderator divided the simulated data set in half in a way that was independent of the dependent variable (i.e., the moderator had no main effect on the dependent variable) and the treatment (i.e., there was no collinearity between moderator and treatment). The Pearson's correlation between the two dependent variables was .20.\n\nQRPs were applied so as to simulate the behavior of a researcher fishing for statistical significance.",
    "is_useful": true,
    "question": "What practices contribute to publication bias in research?"
  },
  {
    "text": "*Questionable research practices.* We studied four forms of QRPs: (a) optional removal of outliers, (b) optional selection between two dependent variables, (c) optional use of moderators, and (d) optional stopping. Each data set that had QRPs applied to it was designed to simulate a study with a 2 (group: experimental vs. control) \u00d7 2 (level of the moderator: Level 1 vs. Level 2) design and two dependent variables. Each dependent variable was measured across n observations. The moderator divided the simulated data set in half in a way that was independent of the dependent variable (i.e., the moderator had no main effect on the dependent variable) and the treatment (i.e., there was no collinearity between moderator and treatment). The Pearson's correlation between the two dependent variables was .20.\n\nQRPs were applied so as to simulate the behavior of a researcher fishing for statistical significance. To test different levels of severity, we created three *individual QRP strategies* a simulated researcher could adopt: (a) pure (no use of QRPs), (b) moderate usage (use of optional dependent variables and the addition of 3 observations per cell for up to three data-collection efforts), and (c) strong usage (optional removal of outliers, use of optional dependent variables and optional moderators, and the addition of 3 observations per cell for up to five data-collection efforts).\n\nIn the case of the strong-usage strategy, the simulated researcher first tested the main effect of experimental manipulation on the first dependent variable.",
    "is_useful": true,
    "question": "What are some forms of questionable research practices that can affect the integrity of scientific studies?"
  },
  {
    "text": "The Pearson's correlation between the two dependent variables was .20.\n\nQRPs were applied so as to simulate the behavior of a researcher fishing for statistical significance. To test different levels of severity, we created three *individual QRP strategies* a simulated researcher could adopt: (a) pure (no use of QRPs), (b) moderate usage (use of optional dependent variables and the addition of 3 observations per cell for up to three data-collection efforts), and (c) strong usage (optional removal of outliers, use of optional dependent variables and optional moderators, and the addition of 3 observations per cell for up to five data-collection efforts).\n\nIn the case of the strong-usage strategy, the simulated researcher first tested the main effect of experimental manipulation on the first dependent variable. If this effect was not statistically significant and positive, the simulated researcher removed outliers (defined as observations with a z score greater than |2|).5 If this second test was not positive and significant, the simulated researcher moved to the second dependent variable and repeated these steps. If no positive and significant effect was found, the researcher moved back to the first dependent variable and tested for an interaction effect between the experimental manipulation and the moderator. If there was a significant interaction, the researcher compared the experimental and control groups in only the subgroup defined by the first level of the moderator. This examination was conducted first with and then without outliers. If no positive and significant effect was found, the subgroup defined by the second level of the moderator was assessed in the same way.",
    "is_useful": true,
    "question": "What are some strategies a researcher might adopt to test for statistical significance while managing data collection practices?"
  },
  {
    "text": "Thus, for each data-collection effort, simulated researchers could potentially apply 12 comparisons. If none of these analyses produced a positive and significant effect, the first test (comparison of the experimental group and the control group on the first dependent variable, with outliers untouched and no subgroups created with the moderator) was taken as the final result. The moderate-usage strategy represented a subset of the approach described in the previous paragraph, in combination with less additional data collection.\n\nGiven the sample sizes of our simulated primary studies, the moderate QRP strategy resulted in an inflated false-positive rate of 9% (computed in conditions without heterogeneity and publication bias, and counting only directionally consistent results), and the strong QRP strategy resulted in a false-positive rate of 27%. Note that more aggressive p-hacking beyond our strong setting is easily possible, for example, by examining even more dependent variables or excluding only directional outliers. Indeed, Simmons et al. (2011) reported a false positive rate of 61% produced by combining certain types of p-hacking.\n\nAs it is unlikely that every researcher in a field applies QRPs in the same fashion, we defined three *QRP environments* to describe possible prototypical research fields characterized by different specific severities of QRP application, according to the strategies of individual researchers. In the *no-QRP* environment, 100% of the simulated researchers adopted the pure strategy.",
    "is_useful": true,
    "question": "What impact do questionable research practices (QRPs) have on the false-positive rates in scientific studies?"
  },
  {
    "text": "Given the sample sizes of our simulated primary studies, the moderate QRP strategy resulted in an inflated false-positive rate of 9% (computed in conditions without heterogeneity and publication bias, and counting only directionally consistent results), and the strong QRP strategy resulted in a false-positive rate of 27%. Note that more aggressive p-hacking beyond our strong setting is easily possible, for example, by examining even more dependent variables or excluding only directional outliers. Indeed, Simmons et al. (2011) reported a false positive rate of 61% produced by combining certain types of p-hacking.\n\nAs it is unlikely that every researcher in a field applies QRPs in the same fashion, we defined three *QRP environments* to describe possible prototypical research fields characterized by different specific severities of QRP application, according to the strategies of individual researchers. In the *no-QRP* environment, 100% of the simulated researchers adopted the pure strategy. In the *medium-QRP* environment, 30% of the simulated researchers adopted the pure strategy, 50% adopted the moderate strategy, and 20% adopted the strong strategy; this mixture led to a false-positive rate of 11%. In the *high-QRP* environment, 10% of the simulated researchers adopted the pure strategy, 40% adopted the moderate strategy, and 50% adopted the strong strategy; this mixture led to a 17% false-positive rate.\n\nNot all QRPs have the same distorting impact on a meta-analysis.",
    "is_useful": true,
    "question": "What is the impact of different questionable research practices (QRPs) on false-positive rates in research studies?"
  },
  {
    "text": "As it is unlikely that every researcher in a field applies QRPs in the same fashion, we defined three *QRP environments* to describe possible prototypical research fields characterized by different specific severities of QRP application, according to the strategies of individual researchers. In the *no-QRP* environment, 100% of the simulated researchers adopted the pure strategy. In the *medium-QRP* environment, 30% of the simulated researchers adopted the pure strategy, 50% adopted the moderate strategy, and 20% adopted the strong strategy; this mixture led to a false-positive rate of 11%. In the *high-QRP* environment, 10% of the simulated researchers adopted the pure strategy, 40% adopted the moderate strategy, and 50% adopted the strong strategy; this mixture led to a 17% false-positive rate.\n\nNot all QRPs have the same distorting impact on a meta-analysis. Furthermore, some QRPs lead biascorrecting techniques to overestimate the true effect, whereas others lead to underestimation (van Aert et al., 2016). Our goal was not to investigate the differential impact of distinct QRPs on bias correction, but rather was to investigate some combinations of QRPs that may be plausible in real settings (John, Loewenstein, & Prelec, 2012).",
    "is_useful": true,
    "question": "What are the different environments defined for the application of questionable research practices (QRPs) in research fields?"
  },
  {
    "text": "In the *high-QRP* environment, 10% of the simulated researchers adopted the pure strategy, 40% adopted the moderate strategy, and 50% adopted the strong strategy; this mixture led to a 17% false-positive rate.\n\nNot all QRPs have the same distorting impact on a meta-analysis. Furthermore, some QRPs lead biascorrecting techniques to overestimate the true effect, whereas others lead to underestimation (van Aert et al., 2016). Our goal was not to investigate the differential impact of distinct QRPs on bias correction, but rather was to investigate some combinations of QRPs that may be plausible in real settings (John, Loewenstein, & Prelec, 2012). As there are infinite possibilities of how QRPs can be implemented, and an infinite number of ways in which these individual researcher strategies can be combined in QRP environments, our study is best considered a sensitivity analysis that explored the effect of a range of three plausible QRP environments on meta-analytic results. Our results do not necessarily generalize to other implementation of QRPs.\n\n*Design.* To summarize, we simulated data for 432 unique combinations of five fully crossed factors (Table 1). We simulated 1,000 meta-analytic data sets for each of the 432 conditions. For a random selection of conditions, we also simulated 10,000 meta-analytic data sets and computed the Monte Carlo simulation error (Koehler, Brown, & Haneuse, 2009). These comparisons clearly\n\n#### Table 1.",
    "is_useful": true,
    "question": "What strategies did researchers adopt in a high-QRP environment based on simulated data?"
  },
  {
    "text": "As there are infinite possibilities of how QRPs can be implemented, and an infinite number of ways in which these individual researcher strategies can be combined in QRP environments, our study is best considered a sensitivity analysis that explored the effect of a range of three plausible QRP environments on meta-analytic results. Our results do not necessarily generalize to other implementation of QRPs.\n\n*Design.* To summarize, we simulated data for 432 unique combinations of five fully crossed factors (Table 1). We simulated 1,000 meta-analytic data sets for each of the 432 conditions. For a random selection of conditions, we also simulated 10,000 meta-analytic data sets and computed the Monte Carlo simulation error (Koehler, Brown, & Haneuse, 2009). These comparisons clearly\n\n#### Table 1. Simulation Parameters\n\n| Experimental factor | Levels |\n| --- | --- |\n| True underlying effect (\u03b4) | 0, 0.2, 0.5, 0.8 |\n| Between-study heterogeneity (\u03c4) | 0, 0.2, 0.4 |\n| Number of studies in the meta-analytic sample (k) | 10, 30, 60, 100 |\n| Publication bias | None, medium, strong |\n| Questionable-research-practices (QRPs) environment | No QRPs, medium use of QRPs, high use of QRPs |\n\ndemonstrated that 1,000 simulated meta-analytic data sets lead to sufficiently stable estimates (see the supplemental material at https://osf.io/rf3ys).",
    "is_useful": true,
    "question": "What are some factors that can influence the results of meta-analyses in research?"
  },
  {
    "text": "## *Performance metrics*\n\nFor each meta-analytic method, to test the hypothesis that the estimate provided differed from zero, we evaluated the false-positive (Type I error) rate at \u03b4 = 0 and the true-positive rate (i.e., the statistical power) at \u03b4 = 0.2, 0.5, and 0.8.\n\nFollowing the recommendations of Burton, Altman, Royston, and Holder (2006), we measured the estimation performance of each method in terms of mean error, root mean squared error (RMSE), and 95% coverage probability (see Box 1).\n\n## *Meta-analytic methods*\n\nWe examined the performance of seven estimators: our baseline estimator, RE meta-analysis, and six estimators that adjust for bias. Further details on our specific implementations are available in the supplemental material at https://osf.io/rf3ys.\n\n*Random-effects meta-analysis.* We applied the RE meta-analysis as described earlier using the *metafor* package in R (Viechtbauer, 2010). This approach makes no adjustment for publication bias or QRPs. We used the restricted maximum likelihood method for estimating between-study variance.\n\n*Trim-and-fill method.* The trim-and-fill method (Duval & Tweedie, 2000) was introduced as a diagnostic test for publication bias and is based on the asymmetry of a funnel plot (a scatterplot showing effect-size estimates as a function of the standard error of those estimates).",
    "is_useful": true,
    "question": "What methods are commonly used to assess the performance and estimation accuracy of meta-analytic techniques in open science?"
  },
  {
    "text": "## *Meta-analytic methods*\n\nWe examined the performance of seven estimators: our baseline estimator, RE meta-analysis, and six estimators that adjust for bias. Further details on our specific implementations are available in the supplemental material at https://osf.io/rf3ys.\n\n*Random-effects meta-analysis.* We applied the RE meta-analysis as described earlier using the *metafor* package in R (Viechtbauer, 2010). This approach makes no adjustment for publication bias or QRPs. We used the restricted maximum likelihood method for estimating between-study variance.\n\n*Trim-and-fill method.* The trim-and-fill method (Duval & Tweedie, 2000) was introduced as a diagnostic test for publication bias and is based on the asymmetry of a funnel plot (a scatterplot showing effect-size estimates as a function of the standard error of those estimates). Publication bias introduces clear rightward asymmetry in a funnel plot (see the supplemental material) because nonsignificant and negative observations are censored. The trim-and-fill method involves iteratively removing (i.e., trimming) observations from one side of the funnel plot until a criterion for symmetry is met, and then \"filling\" these observations back into the funnel plot along with imputed observations that are identical to the trimmed observations but on the opposite side of the mean along the horizontal axis. Standard meta-analytic methods can then be applied to a data set including both observed and imputed studies.",
    "is_useful": true,
    "question": "What are some meta-analytic methods used to assess publication bias in research studies?"
  },
  {
    "text": "We used the restricted maximum likelihood method for estimating between-study variance.\n\n*Trim-and-fill method.* The trim-and-fill method (Duval & Tweedie, 2000) was introduced as a diagnostic test for publication bias and is based on the asymmetry of a funnel plot (a scatterplot showing effect-size estimates as a function of the standard error of those estimates). Publication bias introduces clear rightward asymmetry in a funnel plot (see the supplemental material) because nonsignificant and negative observations are censored. The trim-and-fill method involves iteratively removing (i.e., trimming) observations from one side of the funnel plot until a criterion for symmetry is met, and then \"filling\" these observations back into the funnel plot along with imputed observations that are identical to the trimmed observations but on the opposite side of the mean along the horizontal axis. Standard meta-analytic methods can then be applied to a data set including both observed and imputed studies.\n\nSeveral previous simulation studies suggest that, although the trim-and-fill method can correct for bias in some cases, it tends to be outperformed by other methods and generally fails as heterogeneity increases (e.g., Idris & Ruzni, 2012; Moreno et al., 2009; Peters, Sutton, Jones, Abrams, & Rushton, 2007; Simonsohn, Nelson, & Simmons, 2014; Terrin, Schmid, Lau, & Olkin, 2003). For example, Terrin et al.",
    "is_useful": true,
    "question": "What method is discussed for addressing publication bias in meta-analysis, and how does it function?"
  },
  {
    "text": "Overall, no conclusion has been reached on the best way to implement the trim-and-fill method, as its performance can vary widely depending on the version of the algorithm and the conditions in which it is used (Moreno et al., 2009; Peters et al., 2007). Therefore, we used the default algorithm provided by the *metafor* package. Notably, results for this method did not always converge. Across all conditions, it returned a valid estimate in 95% of data sets. Nonconvergence happened mostly when k was at or above 60 and publication bias was strong.\n\n*Weighted average of the adequately powered studies.* T. D. Stanley and Doucouliagos (2017) proposed the use of an intercept-only weighted-least-squares (WLS) metaregression estimator as a replacement for the naive fixed-effect and RE meta-analytic models. Simulation studies (T. D. Stanley, 2017; T. D. Stanley & Doucouliagos, 2017; T. D. Stanley, Doucouliagos, & Ioannidis, 2017) suggested that the WLS estimator performed on par with the fixed-effect and RE models when the assumptions underlying those models were true, but outperformed both of them when the assumptions were violated (e.g., in the presence of publication bias).\n\nResearchers have suggested extending this WLS estimator to reduce the impact of potential publication bias (Ioannidis, Stanley, & Doucouliagos, 2017; T. D. Stanley et al., 2017).",
    "is_useful": true,
    "question": "How can the impact of publication bias be reduced in meta-analyses?"
  },
  {
    "text": "* T. D. Stanley and Doucouliagos (2017) proposed the use of an intercept-only weighted-least-squares (WLS) metaregression estimator as a replacement for the naive fixed-effect and RE meta-analytic models. Simulation studies (T. D. Stanley, 2017; T. D. Stanley & Doucouliagos, 2017; T. D. Stanley, Doucouliagos, & Ioannidis, 2017) suggested that the WLS estimator performed on par with the fixed-effect and RE models when the assumptions underlying those models were true, but outperformed both of them when the assumptions were violated (e.g., in the presence of publication bias).\n\nResearchers have suggested extending this WLS estimator to reduce the impact of potential publication bias (Ioannidis, Stanley, & Doucouliagos, 2017; T. D. Stanley et al., 2017). In this extension, one first performs a WLS meta-analysis on all primary studies to obtain a (potentially biased) estimate of the true underlying effect. Then, one performs a second WLS meta-analysis on only those studies that had 80% statistical power to detect an effect of the size estimated by the first WLS meta-analysis, so as to obtain a weighted average of adequately powered (WAAP) studies. This approach is intended to avoid bias by discarding underpowered studies, which must overestimate the true effect to find statistical significance.",
    "is_useful": true,
    "question": "What statistical approach has been proposed to improve meta-analytic estimations by reducing the impact of publication bias? "
  },
  {
    "text": "Researchers have suggested extending this WLS estimator to reduce the impact of potential publication bias (Ioannidis, Stanley, & Doucouliagos, 2017; T. D. Stanley et al., 2017). In this extension, one first performs a WLS meta-analysis on all primary studies to obtain a (potentially biased) estimate of the true underlying effect. Then, one performs a second WLS meta-analysis on only those studies that had 80% statistical power to detect an effect of the size estimated by the first WLS meta-analysis, so as to obtain a weighted average of adequately powered (WAAP) studies. This approach is intended to avoid bias by discarding underpowered studies, which must overestimate the true effect to find statistical significance. If there are no adequately powered studies or only one adequately powered study in the data set, the WLS estimate for the entire data set is used. Thus, this conditional estimator, called WAAP-WLS, applies WAAP when there are at least two adequately powered studies and WLS otherwise.\n\nAcross all conditions, the WAAP-WLS method returned 77% WAAP and 23% WLS estimates. In small-k, small-\u03b4 conditions, there were not enough adequately powered studies, and 100% of estimates used WLS. In large-k, large-\u03b4 conditions, 100% of estimates used WAAP.",
    "is_useful": true,
    "question": "What methodology is used to reduce publication bias in research estimates, particularly through the analysis of adequately powered studies?"
  },
  {
    "text": "This approach is intended to avoid bias by discarding underpowered studies, which must overestimate the true effect to find statistical significance. If there are no adequately powered studies or only one adequately powered study in the data set, the WLS estimate for the entire data set is used. Thus, this conditional estimator, called WAAP-WLS, applies WAAP when there are at least two adequately powered studies and WLS otherwise.\n\nAcross all conditions, the WAAP-WLS method returned 77% WAAP and 23% WLS estimates. In small-k, small-\u03b4 conditions, there were not enough adequately powered studies, and 100% of estimates used WLS. In large-k, large-\u03b4 conditions, 100% of estimates used WAAP.\n\nPrevious simulation studies have suggested that the WAAP-WLS method is comparable to the WLS method, standard fixed-effects meta-analysis, and RE meta-analysis in the absence of heterogeneity and publication bias; however, as those conditions changed, the WAAP-WLS method has outperformed both WLS and standard meta-analysis (T. D. Stanley et al., 2017). The same simulation study suggested, however, that in terms of efficiency and overall bias, the WAAP-WLS method is outperformed by the precision-effect test/precisioneffect estimate with standard error (PET-PEESE) method (described later in this section).\n\np*-curve.",
    "is_useful": true,
    "question": "What method is designed to avoid bias by discarding underpowered studies and is compared to other meta-analysis techniques in terms of efficiency and bias?"
  },
  {
    "text": "In small-k, small-\u03b4 conditions, there were not enough adequately powered studies, and 100% of estimates used WLS. In large-k, large-\u03b4 conditions, 100% of estimates used WAAP.\n\nPrevious simulation studies have suggested that the WAAP-WLS method is comparable to the WLS method, standard fixed-effects meta-analysis, and RE meta-analysis in the absence of heterogeneity and publication bias; however, as those conditions changed, the WAAP-WLS method has outperformed both WLS and standard meta-analysis (T. D. Stanley et al., 2017). The same simulation study suggested, however, that in terms of efficiency and overall bias, the WAAP-WLS method is outperformed by the precision-effect test/precisioneffect estimate with standard error (PET-PEESE) method (described later in this section).\n\np*-curve.* A p-curve is the distribution of all statistically significant p values from the set of studies of interest (i.e., ps < .05; Simonsohn, Nelson, & Simmons, 2014). The shape of the p-curve is a function of the statistical power of the studies, which is itself a function of the sample sizes and the true effect size. When studies have no statistical power (i.e., when the null is true), the distribution of significant, independent p values is uniform between .00 and .05. With increasing power, this distribution becomes increasingly right skewed.",
    "is_useful": true,
    "question": "What factors influence the shape of a p-curve in statistical studies?"
  },
  {
    "text": "The same simulation study suggested, however, that in terms of efficiency and overall bias, the WAAP-WLS method is outperformed by the precision-effect test/precisioneffect estimate with standard error (PET-PEESE) method (described later in this section).\n\np*-curve.* A p-curve is the distribution of all statistically significant p values from the set of studies of interest (i.e., ps < .05; Simonsohn, Nelson, & Simmons, 2014). The shape of the p-curve is a function of the statistical power of the studies, which is itself a function of the sample sizes and the true effect size. When studies have no statistical power (i.e., when the null is true), the distribution of significant, independent p values is uniform between .00 and .05. With increasing power, this distribution becomes increasingly right skewed. Because the degree of right skew is a function of the average study power, one can use the degree of right skew in a p-curve to (a) test the absence of a real effect and (b) estimate the average effect size corrected for publication bias.\n\nSimonsohn, Nelson, and Simmons (2014) demonstrated that some typical QRPs cause the p-curve method to underestimate the true effect size. Later work by van Aert et al. (2016), however, suggested that bias can be upward or downward, depending on the specific type of QRPs.",
    "is_useful": true,
    "question": "What method can be used to estimate the average effect size corrected for publication bias and test the absence of a real effect based on the distribution of statistically significant p values?"
  },
  {
    "text": "The shape of the p-curve is a function of the statistical power of the studies, which is itself a function of the sample sizes and the true effect size. When studies have no statistical power (i.e., when the null is true), the distribution of significant, independent p values is uniform between .00 and .05. With increasing power, this distribution becomes increasingly right skewed. Because the degree of right skew is a function of the average study power, one can use the degree of right skew in a p-curve to (a) test the absence of a real effect and (b) estimate the average effect size corrected for publication bias.\n\nSimonsohn, Nelson, and Simmons (2014) demonstrated that some typical QRPs cause the p-curve method to underestimate the true effect size. Later work by van Aert et al. (2016), however, suggested that bias can be upward or downward, depending on the specific type of QRPs. Additional work demonstrated that the p-curve method overestimates the average true underlying effect when there is heterogeneity (Simonsohn, Nelson, & Simmons, 2014; van Aert et al., 2016).\n\nSimonsohn, Nelson, and Simmons (2014) interpreted the p-curve estimate as \"the average effect size one expects to get if one were to rerun all studies included in the p-curve\" (p. 667; see also Simmons, Nelson, & Simonsohn, 2018).",
    "is_useful": true,
    "question": "What factors influence the shape of the p-curve in relation to statistical power and effect size?"
  },
  {
    "text": "Simonsohn, Nelson, and Simmons (2014) demonstrated that some typical QRPs cause the p-curve method to underestimate the true effect size. Later work by van Aert et al. (2016), however, suggested that bias can be upward or downward, depending on the specific type of QRPs. Additional work demonstrated that the p-curve method overestimates the average true underlying effect when there is heterogeneity (Simonsohn, Nelson, & Simmons, 2014; van Aert et al., 2016).\n\nSimonsohn, Nelson, and Simmons (2014) interpreted the p-curve estimate as \"the average effect size one expects to get if one were to rerun all studies included in the p-curve\" (p. 667; see also Simmons, Nelson, & Simonsohn, 2018). In our view, however, meta-analysts generally aim to recover the average of the distribution of all true effects related to the phenomenon of interest (i.e., \u03b4). Indeed, that is the purpose of the other estimators we examined. For consistency, therefore, we interpret p-curve results in the same fashion; however, in the supplemental material, we also assess this method's ability to recover the average true effect size of the studies submitted to it.\n\nWe implemented the p-curve method as recommended by Simonsohn, Nelson, and Simmons (2014), with the following settings. Only statistically significant and directionally consistent studies were submitted to the analysis. Any studies with significant but negative effects were discarded.",
    "is_useful": true,
    "question": "How does the p-curve method relate to the estimation of true effect sizes in the context of questionable research practices (QRPs)?"
  },
  {
    "text": "We implemented the p-curve method as recommended by Simonsohn, Nelson, and Simmons (2014), with the following settings. Only statistically significant and directionally consistent studies were submitted to the analysis. Any studies with significant but negative effects were discarded. Consequently, when no studies with significant positive effects were in a set, this method did not return an estimate (0.8% of all simulations). Across all conditions, the method returned an estimate in 99.2% of all simulated data sets. Not surprisingly, the method failed to produce an estimate almost exclusively when the set (k) consisted of 10 studies, the true effect (\u03b4) was 0, and there was no publication bias.\n\nIn some cases, the p-curve method can return an estimate with a negative sign even though all included studies yielded effects with positive signs. It was our understanding that one should interpret only nonnegative effect-size estimates from the p-curve method, because a negative estimate based on a series of p values just below .05 is likely to indicate that the null hypothesis is true and there has been intensive phacking, rather than that there is a true effect in the opposite direction. Negative effect-size estimates obtained with the p-curve method were set to zero in our study (see recommendations from van Aert et al., 2016).6\n\nIn testing for the presence of an effect, we relied on the test for evidential value (i.e., the test for right skewness) for the full p-curve (Simonsohn, Simmons, & Nelson, 2015).",
    "is_useful": true,
    "question": "What statistical methods are discussed for analyzing the evidential value of studies in open science?"
  },
  {
    "text": "In some cases, the p-curve method can return an estimate with a negative sign even though all included studies yielded effects with positive signs. It was our understanding that one should interpret only nonnegative effect-size estimates from the p-curve method, because a negative estimate based on a series of p values just below .05 is likely to indicate that the null hypothesis is true and there has been intensive phacking, rather than that there is a true effect in the opposite direction. Negative effect-size estimates obtained with the p-curve method were set to zero in our study (see recommendations from van Aert et al., 2016).6\n\nIn testing for the presence of an effect, we relied on the test for evidential value (i.e., the test for right skewness) for the full p-curve (Simonsohn, Simmons, & Nelson, 2015). This test is conceptually\u2014but not statistically\u2014equivalent to a test for \u00b5 > 0. Furthermore, p-curve estimation does not provide confidence intervals, so we could not assess this aspect of estimation for this method.\n\np*-uniform.* As does the p-curve method, the p-uniform method considers only the statistically significant results. It is based on the idea that the distribution of p values is uniform conditional on the population effect size (van Assen et al., 2015). Hence, it focuses on the p-value distribution under the alternative hypothesis, and it yields a fixed-effects estimate of the true effect by finding the value d* that makes the conditional distribution of p values as uniform as possible.",
    "is_useful": true,
    "question": "What are the implications of negative effect-size estimates in p-curve analysis according to open science practices?"
  },
  {
    "text": "This test is conceptually\u2014but not statistically\u2014equivalent to a test for \u00b5 > 0. Furthermore, p-curve estimation does not provide confidence intervals, so we could not assess this aspect of estimation for this method.\n\np*-uniform.* As does the p-curve method, the p-uniform method considers only the statistically significant results. It is based on the idea that the distribution of p values is uniform conditional on the population effect size (van Assen et al., 2015). Hence, it focuses on the p-value distribution under the alternative hypothesis, and it yields a fixed-effects estimate of the true effect by finding the value d* that makes the conditional distribution of p values as uniform as possible.\n\nThe p-uniform method provides a hypothesis test, an estimate of the bias-corrected effect size, and a confidence interval around that estimate. Computationally, the p-curve and p-uniform methods differ only in that they use different implementations of the estimation algorithm, so in general they are expected to have similar strengths and weaknesses (McShane et al., 2016). For the computation of the p-uniform estimate, we used the Irwin-Hall estimator as implemented in the *puniform* package (van Aert, 2017) and recommended by van Aert et al. (2016). We also followed van Aert et al.",
    "is_useful": true,
    "question": "What are the main features and differences between the p-curve method and the p-uniform method in statistical estimation?"
  },
  {
    "text": "Hence, it focuses on the p-value distribution under the alternative hypothesis, and it yields a fixed-effects estimate of the true effect by finding the value d* that makes the conditional distribution of p values as uniform as possible.\n\nThe p-uniform method provides a hypothesis test, an estimate of the bias-corrected effect size, and a confidence interval around that estimate. Computationally, the p-curve and p-uniform methods differ only in that they use different implementations of the estimation algorithm, so in general they are expected to have similar strengths and weaknesses (McShane et al., 2016). For the computation of the p-uniform estimate, we used the Irwin-Hall estimator as implemented in the *puniform* package (van Aert, 2017) and recommended by van Aert et al. (2016). We also followed van Aert et al.'s recommendation to set the estimate to zero if the average of all significant p values was larger than .025, because the average p value is lower than .025 when there is a true positive effect.7\n\nAs is the case for the p-curve method, the p-uniform method does not return an estimate if there are no studies with significant positive effects (0.8% of all simulations). Across all conditions, the p-uniform method returned an estimate in 99.2% of all simulated data sets; in 10.4% of all simulations, the estimate was replaced by zero.\n\n*PET, PEESE, and PET-PEESE.",
    "is_useful": true,
    "question": "What methods are used to provide a hypothesis test and estimate effect sizes in open science research?"
  },
  {
    "text": "For the computation of the p-uniform estimate, we used the Irwin-Hall estimator as implemented in the *puniform* package (van Aert, 2017) and recommended by van Aert et al. (2016). We also followed van Aert et al.'s recommendation to set the estimate to zero if the average of all significant p values was larger than .025, because the average p value is lower than .025 when there is a true positive effect.7\n\nAs is the case for the p-curve method, the p-uniform method does not return an estimate if there are no studies with significant positive effects (0.8% of all simulations). Across all conditions, the p-uniform method returned an estimate in 99.2% of all simulated data sets; in 10.4% of all simulations, the estimate was replaced by zero.\n\n*PET, PEESE, and PET-PEESE.* The precision-effect test (PET; T. D. Stanley & Doucouliagos, 2014) is a metaregression approach to adjusting for small-study effects (see the supplemental material; see also the closely related Egger's test for publication bias\u2014Egger, Smith, Schneider, & Minder, 1997). Small-study effects are said to exist when the observed effect size gets larger as the standard error grows (i.e., as the sample size shrinks).",
    "is_useful": true,
    "question": "What is the p-uniform method and how does it relate to the computation of estimates in research studies?"
  },
  {
    "text": "Across all conditions, the p-uniform method returned an estimate in 99.2% of all simulated data sets; in 10.4% of all simulations, the estimate was replaced by zero.\n\n*PET, PEESE, and PET-PEESE.* The precision-effect test (PET; T. D. Stanley & Doucouliagos, 2014) is a metaregression approach to adjusting for small-study effects (see the supplemental material; see also the closely related Egger's test for publication bias\u2014Egger, Smith, Schneider, & Minder, 1997). Small-study effects are said to exist when the observed effect size gets larger as the standard error grows (i.e., as the sample size shrinks). One cause of this pattern is publication bias, although other benign causes also exist (T. D. Stanley & Doucouliagos, 2014; Sterne, Gavaghan, & Egger, 2000).\n\nThe PET method uses a weighted-least-squares regression in which effect size is regressed on its standard error: di = b0 + b1sei + ei , where b0 and b1 are the intercept and slope terms describing the linear relationship between the ith effect-size estimate, di , and its associated standard error, sei . The regression model is weighted by the inverse of the variances (i.e., the squared standard errors) of the effect-size estimates.",
    "is_useful": true,
    "question": "What is the precision-effect test (PET) used for in the context of research and how does it address small-study effects?"
  },
  {
    "text": "Small-study effects are said to exist when the observed effect size gets larger as the standard error grows (i.e., as the sample size shrinks). One cause of this pattern is publication bias, although other benign causes also exist (T. D. Stanley & Doucouliagos, 2014; Sterne, Gavaghan, & Egger, 2000).\n\nThe PET method uses a weighted-least-squares regression in which effect size is regressed on its standard error: di = b0 + b1sei + ei , where b0 and b1 are the intercept and slope terms describing the linear relationship between the ith effect-size estimate, di , and its associated standard error, sei . The regression model is weighted by the inverse of the variances (i.e., the squared standard errors) of the effect-size estimates. The intercept b0 represents the estimated effect size when the standard error is zero; it is an estimate of the true underlying effect that has been corrected for publication bas and other small-study effects. Of course, if small-study effects have many benign causes, there may be substantial overcorrection.\n\nA closely related approach computes what is called the precision-effect estimate with standard error (PEESE; T. D. Stanley & Doucouliagos, 2014). In this method, a quadratic relationship between effect size and standard error is fitted to the data. The rationale is that if there is some true effect, low-precision studies are poorly powered and publishable only when the effect is badly overestimated.",
    "is_useful": true,
    "question": "What are small-study effects, and how do they relate to publication bias in research findings?"
  },
  {
    "text": "The regression model is weighted by the inverse of the variances (i.e., the squared standard errors) of the effect-size estimates. The intercept b0 represents the estimated effect size when the standard error is zero; it is an estimate of the true underlying effect that has been corrected for publication bas and other small-study effects. Of course, if small-study effects have many benign causes, there may be substantial overcorrection.\n\nA closely related approach computes what is called the precision-effect estimate with standard error (PEESE; T. D. Stanley & Doucouliagos, 2014). In this method, a quadratic relationship between effect size and standard error is fitted to the data. The rationale is that if there is some true effect, low-precision studies are poorly powered and publishable only when the effect is badly overestimated. On the other hand, high-precision studies are well powered and routinely publishable without such overestimation. Thus, publication bias (and the observed small-study effect) is expected to be stronger when the standard error is larger. A quadratic relationship can model such differences in bias across standard errors. The PEESE method uses a weighted-least-squares regression model, in which effect size is regressed on the square of the standard error: di = b0 + b1sei 2 + ei . As in the PET method, the weights are the inverse of the variances and the intercept is interpreted as an estimate of the true underlying effect that is uninfluenced by small-study effects.",
    "is_useful": true,
    "question": "How do regression models address the impact of publication bias and small-study effects in estimating true underlying effects in research?"
  },
  {
    "text": "In an attempt to offset the opposite biases of these methods, T. D. Stanley and Doucouliagos (2014) proposed the conditional PET-PEESE estimator. In this approach the statistical significance of the PET estimate is used to decide whether the PET or the PEESE estimate is taken as the final estimate. When the PET estimate is statistically nonsignificant (i.e., the estimated true effect is not distinguishable from zero) in a one-tailed test with \u03b1 = .05, the PET estimate is taken as the PET-PEESE estimate. In contrast, when the PET estimate is statistically significant, the PEESE estimate is used as the PET-PEESE estimate. For brevity, we focus only on describing the performance of the conditional PET-PEESE estimator, but in our discussion of sensitivity analysis, we recommend using all three methods: PET, PEESE, and PET-PEESE.\n\nAlthough initial simulation results indicated that the PET-PEESE estimator's performance was promising (T. D. Stanley & Doucouliagos, 2014), two later simulations revealed some weaknesses. In one, the standard RE meta-analysis estimator outperformed the PET and PEESE estimators in some ways; for example, it provided greater estimation efficiency (lower mean squared error) when heterogeneity was present (Reed, 2015).",
    "is_useful": true,
    "question": "What is the purpose of the conditional PET-PEESE estimator in statistical analysis?"
  },
  {
    "text": "In one, the standard RE meta-analysis estimator outperformed the PET and PEESE estimators in some ways; for example, it provided greater estimation efficiency (lower mean squared error) when heterogeneity was present (Reed, 2015). The other simulation showed unacceptable performance of the PET-PEESE estimator under conditions that seem common in psychology\u2014a small number of studies, small samples across all studies, and high heterogeneity (T. D. Stanley, 2017).\n\n*Selection model.* Selection-model approaches to mitigation of bias in meta-analysis have been in use for some time (Hedges, 1984; Hedges & Vevea, 1996; Iyengar & Greenhouse, 1988). We employed the three-parameter selection model (3PSM) as developed by Iyengar and Greenhouse (1988) and recently discussed by McShane et al. (2016). This model's three parameters represent the average true underlying effect, \u03b4; the heterogeneity of the random effect sizes, \u03c42 ; and the probability p1 that a nonsignificant effect enters the literature. The last parameter, p1, is modeled by a step function with a single cut point at p = .025 (one-tailed), which corresponds to a twotailed p value of .05. This cut point divides the range of possible p values into two bins: significant and nonsignificant. The three parameters are estimated using maximum likelihood.",
    "is_useful": true,
    "question": "What are some challenges highlighted in the meta-analysis methodologies used in psychology research?"
  },
  {
    "text": "Across all conditions, the 3PSM method returned an estimate in 91.5% of all simulated data sets. Estimation failed mostly when there were 100 studies in the set, the true effect (\u03b4) was 0.2, and there was at least medium publication bias or a medium QRP environment (or both).\n\nSeveral simulation studies of selection models have been conducted previously (e.g., Hedges & Vevea, 1996; Terrin et al., 2003). However, to our knowledge, only one examined the specific method we implemented: McShane et al. (2016) compared the 3PSM method with the p-uniform and p-curve methods, both of which can be understood as single-parameter selection models (i.e., only \u03b4 is estimated, publication bias is set to 100%, and heterogeneity is set to 0). In that study, the 3PSM approach clearly provided the best estimation and hypothesis testing (a) when \u03b4 was no greater than 0.30 and \u03c4 exceeded 0 and (b) when incomplete bias allowed some nonsignificant studies to be published.\n\n## Results\n\nWe simulated 1,000 data sets under 432 unique conditions (Table 1) and analyzed each with seven different meta-analytic methods. Here, we avoid an exhaustive presentation of the results and provide instead a more focused report.",
    "is_useful": true,
    "question": "What factor strongly influences the performance of the 3PSM method in estimating true effects in meta-analysis?"
  },
  {
    "text": "However, to our knowledge, only one examined the specific method we implemented: McShane et al. (2016) compared the 3PSM method with the p-uniform and p-curve methods, both of which can be understood as single-parameter selection models (i.e., only \u03b4 is estimated, publication bias is set to 100%, and heterogeneity is set to 0). In that study, the 3PSM approach clearly provided the best estimation and hypothesis testing (a) when \u03b4 was no greater than 0.30 and \u03c4 exceeded 0 and (b) when incomplete bias allowed some nonsignificant studies to be published.\n\n## Results\n\nWe simulated 1,000 data sets under 432 unique conditions (Table 1) and analyzed each with seven different meta-analytic methods. Here, we avoid an exhaustive presentation of the results and provide instead a more focused report. However, all of our findings, including information on convergence probabilities and exact values for mean error, RMSE, and coverage probabilities for all conditions, are available in Table 2 in the supplemental material (https://osf.io/rf3ys). We also have made several interactive figures available (http://www .shinyapps.org/apps/metaExplorer/) so researchers can explore combinations of conditions that they find to be particularly relevant to their own work.\n\nIn the following sections, we provide figures only for conditions in which \u03b4 = 0 (i.e., the null hypothesis is true) or \u03b4 = 0.5 (i.e., the alternative hypothesis that the effect size is 0.5 is true).",
    "is_useful": true,
    "question": "What type of method comparison was conducted to assess the effectiveness of the 3PSM method in the context of meta-analysis?"
  },
  {
    "text": "## Results\n\nWe simulated 1,000 data sets under 432 unique conditions (Table 1) and analyzed each with seven different meta-analytic methods. Here, we avoid an exhaustive presentation of the results and provide instead a more focused report. However, all of our findings, including information on convergence probabilities and exact values for mean error, RMSE, and coverage probabilities for all conditions, are available in Table 2 in the supplemental material (https://osf.io/rf3ys). We also have made several interactive figures available (http://www .shinyapps.org/apps/metaExplorer/) so researchers can explore combinations of conditions that they find to be particularly relevant to their own work.\n\nIn the following sections, we provide figures only for conditions in which \u03b4 = 0 (i.e., the null hypothesis is true) or \u03b4 = 0.5 (i.e., the alternative hypothesis that the effect size is 0.5 is true). The figures display the effects of increasing heterogeneity (from \u03c4 = 0 to \u03c4 = 0.4) and of increasing numbers of studies (from k = 10 to k = 100). Figure 3 shows both the false-positive rates (when \u03b4 = 0) and the statistical power (when \u03b4 = 0.50) of each method. In Figure 4, rather than providing exact values for mean error, RMSE, and coverage, we display the distributions of mean effect-size estimates with 95% quantile ranges.",
    "is_useful": true,
    "question": "What resources are made available to researchers for exploring combinations of conditions in meta-analytic methods?"
  },
  {
    "text": "successful computations. This figure is available at https://osf.io/hjzfq/, under a CC-BY4.0 license.\n\nc\n\nin the supplemental material reports the exact percentages of returned estimates for each of the other four methods in each condition.\n\n## *No publication bias, no QRPs*\n\n*Type I error rate.* When the null hypothesis was true and there was no heterogeneity, most methods had appropriate Type I error rates, although the error rates for the 3PSM and p-uniform methods were more conservative (0\u20133%) than the nominal alpha rate. The p-curve and p-uniform methods sometimes failed to provide an estimate because of the scarcity of statistically significant results, especially at k = 10.\n\nThe addition of heterogeneity led to slightly higher Type I error rates (< 10%) for RE meta-analysis and the trim-and-fill method. Type I error rates rose moderately (10\u201330%) for the WAAP-WLS and PET-PEESE methods and increased considerably for the p-curve and p-uniform methods (8\u201356%). Adding heterogeneity did not increase Type I error rates for the 3PSM method, which remained excessively conservative.\n\n*Power.* With a sample size (k) of 10 and no heterogeneity, RE meta-analysis offered the best power, followed closely by the trim-and-fill and 3PSM methods. Power was poorer for the p-curve, PET-PEESE, and WAAP-WLS estimators, and markedly poorer for the p-uniform estimator.",
    "is_useful": true,
    "question": "What factors can influence the Type I error rates and power in statistical methods used in open science research?"
  },
  {
    "text": "*RMSE.* In the absence of heterogeneity, RE meta-analysis provided the most efficient results and the smallest RMSE. RMSE was slightly greater for the trim-and-fill and WAAP-WLS methods, moderately greater for the 3PSM and PET-PEESE methods, and noticeably greater for the p-uniform and p-curve methods. RMSE was particularly large for the p-curve and p-uniform methods when the null was true, a pattern that is consistent with their upward bias in that condition, as well as their use of only statistically significant results, of which there are fewer when the null is true. The 3PSM method also had relatively high RMSE when the null was true and k was 10.\n\nAdding heterogeneity increased the RMSE of RE meta-analysis only slightly. The increases were slightly larger for the trim-and-fill, WAAP-WLS, 3PSM, and PET-PEESE methods. Heterogeneity tended to slightly increase the RMSE of the p-curve and p-uniform methods, presumably by causing bias. However, when the null was true or k was small (i.e., k = 10), heterogeneity improved the RMSE of the p-curve and p-uniform methods, presumably by increasing the number of significant results that were drawn upon.\n\n*Coverage of 95% confidence intervals.* In the absence of heterogeneity, coverage rates for RE meta-analysis and the PET-PEESE and trim-and-fill methods were ideal at 95%.",
    "is_useful": true,
    "question": "How does heterogeneity affect the RMSE and coverage rates in meta-analysis methods?"
  },
  {
    "text": "For the PET-PEESE, p-uniform, and WAAP-WLS methods, coverage rates grew worse as k increased from 10 to 60.\n\n## *Strong publication bias, no QRPs*\n\nIn the face of strong publication bias, sets of metaanalyzed results often consisted of only significant results, especially at k = 10.\n\n*Type I error rate.* In the absence of heterogeneity, RE meta-analysis suffered from false-positive rates of 98% and higher. The trim-and-fill method had slightly lower, but still unacceptable, Type I error rates in excess of 70%. The WAAP-WLS method had poor Type I error rates at k = 10 (> 85%), but its Type I error rates decreased with increasing k (45% at k = 60), which was not the case for RE meta-analysis and the trim-and-fill method. The 3PSM method had lower Type I error rates (31%) at k = 10, but these error rates increased with increasing k (82% at k = 60). The p-curve, p-uniform, and PET-PEESE estimators had approximately conservative Type I error rates ranging from 3% to 10%.\n\nThe addition of heterogeneity slightly reduced the Type I error rate for RE meta-analysis, but error rates still approached 100% with increasing k. The trim-andfill method was generally not affected.",
    "is_useful": true,
    "question": "How do different methods of meta-analysis perform in terms of Type I error rates as sample size increases?"
  },
  {
    "text": "The 3PSM method remained unbiased.\n\n*RMSE.* In the absence of heterogeneity, when the null was true, RMSE was considerably elevated for RE metaanalysis. All six adjustment methods led to some improvement in RMSE, with the exception of the p-curve method at k = 10. The improvements were modest for the WAAP-WLS and trim-and-fill methods and successively greater for the p-uniform, p-curve, 3PSM, and PET-PEESE methods. These improvements with the latter four methods were particularly pronounced at k = 60.\n\nWhen there was a true effect, the RMSE of RE metaanalysis was acceptable (0.08 at k = 10, 0.05 at k = 60). Most of the other methods again provided some improvement. At k = 10, the p-uniform and p-curve methods did not improve the RMSE, and the PET-PEESE method increased the RMSE. The 3PSM, WAAP-WLS, and trim-and-fill estimators provided modest improvements in the RMSE. At k = 60, all six adjustment methods yielded improvements in the RMSE; the trim-and-fill, WAAP-WLS, and 3PSM methods were slightly more efficient than the p-uniform, p-curve, and PET-PEESE methods.\n\nAdding heterogeneity to a true effect of zero caused modest increases in RMSE for all the methods, and all the adjustment methods provided some benefit relative to RE meta-analysis.",
    "is_useful": true,
    "question": "What methods have been shown to provide improvements in RMSE for meta-analysis, particularly when there is a true effect?"
  },
  {
    "text": "At k = 60, all six adjustment methods yielded improvements in the RMSE; the trim-and-fill, WAAP-WLS, and 3PSM methods were slightly more efficient than the p-uniform, p-curve, and PET-PEESE methods.\n\nAdding heterogeneity to a true effect of zero caused modest increases in RMSE for all the methods, and all the adjustment methods provided some benefit relative to RE meta-analysis. The greatest improvement was provided by the 3PSM method, followed in order by the PET-PEESE, p-uniform, p-curve, and trim-and-fill methods. The WAAP-WLS estimator improved as k increased, performing worse than the p-curve and p-uniform methods at k = 10 but better than those methods at k = 60.\n\nAdding heterogeneity to a true nonzero effect caused moderate increases in RMSE for all the methods. Again, all the adjustment methods provided some benefit relative to RE meta-analysis, and these benefits were comparable across methods; the one exception was that the PET-PEESE estimator caused an increase in the RMSE at k = 10.\n\n*Coverage of 95% confidence intervals.* Because of the considerable publication bias, when the null hypothesis was true and there was no heterogeneity, 95% coverage was very poor (< 3%) without adjustment. All the adjustment methods led to some improvement in coverage. The benefits of the trim-and-fill method were slight (28% coverage at k = 10, 0% coverage at k = 60).",
    "is_useful": true,
    "question": "What impact do various adjustment methods have on RMSE and confidence interval coverage in meta-analysis?"
  },
  {
    "text": "At k = 60, the coverage of the p-uniform and WAAP-WLS methods fell noticeably (~68% coverage), whereas the coverage of the 3PSM method improved to 93%.\n\n## *The influence of QRPs*\n\n*Influence of QRPs on naive meta-analysis.* QRPs generally led to an increase in bias in RE meta-analysis, provided that the null was true and there was some publication bias. This increase was greatest when there was medium publication bias and the null was true; under these circumstances, high use of QRPs doubled mean error from 0.15 to 0.32. When there was strong publication bias and the null was true, the effect of QRPs was smaller; high use of QRPs increased mean error from about 0.32 to 0.44. The damage was presumably smaller because strong publication bias had already caused considerable mean error. When there was a true effect of 0.5, or when there was no publication bias, the influence of QRPs on the mean error and RMSE was minimal.\n\nQRPs also generally led to an increase in Type I error. With a sample size (k) of 10 and medium publication bias, QRPs approximately doubled the Type I error rate from 51% to 95%. However, at k = 30 or more, when there was at least medium publication bias, Type I error rates were generally at ceiling (90+%), so QRPs did little to further influence Type I error.",
    "is_useful": true,
    "question": "What is the impact of questionable research practices (QRPs) on the bias and error rates in meta-analysis?"
  },
  {
    "text": "When there was strong publication bias and the null was true, the effect of QRPs was smaller; high use of QRPs increased mean error from about 0.32 to 0.44. The damage was presumably smaller because strong publication bias had already caused considerable mean error. When there was a true effect of 0.5, or when there was no publication bias, the influence of QRPs on the mean error and RMSE was minimal.\n\nQRPs also generally led to an increase in Type I error. With a sample size (k) of 10 and medium publication bias, QRPs approximately doubled the Type I error rate from 51% to 95%. However, at k = 30 or more, when there was at least medium publication bias, Type I error rates were generally at ceiling (90+%), so QRPs did little to further influence Type I error. In the absence of publication bias, QRPs did lead to noticeable increases in Type I error given large-enough k (i.e., 60 or 100). For example, when there was no true effect and no heterogeneity, high use of QRPs increased Type I error to 19%, even though the mean error remained a mere 0.03. QRPs similarly tended to harm the coverage of 95% confidence intervals.",
    "is_useful": true,
    "question": "How do questionable research practices (QRPs) affect publication bias and statistical errors in scientific research?"
  },
  {
    "text": "QRPs also generally led to an increase in Type I error. With a sample size (k) of 10 and medium publication bias, QRPs approximately doubled the Type I error rate from 51% to 95%. However, at k = 30 or more, when there was at least medium publication bias, Type I error rates were generally at ceiling (90+%), so QRPs did little to further influence Type I error. In the absence of publication bias, QRPs did lead to noticeable increases in Type I error given large-enough k (i.e., 60 or 100). For example, when there was no true effect and no heterogeneity, high use of QRPs increased Type I error to 19%, even though the mean error remained a mere 0.03. QRPs similarly tended to harm the coverage of 95% confidence intervals.\n\nIn summary, for RE meta-analysis, QRPs exacerbated the effects of publication bias when there was no effect; however, the effects of QRPs on mean error were modest when (a) there was no publication bias, (b) publication bias was strong, or (c) there was a true effect. Thus, QRPs, as we implemented them, seem to play a small role in meta-analytic bias on their own. In the company of moderate publication bias, however, QRPs can considerably amplify problems.\n\n*Influence of QRPs on bias-corrected meta-analysis.* The effect of QRPs in our simulation varied as a function of both meta-analytic method and performance metric.",
    "is_useful": true,
    "question": "What impact do questionable research practices (QRPs) have on Type I error rates and confidence interval coverage in meta-analyses?"
  },
  {
    "text": "QRPs similarly tended to harm the coverage of 95% confidence intervals.\n\nIn summary, for RE meta-analysis, QRPs exacerbated the effects of publication bias when there was no effect; however, the effects of QRPs on mean error were modest when (a) there was no publication bias, (b) publication bias was strong, or (c) there was a true effect. Thus, QRPs, as we implemented them, seem to play a small role in meta-analytic bias on their own. In the company of moderate publication bias, however, QRPs can considerably amplify problems.\n\n*Influence of QRPs on bias-corrected meta-analysis.* The effect of QRPs in our simulation varied as a function of both meta-analytic method and performance metric. In this section, we focus chiefly on bias as the metric because the effects of QRPs on bias are the most straightforward and communicable. The effects of QRPs on RMSE, error rates, and coverage were generally a function of whether QRPs caused an increase or decrease in bias: When QRPs reduced the absolute value of the mean error, the RMSE and coverage probability generally improved; when QRPs increased the absolute value of the mean error, the RMSE, coverage rates, and Type I and II error rates were accordingly poorer. In some cases, a curvilinear effect was observed; as QRPs increased, an initial positive bias was reduced and then became negative; thus, these metrics first improved and then deteriorated.",
    "is_useful": true,
    "question": "How do questionable research practices (QRPs) influence bias in meta-analysis?"
  },
  {
    "text": "*Influence of QRPs on bias-corrected meta-analysis.* The effect of QRPs in our simulation varied as a function of both meta-analytic method and performance metric. In this section, we focus chiefly on bias as the metric because the effects of QRPs on bias are the most straightforward and communicable. The effects of QRPs on RMSE, error rates, and coverage were generally a function of whether QRPs caused an increase or decrease in bias: When QRPs reduced the absolute value of the mean error, the RMSE and coverage probability generally improved; when QRPs increased the absolute value of the mean error, the RMSE, coverage rates, and Type I and II error rates were accordingly poorer. In some cases, a curvilinear effect was observed; as QRPs increased, an initial positive bias was reduced and then became negative; thus, these metrics first improved and then deteriorated. The influence of QRPs was generally strongest when there was a null or small effect, presumably because studies with medium or large true effects required less p-hacking to be published.\n\nThe effects of the QRPs we modeled on performance of the trim-and-fill method were similar to their effects on performance of RE meta-analysis: Bias increased when the null was true and there was medium or strong publication bias. This bias also led to elevated Type I error rates (except when there was heterogeneity and no publication bias, in which case Type I error decreased slightly).",
    "is_useful": true,
    "question": "What is the impact of questionable research practices (QRPs) on bias in meta-analyses?"
  },
  {
    "text": "In some cases, a curvilinear effect was observed; as QRPs increased, an initial positive bias was reduced and then became negative; thus, these metrics first improved and then deteriorated. The influence of QRPs was generally strongest when there was a null or small effect, presumably because studies with medium or large true effects required less p-hacking to be published.\n\nThe effects of the QRPs we modeled on performance of the trim-and-fill method were similar to their effects on performance of RE meta-analysis: Bias increased when the null was true and there was medium or strong publication bias. This bias also led to elevated Type I error rates (except when there was heterogeneity and no publication bias, in which case Type I error decreased slightly). The effects of the QRPs on performance of the WAAP-WLS method were similar, but increases in bias were smaller than those with the trimand-fill method. A curious exception is that QRPs reduced Type I error when the WAAP-WLS method was used and there was medium publication bias and a large sample size (k = 60, k = 100), although even in these conditions the Type I error rates were still unacceptable (\u2265 40%). Perhaps, in these cases, the WAAP-WLS method switched from the better-powered WLS test to the poorer-powered WAAP test.\n\nIn contrast, QRPs caused downward bias in the p-curve and p-uniform methods. In the context of homogeneity, in which these methods are typically unbiased, QRPs led to underestimation of the effect size and an increase in the Type II error rate.",
    "is_useful": true,
    "question": "What impact do questionable research practices (QRPs) have on statistical analysis outcomes and error rates in the context of open science?"
  },
  {
    "text": "The effects of the QRPs on performance of the WAAP-WLS method were similar, but increases in bias were smaller than those with the trimand-fill method. A curious exception is that QRPs reduced Type I error when the WAAP-WLS method was used and there was medium publication bias and a large sample size (k = 60, k = 100), although even in these conditions the Type I error rates were still unacceptable (\u2265 40%). Perhaps, in these cases, the WAAP-WLS method switched from the better-powered WLS test to the poorer-powered WAAP test.\n\nIn contrast, QRPs caused downward bias in the p-curve and p-uniform methods. In the context of homogeneity, in which these methods are typically unbiased, QRPs led to underestimation of the effect size and an increase in the Type II error rate. In the context of heterogeneity, in which these methods tend to overestimate the effect size, QRPs led to less overestimation of the effect size, a decrease in Type I error rates, and an increase in Type II error rates. We consider this pattern to reflect two simple effects of opposite sign: Heterogeneity caused upward bias in the mean error, and QRPs caused downward bias, so the absolute value of the mean error was smaller when both were present than when only one or the other was present. QRPs also helped to reduce the upward bias in the average p-curve and p-uniform estimates when the null was true, perhaps by increasing the number of significant studies available.",
    "is_useful": true,
    "question": "What impact do questionable research practices have on the accuracy of statistical methods in research?"
  },
  {
    "text": "In contrast, QRPs caused downward bias in the p-curve and p-uniform methods. In the context of homogeneity, in which these methods are typically unbiased, QRPs led to underestimation of the effect size and an increase in the Type II error rate. In the context of heterogeneity, in which these methods tend to overestimate the effect size, QRPs led to less overestimation of the effect size, a decrease in Type I error rates, and an increase in Type II error rates. We consider this pattern to reflect two simple effects of opposite sign: Heterogeneity caused upward bias in the mean error, and QRPs caused downward bias, so the absolute value of the mean error was smaller when both were present than when only one or the other was present. QRPs also helped to reduce the upward bias in the average p-curve and p-uniform estimates when the null was true, perhaps by increasing the number of significant studies available.\n\nThe QRPs nudged PET-PEESE estimates downward. When these estimates were biased upward in our simulation, as in the case of small or null effects in the context of publication bias and heterogeneity, QRPs reduced bias and improved Type I error rates slightly. When PET-PEESE estimates were unbiased or biased downward, as in the case of nonzero true effects, QRPs led to greater downward bias. This downward bias was sometimes quite strong when the null was true.",
    "is_useful": true,
    "question": "How do questionable research practices (QRPs) impact effect size estimation and error rates in research methodologies?"
  },
  {
    "text": "We consider this pattern to reflect two simple effects of opposite sign: Heterogeneity caused upward bias in the mean error, and QRPs caused downward bias, so the absolute value of the mean error was smaller when both were present than when only one or the other was present. QRPs also helped to reduce the upward bias in the average p-curve and p-uniform estimates when the null was true, perhaps by increasing the number of significant studies available.\n\nThe QRPs nudged PET-PEESE estimates downward. When these estimates were biased upward in our simulation, as in the case of small or null effects in the context of publication bias and heterogeneity, QRPs reduced bias and improved Type I error rates slightly. When PET-PEESE estimates were unbiased or biased downward, as in the case of nonzero true effects, QRPs led to greater downward bias. This downward bias was sometimes quite strong when the null was true. The PET-PEESE method yielded statistically significant effects of opposite sign in many analyses; the Type I error rates tended to grow with increasing use of QRPs, increasing publication bias, and larger sample sizes, with rates ranging from 9% (medium use of QRPs, strong publication bias, k = 10) to 62% (high use of QRPs, medium publication bias, k = 100).",
    "is_useful": true,
    "question": "How can questionable research practices (QRPs) affect bias in scientific estimates and Type I error rates?"
  },
  {
    "text": "The QRPs nudged PET-PEESE estimates downward. When these estimates were biased upward in our simulation, as in the case of small or null effects in the context of publication bias and heterogeneity, QRPs reduced bias and improved Type I error rates slightly. When PET-PEESE estimates were unbiased or biased downward, as in the case of nonzero true effects, QRPs led to greater downward bias. This downward bias was sometimes quite strong when the null was true. The PET-PEESE method yielded statistically significant effects of opposite sign in many analyses; the Type I error rates tended to grow with increasing use of QRPs, increasing publication bias, and larger sample sizes, with rates ranging from 9% (medium use of QRPs, strong publication bias, k = 10) to 62% (high use of QRPs, medium publication bias, k = 100). Researchers have, at times, considered a significant and negative PET-PEESE estimate as evidence that the estimate is incorrect, choosing instead to interpret results from less extreme adjustments, such as the trim-and-fill method (see, e.g., Bediou et al., 2018). A statistically significant PET-PEESE estimate in the unexpected direction probably is incorrect, but researchers should be aware that when they obtain such an estimate, there is likely to be some combination of QRPs and publication bias and, perhaps, a null effect.\n\nThe QRPs we simulated generally led to a slight downward bias in 3PSM estimates. This bias was stronger when heterogeneity was present.",
    "is_useful": true,
    "question": "How do questionable research practices (QRPs) affect the accuracy of statistical estimates in scientific research?"
  },
  {
    "text": "Researchers have, at times, considered a significant and negative PET-PEESE estimate as evidence that the estimate is incorrect, choosing instead to interpret results from less extreme adjustments, such as the trim-and-fill method (see, e.g., Bediou et al., 2018). A statistically significant PET-PEESE estimate in the unexpected direction probably is incorrect, but researchers should be aware that when they obtain such an estimate, there is likely to be some combination of QRPs and publication bias and, perhaps, a null effect.\n\nThe QRPs we simulated generally led to a slight downward bias in 3PSM estimates. This bias was stronger when heterogeneity was present. At worst, given high heterogeneity (\u03c4 = 0.4), the mean error caused by QRPs was as large as \u20130.32. QRPs therefore tended to reduce Type I error and increase Type II error for this method.\n\nWhat was the worst that happened with each estimator as a consequence of the QRPs we implemented? In the case of RE meta-analysis and the trim-and-fill and WAAP-WLS methods, QRPs exacerbated the effects of publication bias. In the medium-publication-bias condition, QRPs increased the bias in these estimators; mean error increased from 0.14 to 0.31 for RE meta-analysis, from 0.08 to 0.22 for the trim-and-fill method, and from 0.11 to 0.15 for the WAAP-WLS method.",
    "is_useful": true,
    "question": "How do questionable research practices (QRPs) impact the bias and error rates in statistical methods used in research?"
  },
  {
    "text": "These changes in bias caused corresponding increases in Type I error rates, increasing them by as much as 40 percentage points. When publication bias was strong, QRPs increased bias by as much as 0.12, but this increase was less dramatic than that caused by QRPs when there was medium publication bias. The PET-PEESE, p-curve, and p-uniform methods each demonstrated downward bias of up to \u20130.14 when QRPs were simulated. This bias caused a loss of power of up to 17 percentage points for the first two methods and 37 percentage points for the p-uniform method. High application of QRPs also caused the PET-PEESE method to frequently mistake null effects for significant negative effects (up to 62% of the cases with no heterogeneity, high use of QRPs, and medium publication bias). The 3PSM method underestimated the true effect of 0.5 by as much as 0.19, and power was reduced by up to 32 percentage points.\n\nThe reactions of the estimators to QRPs may be broadly considered to fall into two clusters. In the case of RE meta-analysis and the trim-and-fill and WAAP-WLS methods, QRPs caused overestimation, particularly of null effects. In the case of the PET-PEESE, p-curve, p-uniform, and 3PSM methods, QRPs caused underestimation of true effects and noticeable loss of power.\n\nIt is important to bear in mind that the results we have described are specific to the way we simulated QRPs.",
    "is_useful": true,
    "question": "What impact do questionable research practices (QRPs) have on the accuracy and power of statistical methods in research?"
  },
  {
    "text": "The 3PSM method underestimated the true effect of 0.5 by as much as 0.19, and power was reduced by up to 32 percentage points.\n\nThe reactions of the estimators to QRPs may be broadly considered to fall into two clusters. In the case of RE meta-analysis and the trim-and-fill and WAAP-WLS methods, QRPs caused overestimation, particularly of null effects. In the case of the PET-PEESE, p-curve, p-uniform, and 3PSM methods, QRPs caused underestimation of true effects and noticeable loss of power.\n\nIt is important to bear in mind that the results we have described are specific to the way we simulated QRPs. Our approach could likely be changed in a variety of ways that would provide very different results. We see this topic as an important area for future research.\n\n## Discussion\n\nWe inspected and compared the efficacy of meta-analytic adjustments for bias across hundreds of thousands of simulated literatures representing a range of true effect sizes, degrees of heterogeneity, degrees of publication bias, and degrees of QRP usage. We assessed the results according to both the ability to reject a null effect or detect a true effect and the ability to estimate the mean of the distribution of true underlying effects. We begin our discussion of the results with a coarse summary of the three overall patterns we observed, as well as some general recommendations.\n\nFirst, RE meta-analysis and the trim-and-fill and WAAP-WLS methods showed alarmingly high false-positive rates (Fig. 3) and overestimation (Fig.",
    "is_useful": true,
    "question": "How do questionable research practices (QRPs) impact the efficacy of different meta-analytic methods in estimating effect sizes?"
  },
  {
    "text": "Our approach could likely be changed in a variety of ways that would provide very different results. We see this topic as an important area for future research.\n\n## Discussion\n\nWe inspected and compared the efficacy of meta-analytic adjustments for bias across hundreds of thousands of simulated literatures representing a range of true effect sizes, degrees of heterogeneity, degrees of publication bias, and degrees of QRP usage. We assessed the results according to both the ability to reject a null effect or detect a true effect and the ability to estimate the mean of the distribution of true underlying effects. We begin our discussion of the results with a coarse summary of the three overall patterns we observed, as well as some general recommendations.\n\nFirst, RE meta-analysis and the trim-and-fill and WAAP-WLS methods showed alarmingly high false-positive rates (Fig. 3) and overestimation (Fig. 4) in the face of publication bias in combination with a zero or small true effect size. Generally, the WAAP-WLS method outperformed both RE meta-analysis and the trim-and-fill method.\n\nSecond, the p-curve and p-uniform methods had reasonable false-positive rates and little bias under homogeneity. With increasing heterogeneity, however, both showed increasing false-positive rates (Fig. 3) and overestimation (Fig. 4), particularly for a zero or small true effect size. This poor performance was actually mitigated by increasing use of QRPs, and was primarily independent of changes in publication bias and sample size.",
    "is_useful": true,
    "question": "What are some important considerations when assessing the efficacy of different meta-analytic methods in the presence of publication bias and true effect sizes?"
  },
  {
    "text": "We begin our discussion of the results with a coarse summary of the three overall patterns we observed, as well as some general recommendations.\n\nFirst, RE meta-analysis and the trim-and-fill and WAAP-WLS methods showed alarmingly high false-positive rates (Fig. 3) and overestimation (Fig. 4) in the face of publication bias in combination with a zero or small true effect size. Generally, the WAAP-WLS method outperformed both RE meta-analysis and the trim-and-fill method.\n\nSecond, the p-curve and p-uniform methods had reasonable false-positive rates and little bias under homogeneity. With increasing heterogeneity, however, both showed increasing false-positive rates (Fig. 3) and overestimation (Fig. 4), particularly for a zero or small true effect size. This poor performance was actually mitigated by increasing use of QRPs, and was primarily independent of changes in publication bias and sample size. Again, we note that the original developers of the p-curve method have argued that its performance should not be evaluated using the average true underlying effect, as is the usual approach in the meta-analytic literature, but rather should be evaluated using the average of the effects submitted to the analysis (Simonsohn, Nelson, and Simmons, 2014). The performance of the p-curve method evaluated in this way is described in the supplemental material, but in general, it should be noted that this method performed well when estimating the average of the true underlying effects of the submitted studies.",
    "is_useful": true,
    "question": "What are the patterns observed in the performance of various statistical methods used in meta-analysis pertaining to publication bias and effect size?"
  },
  {
    "text": "With increasing heterogeneity, however, both showed increasing false-positive rates (Fig. 3) and overestimation (Fig. 4), particularly for a zero or small true effect size. This poor performance was actually mitigated by increasing use of QRPs, and was primarily independent of changes in publication bias and sample size. Again, we note that the original developers of the p-curve method have argued that its performance should not be evaluated using the average true underlying effect, as is the usual approach in the meta-analytic literature, but rather should be evaluated using the average of the effects submitted to the analysis (Simonsohn, Nelson, and Simmons, 2014). The performance of the p-curve method evaluated in this way is described in the supplemental material, but in general, it should be noted that this method performed well when estimating the average of the true underlying effects of the submitted studies. If no QRPs were present, this estimator recovered that quantity with very low mean error, regardless of the level of publication bias and the value of \u03b4. QRPs as we modeled them induced a downward bias in p-curve estimates, particularly when \u03b4 was small and there was little or no heterogeneity. These results are consistent with previous simulation results (Simonsohn, Nelson, and Simmons, 2014).\n\nThird, the PET-PEESE and 3PSM methods both showed mostly reasonable false-positive rates, but they suffered from reduced power when sample sizes were small, heterogeneity was high, there was little publication bias, or there was heavy use of QRPs (Fig. 3).",
    "is_useful": true,
    "question": "How do questionable research practices (QRPs) affect the performance of statistical methods in estimating true effect sizes?"
  },
  {
    "text": "The performance of the p-curve method evaluated in this way is described in the supplemental material, but in general, it should be noted that this method performed well when estimating the average of the true underlying effects of the submitted studies. If no QRPs were present, this estimator recovered that quantity with very low mean error, regardless of the level of publication bias and the value of \u03b4. QRPs as we modeled them induced a downward bias in p-curve estimates, particularly when \u03b4 was small and there was little or no heterogeneity. These results are consistent with previous simulation results (Simonsohn, Nelson, and Simmons, 2014).\n\nThird, the PET-PEESE and 3PSM methods both showed mostly reasonable false-positive rates, but they suffered from reduced power when sample sizes were small, heterogeneity was high, there was little publication bias, or there was heavy use of QRPs (Fig. 3). These two methods also showed similar patterns of estimation error (Fig. 4): Both methods tended to underestimate effects when sample sizes were small, heterogeneity was high, or there was heavy use of QRPs. Although the two methods produced similar overall patterns of results, the 3PSM estimator almost always outperformed the PET-PEESE estimator.\n\nFurthermore, it is worth noting that RE meta-analysis and the WAAP-WLS and PET-PEESE methods always returned at least some estimate, whereas the other methods sometimes failed to converge or could not be applied because of lack of significant studies.",
    "is_useful": true,
    "question": "What methods are discussed in relation to estimating the true underlying effects and their performance in the context of publication bias and questionable research practices?"
  },
  {
    "text": "Third, the PET-PEESE and 3PSM methods both showed mostly reasonable false-positive rates, but they suffered from reduced power when sample sizes were small, heterogeneity was high, there was little publication bias, or there was heavy use of QRPs (Fig. 3). These two methods also showed similar patterns of estimation error (Fig. 4): Both methods tended to underestimate effects when sample sizes were small, heterogeneity was high, or there was heavy use of QRPs. Although the two methods produced similar overall patterns of results, the 3PSM estimator almost always outperformed the PET-PEESE estimator.\n\nFurthermore, it is worth noting that RE meta-analysis and the WAAP-WLS and PET-PEESE methods always returned at least some estimate, whereas the other methods sometimes failed to converge or could not be applied because of lack of significant studies. Information on the percentages of valid estimates is available in Table 2 in the supplemental material and should be considered alongside the performance information we have reported here. For example, it may be that consistent failures to converge in certain simulated conditions indicate that the method will be less applicable to real-world data than other methods are.\n\nOn the basis of our results, we believe we can confidently make five general recommendations:\n\n- 1. If publication bias is highly unlikely (e.g., data are from a multilaboratory preregistered replication), rely on RE meta-analysis rather than any of the other methods we examined.\n- 2. When there may be publication bias, do not rely on RE meta-analysis alone.",
    "is_useful": true,
    "question": "What considerations should be taken into account when choosing between different estimation methods in the context of publication bias and sample size in research?"
  },
  {
    "text": "Furthermore, it is worth noting that RE meta-analysis and the WAAP-WLS and PET-PEESE methods always returned at least some estimate, whereas the other methods sometimes failed to converge or could not be applied because of lack of significant studies. Information on the percentages of valid estimates is available in Table 2 in the supplemental material and should be considered alongside the performance information we have reported here. For example, it may be that consistent failures to converge in certain simulated conditions indicate that the method will be less applicable to real-world data than other methods are.\n\nOn the basis of our results, we believe we can confidently make five general recommendations:\n\n- 1. If publication bias is highly unlikely (e.g., data are from a multilaboratory preregistered replication), rely on RE meta-analysis rather than any of the other methods we examined.\n- 2. When there may be publication bias, do not rely on RE meta-analysis alone. Publication bias can quickly accumulate in even small sets of published studies, leading to overestimated effects and high Type I error rates.\n- 3. Recognize that the popular trim-and-fill adjustment, although efficient, reduces bias and Type I error rates only slightly. To achieve stronger reductions in bias, consider the PET-PEESE, p-curve, p-uniform, and 3PSM adjustments. However, keep in mind that these adjustments are often inefficient and a given individual estimate may be poor, even if these adjustments are unbiased in the long run.\n- 4.",
    "is_useful": true,
    "question": "What recommendations can be made regarding the reliability of different meta-analysis methods in the presence of publication bias?"
  },
  {
    "text": "If publication bias is highly unlikely (e.g., data are from a multilaboratory preregistered replication), rely on RE meta-analysis rather than any of the other methods we examined.\n- 2. When there may be publication bias, do not rely on RE meta-analysis alone. Publication bias can quickly accumulate in even small sets of published studies, leading to overestimated effects and high Type I error rates.\n- 3. Recognize that the popular trim-and-fill adjustment, although efficient, reduces bias and Type I error rates only slightly. To achieve stronger reductions in bias, consider the PET-PEESE, p-curve, p-uniform, and 3PSM adjustments. However, keep in mind that these adjustments are often inefficient and a given individual estimate may be poor, even if these adjustments are unbiased in the long run.\n- 4. Do not use the p-curve or p-uniform methods for estimating \u03b4 if heterogeneity is expected or if many studies yielded nonsignificant results.\n\n- 5. Given that adjustments for publication bias are only partially successful, we offer a final recommendation that must be implemented not by meta-analysts, but by primary researchers and journal editors: Take steps to ensure the completeness and transparency of the original literature. An ounce of registered report is worth a pound of bias correction.\n## *Limits on generalizability*\n\nSimulation studies necessarily require making assumptions that might limit the generalizability of their results to real data.",
    "is_useful": true,
    "question": "What are some recommendations for addressing publication bias in research? "
  },
  {
    "text": "To achieve stronger reductions in bias, consider the PET-PEESE, p-curve, p-uniform, and 3PSM adjustments. However, keep in mind that these adjustments are often inefficient and a given individual estimate may be poor, even if these adjustments are unbiased in the long run.\n- 4. Do not use the p-curve or p-uniform methods for estimating \u03b4 if heterogeneity is expected or if many studies yielded nonsignificant results.\n\n- 5. Given that adjustments for publication bias are only partially successful, we offer a final recommendation that must be implemented not by meta-analysts, but by primary researchers and journal editors: Take steps to ensure the completeness and transparency of the original literature. An ounce of registered report is worth a pound of bias correction.\n## *Limits on generalizability*\n\nSimulation studies necessarily require making assumptions that might limit the generalizability of their results to real data. Although we simulated a data-generation process that might plausibly underlie real-world research in psychology, several limits to our study should be kept in mind when considering our findings. First, we simulated the data-generation process in the absence of QRPs as a two-group design, despite the fact that real research designs are rarely this simple. However, the vast majority of meta-analyses use effect-size measures, such as correlations and standardized mean differences, that ignore such design complexities (see, e.g., Table S1 in Fanelli et al., 2017).",
    "is_useful": true,
    "question": "What steps can be taken to enhance the completeness and transparency of the original literature in order to reduce publication bias?"
  },
  {
    "text": "An ounce of registered report is worth a pound of bias correction.\n## *Limits on generalizability*\n\nSimulation studies necessarily require making assumptions that might limit the generalizability of their results to real data. Although we simulated a data-generation process that might plausibly underlie real-world research in psychology, several limits to our study should be kept in mind when considering our findings. First, we simulated the data-generation process in the absence of QRPs as a two-group design, despite the fact that real research designs are rarely this simple. However, the vast majority of meta-analyses use effect-size measures, such as correlations and standardized mean differences, that ignore such design complexities (see, e.g., Table S1 in Fanelli et al., 2017). For example, to meta-analyze data from an experiment with a 2 \u00d7 2 factorial design, one would typically calculate a standardized mean difference by either discarding or collapsing across the factor that is not of primary interest. So for a 2 \u00d7 2 design with a per-group sample size of 20, the comparison entered into the meta-analysis would have either a total N of 40 (i.e., 2 \u00d7 20; when the second factor is discarded) or a total N of 80 (i.e., 2 \u00d7 2 \u00d7 20; when data are collapsed across the second factor). Therefore, although most designs are more complex than the twogroup case we simulated, data in meta-analyses are often reduced to this simple form.",
    "is_useful": true,
    "question": "What are some challenges in generalizing the results of simulation studies to real-world research designs in psychology?"
  },
  {
    "text": "For example, to meta-analyze data from an experiment with a 2 \u00d7 2 factorial design, one would typically calculate a standardized mean difference by either discarding or collapsing across the factor that is not of primary interest. So for a 2 \u00d7 2 design with a per-group sample size of 20, the comparison entered into the meta-analysis would have either a total N of 40 (i.e., 2 \u00d7 20; when the second factor is discarded) or a total N of 80 (i.e., 2 \u00d7 2 \u00d7 20; when data are collapsed across the second factor). Therefore, although most designs are more complex than the twogroup case we simulated, data in meta-analyses are often reduced to this simple form. As a result, our findings generalize to meta-analyses in which more complex designs are handled by discarding nonfocal factors. In the case of collapsing across these factors, our simulation likely underestimates sample size on average. However, it should be noted that the choice to collapse across factors is problematic given the required assumption that the nonfocal factors do not interact with the comparison of interest (i.e., there is a true interaction effect of 0). Thus, our findings generalize to the least problematic case.\n\nA second, related issue arises for real-world data generated by single-sample designs (e.g., correlational studies). If such studies tend to have larger or smaller sample sizes than those with factorial designs, our simulation might under- or overestimate sample size, respectively.",
    "is_useful": true,
    "question": "What considerations should be taken into account when conducting meta-analyses involving factorial design data?"
  },
  {
    "text": "A third point to consider is whether our implementation of publication bias mirrors bias in real-world data. We implemented publication bias using specific functions with specific parameter values. Of course, it would be entirely possible to use different functions or different parameter values. What is unclear, however, is the degree to which different choices at this level would produce different results. For example, we intentionally modeled publication bias in a way that differed from the bias that the 3PSM, p-curve, and p-uniform methods are designed for, so it may be that these methods would show improved performance under different specifications of publication bias. Ultimately, this is an empirical question and should be the focus of future research. Additionally, our implementation of QRPs was extremely specific and might limit the generalizability of our results. Because the kinds of QRPs that can be applied depend entirely on the design of the specific study, there are an infinite number of possible ways to simulate QRPs (Hartgerink, van Aert, Nuijten, Wicherts, & Van Assen, 2016). Thus, our results for QRPs likely will not generalize to designs that are dramatically different from those we simulated.\n\nFourth, it is impossible to perfectly mirror how real data are generated. However, it is our hope that researchers can use our framework to close this gap and tackle some of the issues we have mentioned. It would be relatively easy, for example, to modify our code to use larger or smaller sample sizes and then assess whether this substantially changes how the methods perform.",
    "is_useful": true,
    "question": "What considerations should researchers take into account when modeling publication bias and questionable research practices in scientific studies?"
  },
  {
    "text": "Ultimately, this is an empirical question and should be the focus of future research. Additionally, our implementation of QRPs was extremely specific and might limit the generalizability of our results. Because the kinds of QRPs that can be applied depend entirely on the design of the specific study, there are an infinite number of possible ways to simulate QRPs (Hartgerink, van Aert, Nuijten, Wicherts, & Van Assen, 2016). Thus, our results for QRPs likely will not generalize to designs that are dramatically different from those we simulated.\n\nFourth, it is impossible to perfectly mirror how real data are generated. However, it is our hope that researchers can use our framework to close this gap and tackle some of the issues we have mentioned. It would be relatively easy, for example, to modify our code to use larger or smaller sample sizes and then assess whether this substantially changes how the methods perform.\n\nFinally, bias correction in meta-analyses is an active field of research, and multiple new methods were published after our simulations were completed. These include an extension of the p-uniform method, called p-uniform*, that estimates heterogeneity and includes nonsignificant results (R. C. M. van Aert & van Assen, 2018) and a Bayesian fill-in method called BALM (Du, Liu, & Wang, 2017).",
    "is_useful": true,
    "question": "What challenges are associated with generalizing results from simulations of questionable research practices (QRPs) to real-world studies in open science?"
  },
  {
    "text": "Thus, our results for QRPs likely will not generalize to designs that are dramatically different from those we simulated.\n\nFourth, it is impossible to perfectly mirror how real data are generated. However, it is our hope that researchers can use our framework to close this gap and tackle some of the issues we have mentioned. It would be relatively easy, for example, to modify our code to use larger or smaller sample sizes and then assess whether this substantially changes how the methods perform.\n\nFinally, bias correction in meta-analyses is an active field of research, and multiple new methods were published after our simulations were completed. These include an extension of the p-uniform method, called p-uniform*, that estimates heterogeneity and includes nonsignificant results (R. C. M. van Aert & van Assen, 2018) and a Bayesian fill-in method called BALM (Du, Liu, & Wang, 2017).\n\n# *Method performance checks and sensitivity analysis for meta-analysis in psychology*\n\nSeveral authors have suggested that sensitivity analysis can be a valuable tool for evaluating the robustness of\n\nconclusions from a meta-analysis (e.g., APA Publications and Communications Board Working Group on Journal Article Reporting Standards, 2008; McShane et al., 2016; van Aert, 2017; van Aert et al., 2016). If results do not substantially change across a range of different methods and assumptions, the conclusions can be considered to be robust. However, the set of methods employed in a sensitivity analysis should include only those that can be expected to perform reasonably well.",
    "is_useful": true,
    "question": "What is the significance of sensitivity analysis in the context of meta-analysis?"
  },
  {
    "text": "# *Method performance checks and sensitivity analysis for meta-analysis in psychology*\n\nSeveral authors have suggested that sensitivity analysis can be a valuable tool for evaluating the robustness of\n\nconclusions from a meta-analysis (e.g., APA Publications and Communications Board Working Group on Journal Article Reporting Standards, 2008; McShane et al., 2016; van Aert, 2017; van Aert et al., 2016). If results do not substantially change across a range of different methods and assumptions, the conclusions can be considered to be robust. However, the set of methods employed in a sensitivity analysis should include only those that can be expected to perform reasonably well. Put differently, if a method is known to perform poorly under the conditions that apply to a metaanalysis at hand, it should not be included in a sensitivity analysis, or it should at least be treated with skepticism and given less weight than other methods when the results are evaluated.\n\nTo encourage and facilitate sensitivity analysis in meta-analysis, we suggest a two-step procedure: The first step is to evaluate which bias-correcting methods can be expected a priori to perform reasonably well in research conditions that are plausible for the metaanalysis at hand (*method performance check*). The second step is to compute meta-analytic estimates using all the included methods and compare them in order to evaluate the variability (or robustness) of conclusions (*sensitivity analysis*). This evaluation should respect the results from the method performance check and weigh the methods accordingly.",
    "is_useful": true,
    "question": "What is the importance of sensitivity analysis in evaluating the robustness of conclusions from a meta-analysis?"
  },
  {
    "text": "However, the set of methods employed in a sensitivity analysis should include only those that can be expected to perform reasonably well. Put differently, if a method is known to perform poorly under the conditions that apply to a metaanalysis at hand, it should not be included in a sensitivity analysis, or it should at least be treated with skepticism and given less weight than other methods when the results are evaluated.\n\nTo encourage and facilitate sensitivity analysis in meta-analysis, we suggest a two-step procedure: The first step is to evaluate which bias-correcting methods can be expected a priori to perform reasonably well in research conditions that are plausible for the metaanalysis at hand (*method performance check*). The second step is to compute meta-analytic estimates using all the included methods and compare them in order to evaluate the variability (or robustness) of conclusions (*sensitivity analysis*). This evaluation should respect the results from the method performance check and weigh the methods accordingly.\n\nFor a sensible sensitivity analysis, we recommend that meta-analysts and consumers of meta-analyses focus on the following question: \"Do my conclusions depend on a meta-analytic method that performs poorly in plausible conditions?\" If the answer is \"yes,\" then those conclusions should clearly be revisited. To help analysts and consumers answer this question, we have provided an interactive app (http://www.shinyapps .org/apps/metaExplorer/) that, for a given method and a given definition of \"performs poorly,\" identifies all of the conditions in our simulation for which the answer to the question is \"yes.\"",
    "is_useful": true,
    "question": "What key considerations should be taken into account when conducting a sensitivity analysis in meta-analysis?"
  },
  {
    "text": "The second step is to compute meta-analytic estimates using all the included methods and compare them in order to evaluate the variability (or robustness) of conclusions (*sensitivity analysis*). This evaluation should respect the results from the method performance check and weigh the methods accordingly.\n\nFor a sensible sensitivity analysis, we recommend that meta-analysts and consumers of meta-analyses focus on the following question: \"Do my conclusions depend on a meta-analytic method that performs poorly in plausible conditions?\" If the answer is \"yes,\" then those conclusions should clearly be revisited. To help analysts and consumers answer this question, we have provided an interactive app (http://www.shinyapps .org/apps/metaExplorer/) that, for a given method and a given definition of \"performs poorly,\" identifies all of the conditions in our simulation for which the answer to the question is \"yes.\" In the following, we provide an illustration of how one might perform this form of method performance check and discuss how this step guides the subsequent sensitivity analysis.\n\n#### *A real-world example: two data sets on ego depletion.*\n\nWe use data from studies on the topic of ego depletion for this example because it is relatively easy to understand and there are data from both meta-analyses of the literature and a large-scale preregistered replication.\n\nBriefly, the limited-strength model of self-control holds that any act of self-control will result in subsequent acts of self-control being less likely to succeed\u2014a state referred to as ego depletion (Muraven, Tice, & Baumeister, 1998).",
    "is_useful": true,
    "question": "What is the importance of conducting a sensitivity analysis in meta-analyses regarding the reliability of conclusions?"
  },
  {
    "text": "In the following, we provide an illustration of how one might perform this form of method performance check and discuss how this step guides the subsequent sensitivity analysis.\n\n#### *A real-world example: two data sets on ego depletion.*\n\nWe use data from studies on the topic of ego depletion for this example because it is relatively easy to understand and there are data from both meta-analyses of the literature and a large-scale preregistered replication.\n\nBriefly, the limited-strength model of self-control holds that any act of self-control will result in subsequent acts of self-control being less likely to succeed\u2014a state referred to as ego depletion (Muraven, Tice, & Baumeister, 1998). Typically, experiments aimed at testing this hypothesis involve participants completing a sequence of two tasks\u2014a manipulation task and an outcome task. Prior to the outcome task, participants in the depletion condition are given a version of the manipulation task that is designed to require more selfcontrol than the version given to participants in the control condition. Support for ego depletion is claimed when participants in the depletion condition perform worse on the subsequent outcome task than participants in the control condition do. Following convention, we represent this effect as a standardized mean difference (d); higher values indicate a greater depletion effect. In the following example, we analyze both a preexisting meta-analytic sample of 116 ego-depletion studies (Carter, Kofler, Forster, & McCullough, 2015) and a data set of 23 studies from a Registered Replication Report (Hagger et al., 2016).",
    "is_useful": true,
    "question": "What is the concept illustrated by ego depletion in self-control studies?"
  },
  {
    "text": "We apply each of our methods to these two data sets (see Tables 2 and 3).\n\nImagine that researchers agree to the logic that a depletion effect (\u03b4) less than or equal to 0.15 should be considered practically equivalent to zero (Carter & McCullough, 2018). Unfortunately, in this case, different meta-analytic methods would lead to different conclusions (see Table 2). On the basis of the results from the RE model and the WAAP-WLS, trim-and-fill, p-curve, p-uniform, and 3PSM methods, a group of researchers could conclude that the depletion effect is practically significant (i.e., \u03b4 > 0.15). In contrast, on the basis of the results from the PET, PEESE, and PET-PEESE methods, a separate group of researchers could conclude that the depletion effect is practically nonsignificant (i.e., \u03b4 \u2264 0.15). Hence, a naive sensitivity analysis would be inconclusive, as the variability in results is so large that either conclusion can be drawn. To help overcome this inconclusiveness, we recommend that researchers ask, \"Do my conclusions depend on a meta-analytic method that performs poorly in plausible conditions?\" Such a method performance check gives guidance as to which results should be given more weight and credibility.\n\n*What are \"plausible conditions\"?* For a method performance check, one must define \"plausible conditions.\" As in an a priori power analysis, considerations should relate to the specific research environment at hand.",
    "is_useful": true,
    "question": "What considerations should researchers evaluate to determine if their conclusions are affected by the choice of meta-analytic methods?"
  },
  {
    "text": "In contrast, on the basis of the results from the PET, PEESE, and PET-PEESE methods, a separate group of researchers could conclude that the depletion effect is practically nonsignificant (i.e., \u03b4 \u2264 0.15). Hence, a naive sensitivity analysis would be inconclusive, as the variability in results is so large that either conclusion can be drawn. To help overcome this inconclusiveness, we recommend that researchers ask, \"Do my conclusions depend on a meta-analytic method that performs poorly in plausible conditions?\" Such a method performance check gives guidance as to which results should be given more weight and credibility.\n\n*What are \"plausible conditions\"?* For a method performance check, one must define \"plausible conditions.\" As in an a priori power analysis, considerations should relate to the specific research environment at hand. Only if no specific prior knowledge is available can general knowledge about the field be used as an approximation. For example, some degree of bias seems possible: In the fields of psychology and psychiatry, more than 90% of all published hypothesis tests are significant (Fanelli, 2011), despite estimates that average power is around 35% (Bakker, van Dijk, & Wicherts, 2012), and whereas reported effects tend to be statistically significant, unreported effects tend not to be (Franco, Malhotra, & Simonovits, 2016). Moreover, there is both direct (Franco et al. 2016; LeBel et al.",
    "is_useful": true,
    "question": "What is a recommended approach for researchers to address inconclusive results in meta-analytic methods?"
  },
  {
    "text": "*What are \"plausible conditions\"?* For a method performance check, one must define \"plausible conditions.\" As in an a priori power analysis, considerations should relate to the specific research environment at hand. Only if no specific prior knowledge is available can general knowledge about the field be used as an approximation. For example, some degree of bias seems possible: In the fields of psychology and psychiatry, more than 90% of all published hypothesis tests are significant (Fanelli, 2011), despite estimates that average power is around 35% (Bakker, van Dijk, & Wicherts, 2012), and whereas reported effects tend to be statistically significant, unreported effects tend not to be (Franco, Malhotra, & Simonovits, 2016). Moreover, there is both direct (Franco et al. 2016; LeBel et al. 2017) and self-report (John et al. 2012) evidence of\n\n| Method | Estimate (with 95% confidence interval) |  | Poor performancea | \u03b4 = 0.50 |\n| --- | --- | --- | --- | --- |\n|  |  | \u03b4 = 0 | \u03b4 = 0.20 |  |\n| Random-effects | 0.43 [0.34, 0.52] | Yes | No | No |\n| Trim-and-fill | 0.24 [0.13, 0.34] | Yes | No | No |\n| WAAP-WLS | 0.35 [0.26, 0.",
    "is_useful": true,
    "question": "What factors should be considered when defining plausible conditions for method performance checks in research?"
  },
  {
    "text": "a These columns report whether or not each model performed poorly in the plausible conditions of the simulation study when the true population effect (\u03b4) had the indicated value.\n\nthe use of QRPs, and several studies have found evidence of small-study effects consistent with the presence of publication bias (Bakker et al., 2012; Fanelli et al., 2017; K\u00fchberger, Fritz, & Scherndl, 2014).8\n\nIn addition to bias, a degree of heterogeneity seems very likely when diverse experimental paradigms are summarized in a meta-analysis (e.g., as opposed to a multilab registered report; T. D. Stanley, Carter, & Doucouliagos, 2018; Tackett, McShane, Bockenholt, & Gelman, 2017; van Erp et al., 2017). Finally, it seems that the typical true effect in psychology research is rather small: The median published effect size (d) is around 0.3 to 0.4 (Bosco, Aguinis, Singh, Field, & Pierce, 2015; Richard, Bond, & Stokes-Zoota, 2003), and as this estimate is not corrected for publication bias, the typical true effect is likely smaller. This general observation, of course, does not preclude the possibility that some effects in psychology are indeed large.",
    "is_useful": true,
    "question": "What factors contribute to challenges in accurately estimating true effect sizes in psychology research?"
  },
  {
    "text": "We evaluate the performance of all the estimators we examined under these plausible conditions at k = 100, the simulated value closest to the observed k of 116.\n\n#### *What is \"poor performance\" of a meta-analytic*\n\n*method?* Given this definition of plausible conditions, the next step for a method performance check is to identify defensible choices for the definition of \"poor performance.\" For simplicity, in this example we consider only one metric, mean error,9 and ask whether each method is likely to be biased enough that a true null effect is mistaken for a practically significant effect or, conversely, that a true effect is mistaken for a practically null effect. For each possible true effect, a given method's performance is considered poor if it leads to either of these mistakes. Thus, if \u03b4 = 0, an upward bias in mean error of 0.15 or more is poor performance, as the method, on average, would indicate a practically significant effect even though there is a true null effect (because 0 + 0.15 = 0.15). If \u03b4 = 0.2, a downward bias in mean error of 0.05 or more is poor performance, as a practically significant true effect would be underestimated as practically nonsignificant (i.e., 0.2 \u2013 0.05 = 0.15). If \u03b4 = 0.5, a downward bias in mean error of 0.35 or more would lead to the same mistake (because 0.5 \u2013 0.35 = 0.15).",
    "is_useful": true,
    "question": "What constitutes poor performance in a meta-analytic method when evaluating the accuracy of estimators?"
  },
  {
    "text": "For each possible true effect, a given method's performance is considered poor if it leads to either of these mistakes. Thus, if \u03b4 = 0, an upward bias in mean error of 0.15 or more is poor performance, as the method, on average, would indicate a practically significant effect even though there is a true null effect (because 0 + 0.15 = 0.15). If \u03b4 = 0.2, a downward bias in mean error of 0.05 or more is poor performance, as a practically significant true effect would be underestimated as practically nonsignificant (i.e., 0.2 \u2013 0.05 = 0.15). If \u03b4 = 0.5, a downward bias in mean error of 0.35 or more would lead to the same mistake (because 0.5 \u2013 0.35 = 0.15).\n\n#### *Considering method performance in a sensitivity*\n\n*analysis.* With these definitions for poor performance and plausible conditions in hand, our interactive app (http://www.shinyapps.org/apps/metaExplorer/) can be used to judge whether meta-analytic conclusions rely on methods that perform poorly in plausible conditions. The app indicates that the conclusion that the depletion effect is practically significant is indefensible, as the RE, trimand-fill, WAAP-WLS, 3PSM, p-curve, and p-uniform methods, which lead to this conclusion, all perform poorly in at least one of the defined plausible conditions (primarily when \u03b4 = 0; Table 2).",
    "is_useful": true,
    "question": "What factors determine poor performance in statistical methods used for analyzing true effects in open science?"
  },
  {
    "text": "For the data from Hagger et al. (2016), a very different set of conditions seem plausible. Because Hagger et al.'s data come from a Registered Replication Report, there is no reason to think that the results were influenced by publication bias or QRPs to any substantial degree. Furthermore, one can expect significantly less heterogeneity in these data as compared with those from Carter et al. (2015) because data collection at each location was conducted using the identical study design and a preregistered, automated script. Furthermore, there is evidence that heterogeneity in these kinds of multilab registered reports is generally low (Klein et al., 2018). Thus, from among the conditions we simulated, we imagine that researchers would view the most plausible conditions as having no publication bias, no QRPs, no heterogeneity (\u03c4 = 0), and effect sizes (\u03b4) less than or equal to 0.5. Using the same definition of poor performance as for the meta-analysis of Carter et al., we evaluate each method's performance in these conditions at k = 30 (the simulated value closest to the observed k of 23 studies).\n\nThe results from our Web app are shown in Table 3. Unlike the results obtained with the Carter et al. (2015) data (Table 2), these results uniformly suggest that the depletion effect is practically nonsignificant. None of the methods\u2014except p-curve and p-uniform\u2014perform poorly in any of the plausible conditions we defined.",
    "is_useful": true,
    "question": "What conditions are anticipated to be present in a Registered Replication Report that could impact the results regarding publication bias and heterogeneity?"
  },
  {
    "text": "Thus, from among the conditions we simulated, we imagine that researchers would view the most plausible conditions as having no publication bias, no QRPs, no heterogeneity (\u03c4 = 0), and effect sizes (\u03b4) less than or equal to 0.5. Using the same definition of poor performance as for the meta-analysis of Carter et al., we evaluate each method's performance in these conditions at k = 30 (the simulated value closest to the observed k of 23 studies).\n\nThe results from our Web app are shown in Table 3. Unlike the results obtained with the Carter et al. (2015) data (Table 2), these results uniformly suggest that the depletion effect is practically nonsignificant. None of the methods\u2014except p-curve and p-uniform\u2014perform poorly in any of the plausible conditions we defined.\n\nIn summary, we performed two method performance checks in two different plausible research environments. In both cases, the estimates produced by those methods that a priori could be expected to perform reasonably well in plausible conditions suggested that the true ego-depletion effect is not practically or significantly different from zero.\n\n*Further considerations for method performance checks and sensitivity analyses.* We conclude our discussion of our proposed approach by mentioning some important considerations. First, the form of our method performance check depends on the specifics of our simulation design. Because our simulation covered only a limited set of possible data-generation processes, it is possible that our approach does not generalize to real-world situations that meta-analysts and consumers of meta-analysis will encounter.",
    "is_useful": true,
    "question": "What factors should researchers consider to ensure effective performance checks in simulated research environments?"
  },
  {
    "text": "First, the form of our method performance check depends on the specifics of our simulation design. Because our simulation covered only a limited set of possible data-generation processes, it is possible that our approach does not generalize to real-world situations that meta-analysts and consumers of meta-analysis will encounter. Indeed, because of the infinite possible processes that might generate real-world\n\ndata, generalizability will always be a concern.\n\nSecond, we suspect that some readers will worry that the method performance check we have described is subject to a kind of \"assumption hacking\" whereby researchers who are partial to a certain view can pick and choose the definitions of plausible conditions and poor performance that provide the result they want. This concern is technically correct, but the key strength of our approach is that it is explicit and transparent. Researchers will need to clearly state their assumptions to run this method performance check, and consumers of the results can assess whether such assumptions appear reasonable to them. If potentially hacked assumptions do not appear to be reasonable, our Web app can easily be used to run an alternative method performance check, thereby preventing the possibility of effective assumption hacking. Furthermore, we suggest that analysts preregister a method performance check prior to data collection and define which methods will be given the greatest weight if different methods provide conflicting results. Finally, we encourage researchers to report results from all meta-analytic methods that reasonably can be considered, even if they did not pass some of the method performance checks, because other researchers might want to apply a different emphasis in their subjective evaluation.",
    "is_useful": true,
    "question": "What are some concerns related to the generalizability and transparency of method performance checks in open science? "
  },
  {
    "text": "This concern is technically correct, but the key strength of our approach is that it is explicit and transparent. Researchers will need to clearly state their assumptions to run this method performance check, and consumers of the results can assess whether such assumptions appear reasonable to them. If potentially hacked assumptions do not appear to be reasonable, our Web app can easily be used to run an alternative method performance check, thereby preventing the possibility of effective assumption hacking. Furthermore, we suggest that analysts preregister a method performance check prior to data collection and define which methods will be given the greatest weight if different methods provide conflicting results. Finally, we encourage researchers to report results from all meta-analytic methods that reasonably can be considered, even if they did not pass some of the method performance checks, because other researchers might want to apply a different emphasis in their subjective evaluation.\n\n## *Ways forward*\n\nOn the basis of our results, we emphasize that metaanalysis in psychology is difficult. Observable factors such as small samples\u2014both in the primary literature and at the level of the meta-analysis\u2014interact with heterogeneity and bias, both of which have unknowable severity and functional form (e.g., do the true effects follow a normal distribution?). Thus, it is hard to interpret the results of a meta-analysis in psychology, just as it is difficult to interpret the results of any single replication study (Braver, Thoemmes, & Rosenthal, 2014; Fabrigar & Wegener, 2016; D. J. Stanley & Spence, 2014).",
    "is_useful": true,
    "question": "What practices are recommended to enhance transparency and reliability in conducting method performance checks in research?"
  },
  {
    "text": "Finally, we encourage researchers to report results from all meta-analytic methods that reasonably can be considered, even if they did not pass some of the method performance checks, because other researchers might want to apply a different emphasis in their subjective evaluation.\n\n## *Ways forward*\n\nOn the basis of our results, we emphasize that metaanalysis in psychology is difficult. Observable factors such as small samples\u2014both in the primary literature and at the level of the meta-analysis\u2014interact with heterogeneity and bias, both of which have unknowable severity and functional form (e.g., do the true effects follow a normal distribution?). Thus, it is hard to interpret the results of a meta-analysis in psychology, just as it is difficult to interpret the results of any single replication study (Braver, Thoemmes, & Rosenthal, 2014; Fabrigar & Wegener, 2016; D. J. Stanley & Spence, 2014).\n\nMeta-analysts might hope that different bias-correcting methods will all converge on a true value. However, our simulations show that different methods often do not converge. For example, in the case of a true null effect and strong publication bias, the PET and trimand-fill methods will virtually never give the same answer because the latter performs so poorly. For this reason, we caution against ideas of \"triangulation\" or basing conclusions on a \"majority vote\" of multiple methods. One should instead think carefully about which method (or methods) can be expected to perform well.",
    "is_useful": true,
    "question": "What challenges do researchers face when interpreting results from meta-analyses in psychology?"
  },
  {
    "text": "Thus, it is hard to interpret the results of a meta-analysis in psychology, just as it is difficult to interpret the results of any single replication study (Braver, Thoemmes, & Rosenthal, 2014; Fabrigar & Wegener, 2016; D. J. Stanley & Spence, 2014).\n\nMeta-analysts might hope that different bias-correcting methods will all converge on a true value. However, our simulations show that different methods often do not converge. For example, in the case of a true null effect and strong publication bias, the PET and trimand-fill methods will virtually never give the same answer because the latter performs so poorly. For this reason, we caution against ideas of \"triangulation\" or basing conclusions on a \"majority vote\" of multiple methods. One should instead think carefully about which method (or methods) can be expected to perform well. We think a good approach is the combination of a method performance check with a subsequent sensitivity analysis, either as we have defined sensitivity analysis or as put forward by other researchers (e.g., Copas, 2013; Copas & Shi, 2000; Kim, Bangdiwala, Thaler, & Gartlehner, 2014; Vevea & Woods, 2005).\n\nFurthermore, we conclude that the field should modify its expectations about meta-analysis (a similar argument has been made regarding replication results, e.g., by D. J. Stanley & Spence, 2014).",
    "is_useful": true,
    "question": "What challenges are associated with interpreting meta-analyses and replication studies in psychology?"
  },
  {
    "text": "For this reason, we caution against ideas of \"triangulation\" or basing conclusions on a \"majority vote\" of multiple methods. One should instead think carefully about which method (or methods) can be expected to perform well. We think a good approach is the combination of a method performance check with a subsequent sensitivity analysis, either as we have defined sensitivity analysis or as put forward by other researchers (e.g., Copas, 2013; Copas & Shi, 2000; Kim, Bangdiwala, Thaler, & Gartlehner, 2014; Vevea & Woods, 2005).\n\nFurthermore, we conclude that the field should modify its expectations about meta-analysis (a similar argument has been made regarding replication results, e.g., by D. J. Stanley & Spence, 2014). Researchers in psychology should not expect to produce conclusive, debate-ending results by conducting meta-analyses on existing literatures. Instead, we think that meta-analyses may serve best to draw attention to the existing strengths and weaknesses in a literature (e.g., Carter et al., 2015; Hilgard, Engelhardt, & Rouder, 2017; van Elk et al., 2015). Meta-analytic results can then inspire a careful reexamination of methodology and theory, perhaps followed by large-scale, preregistered replication efforts (e.g., Hagger et al., 2016).",
    "is_useful": true,
    "question": "What is a recommended approach for improving the reliability of research conclusions in the context of meta-analysis?"
  },
  {
    "text": "Furthermore, we conclude that the field should modify its expectations about meta-analysis (a similar argument has been made regarding replication results, e.g., by D. J. Stanley & Spence, 2014). Researchers in psychology should not expect to produce conclusive, debate-ending results by conducting meta-analyses on existing literatures. Instead, we think that meta-analyses may serve best to draw attention to the existing strengths and weaknesses in a literature (e.g., Carter et al., 2015; Hilgard, Engelhardt, & Rouder, 2017; van Elk et al., 2015). Meta-analytic results can then inspire a careful reexamination of methodology and theory, perhaps followed by large-scale, preregistered replication efforts (e.g., Hagger et al., 2016). Such efforts can then be summarized with RE meta-analytic methods, which show the best performance in the absence of bias (Figs. 3 and 4).\n\n## Conclusion\n\nIn simulations using effect sizes, sample sizes, QRPs, and degrees of publication bias that plausibly represent real data in psychology, we compared the performance of seven meta-analytic methods, including six intended to correct for publication bias. We found that each of the seven methods showed unacceptable performance in at least some conditions.",
    "is_useful": true,
    "question": "How should the expectations around meta-analysis be adjusted in the field of psychology according to recent findings?"
  },
  {
    "text": "Meta-analytic results can then inspire a careful reexamination of methodology and theory, perhaps followed by large-scale, preregistered replication efforts (e.g., Hagger et al., 2016). Such efforts can then be summarized with RE meta-analytic methods, which show the best performance in the absence of bias (Figs. 3 and 4).\n\n## Conclusion\n\nIn simulations using effect sizes, sample sizes, QRPs, and degrees of publication bias that plausibly represent real data in psychology, we compared the performance of seven meta-analytic methods, including six intended to correct for publication bias. We found that each of the seven methods showed unacceptable performance in at least some conditions. This is not an entirely surprising result given previous simulation studies (e.g., Hedges & Vevea, 1996; McShane et al., 2016; Moreno et al., 2009; R\u00fccker et al., 2011; Simonsohn, Nelson, & Simmons, 2014; T. D. Stanley, 2017; T. D. Stanley & Doucouliagos, 2014; van Aert et al., 2016). However, it highlights an important conclusion that we believe needs to be more widely acknowledged: Meta-analysts in psychology and consumers of those meta-analyses should not expect to come to definitive conclusions. Instead, we believe that the most productive outcomes will be generated by method performance checks, sensitivity analyses, and a willingness to carefully design and conduct preregistered replications.",
    "is_useful": true,
    "question": "What are some recommended practices for improving the reliability and validity of meta-analytic research in psychology?"
  },
  {
    "text": "We found that each of the seven methods showed unacceptable performance in at least some conditions. This is not an entirely surprising result given previous simulation studies (e.g., Hedges & Vevea, 1996; McShane et al., 2016; Moreno et al., 2009; R\u00fccker et al., 2011; Simonsohn, Nelson, & Simmons, 2014; T. D. Stanley, 2017; T. D. Stanley & Doucouliagos, 2014; van Aert et al., 2016). However, it highlights an important conclusion that we believe needs to be more widely acknowledged: Meta-analysts in psychology and consumers of those meta-analyses should not expect to come to definitive conclusions. Instead, we believe that the most productive outcomes will be generated by method performance checks, sensitivity analyses, and a willingness to carefully design and conduct preregistered replications.\n\n#### Action Editor\n\nDaniel J. Simons served as action editor for this article.\n\n#### Author Contributions\n\nE. C. Carter, F. D. Sch\u00f6nbrodt, and J. Hilgard developed the code for and managed the simulation study. F. D. Sch\u00f6nbrodt programmed and maintains the Web app. E. C. Carter, F. D. Sch\u00f6nbrodt, J. Hilgard and W. M. Gervais planned the project and wrote the manuscript. E. C. Carter and F. D. Sch\u00f6nbrodt contributed equally to this work.",
    "is_useful": true,
    "question": "What should meta-analysts in psychology and consumers of meta-analyses consider regarding the performance of methods in their research?"
  },
  {
    "text": "#### Author Contributions\n\nE. C. Carter, F. D. Sch\u00f6nbrodt, and J. Hilgard developed the code for and managed the simulation study. F. D. Sch\u00f6nbrodt programmed and maintains the Web app. E. C. Carter, F. D. Sch\u00f6nbrodt, J. Hilgard and W. M. Gervais planned the project and wrote the manuscript. E. C. Carter and F. D. Sch\u00f6nbrodt contributed equally to this work.\n\n#### ORCID iD\n\nJoseph Hilgard https://orcid.org/0000-0002-7278-4698\n\n#### Acknowledgments\n\nWe would like to acknowledge Tyler Yost for helpful comments on an earlier version of the simulation code and manuscript.\n\n#### Declaration of Conflicting Interests\n\nThe author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.\n\n# Open Practices\n\nOpen Data: https://osf.io/rf3ys/\n\nOpen Materials: https://osf.io/rf3ys/, http://www.shinyapps .org/apps/metaExplorer/\n\nPreregistration: not applicable\n\nAll data have been made publicly available via the Open Science Framework and can be accessed at https://osf.io/rf3ys/. All materials have been made publically available at https:// osf.io/rf3ys/ and http://www.shinyapps.org/apps/meta Explorer/.",
    "is_useful": true,
    "question": "What practices are associated with transparency and accessibility in research according to open science principles?"
  },
  {
    "text": "#### ORCID iD\n\nJoseph Hilgard https://orcid.org/0000-0002-7278-4698\n\n#### Acknowledgments\n\nWe would like to acknowledge Tyler Yost for helpful comments on an earlier version of the simulation code and manuscript.\n\n#### Declaration of Conflicting Interests\n\nThe author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.\n\n# Open Practices\n\nOpen Data: https://osf.io/rf3ys/\n\nOpen Materials: https://osf.io/rf3ys/, http://www.shinyapps .org/apps/metaExplorer/\n\nPreregistration: not applicable\n\nAll data have been made publicly available via the Open Science Framework and can be accessed at https://osf.io/rf3ys/. All materials have been made publically available at https:// osf.io/rf3ys/ and http://www.shinyapps.org/apps/meta Explorer/. The complete Open Practices Disclosure for this article can be found at http://journals.sagepub.com/doi/ suppl/10.1177/2515245919847196. This article has received badges for Open Data and Open Materials. More information about the Open Practices badges can be found at http:// www.psychologicalscience.org/publications/badges.\n\n#### Prior Versions\n\nAn earlier version of this manuscript was posted as a preprint at https://osf.io/rf3ys.\n\n#### Notes\n\n1.",
    "is_useful": true,
    "question": "What are some examples of open practices in research that promote transparency and access to data?"
  },
  {
    "text": "All materials have been made publically available at https:// osf.io/rf3ys/ and http://www.shinyapps.org/apps/meta Explorer/. The complete Open Practices Disclosure for this article can be found at http://journals.sagepub.com/doi/ suppl/10.1177/2515245919847196. This article has received badges for Open Data and Open Materials. More information about the Open Practices badges can be found at http:// www.psychologicalscience.org/publications/badges.\n\n#### Prior Versions\n\nAn earlier version of this manuscript was posted as a preprint at https://osf.io/rf3ys.\n\n#### Notes\n\n1. It is worth noting that our original intent with this study was in fact to identify, if possible, a single best method across many conditions. Further consideration and helpful comments from our peers changed our minds about this goal.\n\n2. In terms of the heterogeneity metric I 2 , the \u03c4 values of 0.2 and 0.4, in combination with the specific primary sample sizes we simulated, are approximately equal to what Pigott (2012) proposed as \"medium\" (I 2 = 50%) and \"large\" (I 2 = 75%) heterogeneity. RE meta-analysis of our simulated data in the condition with no publication bias and no QRPs, with results aggregated over k and \u03b4, yielded an average observed I 2 of 46% (SD = 17%) for \u03c4 = 0.2 and 77% (SD = 10%) for \u03c4 = 0.4.",
    "is_useful": true,
    "question": "What resources are available for accessing the materials related to open science in this study?"
  },
  {
    "text": "4. If the probability of publication was 25%, for example, we drew one random sample from a Bernoulli distribution with p = .25. If the sample value was 1, the simulated result was published.\n\n5. We also could have deleted only outliers in one direction, which would have made the p-hacking more efficient.\n\n6. Technically, we constrained the numerical optimizer to values of 0 or greater. In 10.3% of all cases, the estimate was less than 0.0001.\n\n7. Although in this special case no p-value and no confidence interval are provided, we treated these cases as indicating that the null should not be rejected. Hence, this special case was utilized in the computation of the false-positive error rate, but not the coverage probability.\n\n8. It should be noted that, in contrast to the work just cited, two meta-meta-analyses suggest that the influence of bias in psychology is relatively small (T. D. Stanley, Carter, & Doucouliagos, 2018; van Aert, 2018). Because of the specifics of each of these studies, it is difficult to reconcile their general conclusions. For our analysis here, we decided to take the conservative route and err on the side of assuming the existence of bias, but we recommend that, when applying our approach, meta-analysts consider the degree to which bias exists and explicitly describe their reasoning.\n\n9.",
    "is_useful": true,
    "question": "What statistical methods might be employed to appear more favorable in publication outcomes, particularly concerning bias and error rates?"
  },
  {
    "text": "![](_page_0_Picture_0.jpeg)\n\nAutistic Community and the Neurodiversity Movement Stories from the Frontline\n\n*Edited by* Steven K. Kapp\n\n### Autistic Community and the Neurodiversity Movement\n\nSteven K. Kapp Editor\n\n# Autistic Community and the Neurodiversity Movement\n\nStories from the Frontline\n\n*Editor* Steven K. Kapp College of Social Sciences and International Studies University of Exeter Exeter, UK\n\nDepartment of Psychology University of Portsmouth Portsmouth, UK\n\n![](_page_3_Picture_2.jpeg)\n\n#### ISBN 978-981-13-8436-3 ISBN 978-981-13-8437-0 (eBook) https://doi.org/10.1007/978-981-13-8437-0\n\n\u00a9 Te Editor(s) (if applicable) and Te Author(s) 2020. Tis book is an open access publication. **Open Access** Tis book is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.\n\nTe images or other third party material in this book are included in the book's Creative Commons license, unless indicated otherwise in a credit line to the material.",
    "is_useful": true,
    "question": "What is the licensing model that allows for the use, sharing, adaptation, and reproduction of the content in this publication?"
  },
  {
    "text": "Tis book is an open access publication. **Open Access** Tis book is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.\n\nTe images or other third party material in this book are included in the book's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the book's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\nTe use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specifc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.\n\nTe publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. Te publisher remains neutral with regard to jurisdictional claims in published maps and institutional afliations.",
    "is_useful": true,
    "question": "What are the key permissions and conditions associated with the use of an open access publication? "
  },
  {
    "text": "Te registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, Singapore\n\n## **Foreword**\n\nTis book describes some of the key actions that have defned the autism rights branch of the neurodiversity movement since it organized into a unique community over 20 years ago. Te actions covered are legendary in the autism community and range from \"Te Autistic Genocide Clock\" through to the \"Institute for the Study of the Neurologically Typical\", and famous pieces of work like \"Don't Mourn for Us\".\n\nTese acts have forged new thinking on autism and established the neurodiversity movement as a key force in promoting social change for autistic people. It is primarily autistic activists who have been at the vanguard of the neurodiversity movement. All but two of the 21 contributors to this volume identify as autistic. Te collection describes the biographies and rationale of key activists **in their own words**, thus the motto of disability rights activism \"nothing about us without us\" is a guiding tenet for the book. Te phrase (and this volume) are rooted in the concept of *standpoint epistemology*. A standpoint position claims that authority over knowledge is created through direct experience of a condition or situation. Standpoint epistemology is related to the idea of lay expertise, which is discussed extensively in the sociological literature. So, the book values the experience of autistic people as a source of knowledge about their own plight.",
    "is_useful": true,
    "question": "What key principles guide the neurodiversity movement and its approach to autism advocacy?"
  },
  {
    "text": "Tese acts have forged new thinking on autism and established the neurodiversity movement as a key force in promoting social change for autistic people. It is primarily autistic activists who have been at the vanguard of the neurodiversity movement. All but two of the 21 contributors to this volume identify as autistic. Te collection describes the biographies and rationale of key activists **in their own words**, thus the motto of disability rights activism \"nothing about us without us\" is a guiding tenet for the book. Te phrase (and this volume) are rooted in the concept of *standpoint epistemology*. A standpoint position claims that authority over knowledge is created through direct experience of a condition or situation. Standpoint epistemology is related to the idea of lay expertise, which is discussed extensively in the sociological literature. So, the book values the experience of autistic people as a source of knowledge about their own plight. Te volume acknowledges that individual contributions are shaped by contributors' political and social experience as well as their lived understanding of autism. Standpoint theory suggests inequalities foster particular standpoints, and that the perspectives of marginalized and oppressed groups can generate a fairer account of the world. Individuals from such groups are in a distinctive position to call out forms of behavior and practices of the dominant group, hopefully leading to social change. Tis collection illustrates the perspective of each contributor's unique voice. But together, the chapters powerfully illustrate the sense of a group with a shared point of view, united in a common movement.",
    "is_useful": true,
    "question": "What guiding principle emphasizes the importance of including the perspectives of marginalized groups in knowledge creation and advocacy?"
  },
  {
    "text": "A standpoint position claims that authority over knowledge is created through direct experience of a condition or situation. Standpoint epistemology is related to the idea of lay expertise, which is discussed extensively in the sociological literature. So, the book values the experience of autistic people as a source of knowledge about their own plight. Te volume acknowledges that individual contributions are shaped by contributors' political and social experience as well as their lived understanding of autism. Standpoint theory suggests inequalities foster particular standpoints, and that the perspectives of marginalized and oppressed groups can generate a fairer account of the world. Individuals from such groups are in a distinctive position to call out forms of behavior and practices of the dominant group, hopefully leading to social change. Tis collection illustrates the perspective of each contributor's unique voice. But together, the chapters powerfully illustrate the sense of a group with a shared point of view, united in a common movement.\n\nEnormous credit goes to the Editor, Steven K. Kapp, who bought this volume together and was able to simultaneously command the trust of the autism activist community and the respect of the academic community\u2014not an easy balance to get right!\n\nTe hope is this book will provide a reference text for readers interested in the history and ideas of the neurodiversity movement and how these ideas have shaped production of expert and especially lay knowledge about autism. However, the neurodiversity movement has been problematized by both parents and academics for being unrepresentative and divisive, and the book also addresses some of these critiques.",
    "is_useful": true,
    "question": "How does standpoint theory contribute to understanding knowledge production, particularly in relation to marginalized groups?"
  },
  {
    "text": "Individuals from such groups are in a distinctive position to call out forms of behavior and practices of the dominant group, hopefully leading to social change. Tis collection illustrates the perspective of each contributor's unique voice. But together, the chapters powerfully illustrate the sense of a group with a shared point of view, united in a common movement.\n\nEnormous credit goes to the Editor, Steven K. Kapp, who bought this volume together and was able to simultaneously command the trust of the autism activist community and the respect of the academic community\u2014not an easy balance to get right!\n\nTe hope is this book will provide a reference text for readers interested in the history and ideas of the neurodiversity movement and how these ideas have shaped production of expert and especially lay knowledge about autism. However, the neurodiversity movement has been problematized by both parents and academics for being unrepresentative and divisive, and the book also addresses some of these critiques.\n\nTe target academic audience is primarily undergraduates and scholars in sociology, history of medicine, and psychiatry. Tis collection of activists' stories should act as a reference text useful as a source for further academic debate and analysis. Another important set of readers are parents wishing to learn more, and of course autistic persons themselves. Our generous funder, the UK's Wellcome Trust, has supported the publication of this volume on an open access platform to make it available for free online.\n\nTe book is not a complex analysis or a \"celebratory\" piece; instead it ofers raw frst-person accounts, relating how and why activists have contributed.",
    "is_useful": true,
    "question": "What role do marginalized groups play in challenging dominant narratives and fostering social change in the context of open science?"
  },
  {
    "text": "However, the neurodiversity movement has been problematized by both parents and academics for being unrepresentative and divisive, and the book also addresses some of these critiques.\n\nTe target academic audience is primarily undergraduates and scholars in sociology, history of medicine, and psychiatry. Tis collection of activists' stories should act as a reference text useful as a source for further academic debate and analysis. Another important set of readers are parents wishing to learn more, and of course autistic persons themselves. Our generous funder, the UK's Wellcome Trust, has supported the publication of this volume on an open access platform to make it available for free online.\n\nTe book is not a complex analysis or a \"celebratory\" piece; instead it ofers raw frst-person accounts, relating how and why activists have contributed. It aims to preserve and document the stories of some of the original activists whose voices helped shape and inspire the fedgling neurodiversity movement.\n\nEnjoy!\n\nExeter, UK Ginny Russell\n\n## **Preface**\n\nTis book has emerged from a postdoctoral research fellowship within sociology in the U.K. as part of the Wellcome Trust-funded project *Exploring Diagnosis: Autism and the Neurodiversity Movement*, which includes academic engagement with the movement as well as critical analysis of its position. Te following chapters provide an overview of the neurodiversity movement, describing the key actions of autistic activists in the movement between 1992 and the present day, in their own words.",
    "is_useful": true,
    "question": "What purpose does the book serve for its readers in relation to the neurodiversity movement?"
  },
  {
    "text": "Te book is not a complex analysis or a \"celebratory\" piece; instead it ofers raw frst-person accounts, relating how and why activists have contributed. It aims to preserve and document the stories of some of the original activists whose voices helped shape and inspire the fedgling neurodiversity movement.\n\nEnjoy!\n\nExeter, UK Ginny Russell\n\n## **Preface**\n\nTis book has emerged from a postdoctoral research fellowship within sociology in the U.K. as part of the Wellcome Trust-funded project *Exploring Diagnosis: Autism and the Neurodiversity Movement*, which includes academic engagement with the movement as well as critical analysis of its position. Te following chapters provide an overview of the neurodiversity movement, describing the key actions of autistic activists in the movement between 1992 and the present day, in their own words. Although previous books have provided coverage of the history of autism inclusive of the neurodiversity movement as narrated by a journalist or researcher [10, 12] or featured anthologies of contributors from autistic people published within the movement [1, 3, 9], this edited collection provides the frst history of the movement from frsthand accounts of members of the autistic community and both autistic and non-autistic parent movement activists. Following my introduction to the movement and contributions, the book contains 19 chapters by 21 authors organized into parts about the forming of the autistic community and neurodiversity movement, progress in their infuence on the broader autism community and feld, and their possible threshold of the advocacy establishment.",
    "is_useful": true,
    "question": "What is the significance of firsthand accounts in documenting the neurodiversity movement and its activists?"
  },
  {
    "text": "Te following chapters provide an overview of the neurodiversity movement, describing the key actions of autistic activists in the movement between 1992 and the present day, in their own words. Although previous books have provided coverage of the history of autism inclusive of the neurodiversity movement as narrated by a journalist or researcher [10, 12] or featured anthologies of contributors from autistic people published within the movement [1, 3, 9], this edited collection provides the frst history of the movement from frsthand accounts of members of the autistic community and both autistic and non-autistic parent movement activists. Following my introduction to the movement and contributions, the book contains 19 chapters by 21 authors organized into parts about the forming of the autistic community and neurodiversity movement, progress in their infuence on the broader autism community and feld, and their possible threshold of the advocacy establishment. Tis is followed by a description of some critiques of the movement, and I follow with the conclusion.\n\nTe *Exploring Diagnosis* project research inquires into how diagnosis catalyzes mobilization, focusing on autistic adults and the neurodiversity movement (NDM). Te project aimed to assess what the understandings of neurodiversity are among autistic adults. Ginny Russell, the project lead, and I conducted a thematic analysis [2] of autistic adults' responses to the question \"What is neurodiversity in your own words\" from a study I co-led with Kristen Gillespie Lynch et al. [4].",
    "is_useful": true,
    "question": "What does the neurodiversity movement focus on regarding the experiences and perspectives of autistic individuals? "
  },
  {
    "text": "Following my introduction to the movement and contributions, the book contains 19 chapters by 21 authors organized into parts about the forming of the autistic community and neurodiversity movement, progress in their infuence on the broader autism community and feld, and their possible threshold of the advocacy establishment. Tis is followed by a description of some critiques of the movement, and I follow with the conclusion.\n\nTe *Exploring Diagnosis* project research inquires into how diagnosis catalyzes mobilization, focusing on autistic adults and the neurodiversity movement (NDM). Te project aimed to assess what the understandings of neurodiversity are among autistic adults. Ginny Russell, the project lead, and I conducted a thematic analysis [2] of autistic adults' responses to the question \"What is neurodiversity in your own words\" from a study I co-led with Kristen Gillespie Lynch et al. [4]. Russell and I found that the data largely mapped onto defnitions autistic adults in the movement have given [5, 11] defning the NDM as encompassing both human biological diferences in cognition, brains, and genes while also serving as an activist tool for change toward acceptance and inclusion of autistic and other *neurodivergent* people. Responses described neurodivergent people as socially oppressed and stigmatized, yet possessing valuable diferences that should not be cured but instead supported with rights and accommodations.",
    "is_useful": true,
    "question": "What insights does research into the understanding of neurodiversity among autistic adults reveal about their experiences and perspectives within the neurodiversity movement?"
  },
  {
    "text": "Te project aimed to assess what the understandings of neurodiversity are among autistic adults. Ginny Russell, the project lead, and I conducted a thematic analysis [2] of autistic adults' responses to the question \"What is neurodiversity in your own words\" from a study I co-led with Kristen Gillespie Lynch et al. [4]. Russell and I found that the data largely mapped onto defnitions autistic adults in the movement have given [5, 11] defning the NDM as encompassing both human biological diferences in cognition, brains, and genes while also serving as an activist tool for change toward acceptance and inclusion of autistic and other *neurodivergent* people. Responses described neurodivergent people as socially oppressed and stigmatized, yet possessing valuable diferences that should not be cured but instead supported with rights and accommodations. Considering that research participants knew of but did not necessarily subscribe to the movement, the consensus that emerged suggested clarity about the meaning of (the) neurodiversity (movement) as understood by aware autistic adults. Indeed, the descriptions of these terms from contributors to this book who chose to put them forward, aligned well with those in both the popular literature and aforementioned analysis.\n\nAs contributors (including myself) make clear, we think autism involves strengths and weaknesses that amount to both a diference and a disability, and do not consider autism an advantage overall but see autistic people as socially disadvantaged.",
    "is_useful": true,
    "question": "What are the main understandings of neurodiversity as expressed by autistic adults, including their views on inclusion and support?"
  },
  {
    "text": "Responses described neurodivergent people as socially oppressed and stigmatized, yet possessing valuable diferences that should not be cured but instead supported with rights and accommodations. Considering that research participants knew of but did not necessarily subscribe to the movement, the consensus that emerged suggested clarity about the meaning of (the) neurodiversity (movement) as understood by aware autistic adults. Indeed, the descriptions of these terms from contributors to this book who chose to put them forward, aligned well with those in both the popular literature and aforementioned analysis.\n\nAs contributors (including myself) make clear, we think autism involves strengths and weaknesses that amount to both a diference and a disability, and do not consider autism an advantage overall but see autistic people as socially disadvantaged. No one emphasized strengths overall or highlighted particular strengths, but we take a \"big-tent\" approach to autism that recognizes no one's worth depends on having particular talents or abilities. Te *Exploring Diagnosis* project corroborated this further by interviewing autistic adults in the UK about the strengths participants think autism confers, which almost everyone identifed, and yet they mentioned moderating infuences that could make these traits function as challenges [8].\n\nConversely, the book most addresses the question of what political activities the NDM has taken part in, and how these have challenged the notions of diagnosis and intervention for autism.",
    "is_useful": true,
    "question": "What are the perspectives on neurodivergent individuals regarding their differences, rights, and the political activities related to autism?"
  },
  {
    "text": "As contributors (including myself) make clear, we think autism involves strengths and weaknesses that amount to both a diference and a disability, and do not consider autism an advantage overall but see autistic people as socially disadvantaged. No one emphasized strengths overall or highlighted particular strengths, but we take a \"big-tent\" approach to autism that recognizes no one's worth depends on having particular talents or abilities. Te *Exploring Diagnosis* project corroborated this further by interviewing autistic adults in the UK about the strengths participants think autism confers, which almost everyone identifed, and yet they mentioned moderating infuences that could make these traits function as challenges [8].\n\nConversely, the book most addresses the question of what political activities the NDM has taken part in, and how these have challenged the notions of diagnosis and intervention for autism. Te contributors illustrate notable examples of the manifestos (both ideological and as applied to policy), mailing lists, websites, conferences, issue campaigns, academic projects and journals, books, organizations, and advisory roles to parent- and professional-led bodies that constitute some of the range of the neurodiversity movement's political activities. Tese actions have had widespread impacts toward an emerging view among families, practitioners, researchers, and the public on autism as *both* a diference and disability.\n\n#### **Positionality Brought to the Book**\n\nI am both an autism scholar and an autistic neurodiversity activist, so while I seek to maintain high standards of rigor and fairness in editing this collection, it may refect this positionality.",
    "is_useful": true,
    "question": "How does the neurodiversity movement influence perceptions of autism in relation to its strengths and challenges?"
  },
  {
    "text": "Tese actions have had widespread impacts toward an emerging view among families, practitioners, researchers, and the public on autism as *both* a diference and disability.\n\n#### **Positionality Brought to the Book**\n\nI am both an autism scholar and an autistic neurodiversity activist, so while I seek to maintain high standards of rigor and fairness in editing this collection, it may refect this positionality. In 2007 Scott Robertson, the co-founder and then Vice President of the Autistic Self Advocacy Network, reached out to me privately on Facebook and undertook a mentorship role in which he introduced me to the neurodiversity and disability rights movements. At that time, as an undergraduate, I lived in my hometown of Los Angeles in the world's region (Southern California) most dominated by the mainstream medical and alternative medical autism establishment. Groups like the Center for Autism and Related Disorders (CARD) and the Lovaas Institute anchored the provision of therapy based in applied behavioral analysis designed to \"recover\" autistic children, alongside the parent-based advocacy organization Cure Autism Now (which the similar organization Autism Speaks absorbed that year). Meanwhile, groups such as Talking About Curing Autism (TACA) and Generation Rescue (both represented around this time by Jenny McCarthy, \"Ph.D. in Google Research\" [6]) spread potentially deadly disinformation including vaccine skepticism and chelation therapy. Amid this hostile climate, I decided to earn an actual Ph.D.",
    "is_useful": true,
    "question": "What perspective on autism are families, practitioners, researchers, and the public increasingly embracing?"
  },
  {
    "text": "At that time, as an undergraduate, I lived in my hometown of Los Angeles in the world's region (Southern California) most dominated by the mainstream medical and alternative medical autism establishment. Groups like the Center for Autism and Related Disorders (CARD) and the Lovaas Institute anchored the provision of therapy based in applied behavioral analysis designed to \"recover\" autistic children, alongside the parent-based advocacy organization Cure Autism Now (which the similar organization Autism Speaks absorbed that year). Meanwhile, groups such as Talking About Curing Autism (TACA) and Generation Rescue (both represented around this time by Jenny McCarthy, \"Ph.D. in Google Research\" [6]) spread potentially deadly disinformation including vaccine skepticism and chelation therapy. Amid this hostile climate, I decided to earn an actual Ph.D. (in educational psychology) at the heart of the medical model of autism research, the University of California Los Angeles, to learn the thinking and language of mainstream science to better critique them. Over this time I observed early autistic leaders (most prominently Jim Sinclair) become much less active, and their products become increasingly difcult to access, such as websites no longer hosted or archived. Leaders of autism's cure movement took down materials as well, but this more often happened to obscure their more outrageous products in response to autistic-led resistance, as they usually have the resources to continue displaying them (see Rosenblatt [7] for examples). Te need to preserve and document the history of the autistic community and neurodiversity movement had become apparent.",
    "is_useful": true,
    "question": "What challenges does the autism community face in preserving its history and advocating for neurodiversity?"
  },
  {
    "text": "in Google Research\" [6]) spread potentially deadly disinformation including vaccine skepticism and chelation therapy. Amid this hostile climate, I decided to earn an actual Ph.D. (in educational psychology) at the heart of the medical model of autism research, the University of California Los Angeles, to learn the thinking and language of mainstream science to better critique them. Over this time I observed early autistic leaders (most prominently Jim Sinclair) become much less active, and their products become increasingly difcult to access, such as websites no longer hosted or archived. Leaders of autism's cure movement took down materials as well, but this more often happened to obscure their more outrageous products in response to autistic-led resistance, as they usually have the resources to continue displaying them (see Rosenblatt [7] for examples). Te need to preserve and document the history of the autistic community and neurodiversity movement had become apparent. Like my career, the book merges science and advocacy, intended for both academia and the autism community.\n\n#### Exeter, UK Steven K. Kapp\n\n### **References**\n\n- 1. Bascom, J. (2012). *Loud hands: Autistic people, speaking*. Washington, DC: Autistic Self Advocacy Network.\n- 2. Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. *Qualitative Research in Psychology*, 3(2), 77\u2013101.\n- 3.",
    "is_useful": true,
    "question": "What is the importance of documenting the history of the autistic community and the neurodiversity movement? "
  },
  {
    "text": "Leaders of autism's cure movement took down materials as well, but this more often happened to obscure their more outrageous products in response to autistic-led resistance, as they usually have the resources to continue displaying them (see Rosenblatt [7] for examples). Te need to preserve and document the history of the autistic community and neurodiversity movement had become apparent. Like my career, the book merges science and advocacy, intended for both academia and the autism community.\n\n#### Exeter, UK Steven K. Kapp\n\n### **References**\n\n- 1. Bascom, J. (2012). *Loud hands: Autistic people, speaking*. Washington, DC: Autistic Self Advocacy Network.\n- 2. Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. *Qualitative Research in Psychology*, 3(2), 77\u2013101.\n- 3. Brown, L. X., Ashkenazy, E., & Onaiwu, M. G. (2017). *All the weight of our dreams: On living racialized autism*. Lincoln, NE: DragonBee Press.\n- 4. Gillespie-Lynch, K., Kapp, S. K., Pickens, J., Brooks, P., & Schwartzman, B. (2017). Whose expertise is it? Evidence for autistic adults as critical autism experts. *Frontiers in Psychology, 8*, 438.\n- 5.",
    "is_useful": true,
    "question": "What is the significance of preserving the history of the autistic community and the neurodiversity movement in the context of science and advocacy?"
  },
  {
    "text": "Ari Ne'eman provided input that shaped the direction of the book, most directly through helping with selecting and recruiting authors. Ari and Dr. Scott Michael Robertson created a vehicle for my activism through co-founding the Autistic Self Advocacy Network, and I am indebted to Scott for introducing me to the disability rights and neurodiversity movements and providing patient, knowledgeable mentorship. Nick Chown produced a professional index that beneftted from his insider status as an autistic neurodiversity scholar. Ryan Smoluk created the cover art, \"Te Path (Planning Alternative Tomorrows with Hope)\", which ofers a brilliant image from a member of the autistic community that resonates with work of the neurodiversity movement and spirit of the book.\n\n## **Contents**\n\n| 1 | Introduction | 1 |\n| --- | --- | --- |\n|  | Steven K. Kapp |  |\n| Part I | Gaining Community |  |\n| 2 | Historicizing Jim Sinclair's \"Don't Mourn for Us\": |  |\n|  | A Cultural and Intellectual History of Neurodiversity's |  |\n|  | First Manifesto | 23 |\n|  | Sarah Pripas-Kapit |  |\n| 3 | From Exclusion to Acceptance: Independent Living |  |\n|  | on the Autistic Spectrum | 41 |\n|  | Martijn Dekker |  |\n| 4 | Autistic People Against Neuroleptic Abuse | 51 |\n|  | Dinah Murray |  |\n\n5 **Autistics.",
    "is_useful": true,
    "question": "What contributions have been made to the development and advocacy of neurodiversity and the autistic community?"
  },
  {
    "text": "| 4.3 | Te APANA logo, designed by Ralph Smith | 58 |\n| Fig. | 9.1 | Te Autistic Genocide Clock by Meg Evans | 124 |\n| Fig. | 11.1 | Autistic Women and Non-Binary Network tagline | 153 |\n| Fig. | 20.1 | Anti-Autism Speaks Logo designed by Dinah Murray | 279 |\n| Fig. | 20.2 | Another Anti-Autism Speaks Logo circulating in 2008, |  |\n|  |  | anonymous | 279 |\n\n![](_page_17_Picture_0.jpeg)\n\n# 1\n\n## **Introduction**\n\n**Steven K. Kapp**\n\nThis book marks the first historical overview of the autistic community and the neurodiversity movement that describes the activities and rationale of key leaders in their own words. All authors of the core chapters consider themselves part of the autistic community or the neurodiversity movement (including a couple among the growing legion of non-autistic parents), or both in most cases. Their first-hand accounts provide coverage from the radical beginnings of autistic culture to the present cross-disability socio-political impacts. These have shifted the landscape toward viewing autism in social terms of human rights and identity to accept, rather than as a medical collection of deficits and symptoms to cure.",
    "is_useful": true,
    "question": "What does the historical overview of the autistic community and the neurodiversity movement focus on in terms of their activities and rationale?"
  },
  {
    "text": "| 20.2 | Another Anti-Autism Speaks Logo circulating in 2008, |  |\n|  |  | anonymous | 279 |\n\n![](_page_17_Picture_0.jpeg)\n\n# 1\n\n## **Introduction**\n\n**Steven K. Kapp**\n\nThis book marks the first historical overview of the autistic community and the neurodiversity movement that describes the activities and rationale of key leaders in their own words. All authors of the core chapters consider themselves part of the autistic community or the neurodiversity movement (including a couple among the growing legion of non-autistic parents), or both in most cases. Their first-hand accounts provide coverage from the radical beginnings of autistic culture to the present cross-disability socio-political impacts. These have shifted the landscape toward viewing autism in social terms of human rights and identity to accept, rather than as a medical collection of deficits and symptoms to cure. The exception to personal accounts and part of the impetus for the book, Jim Sinclair, has become inactive since leading the autism rights movement's development\n\nS. K. Kapp (B)\n\nCollege of Social Sciences and International Studies, University of Exeter, Exeter, UK\n\ne-mail: steven.kapp@port.ac.uk\n\nDepartment of Psychology, University of Portsmouth, Portsmouth, UK\n\nof culture and identity after co-founding its first organization Autism Network International (ANI) in 1992.",
    "is_useful": true,
    "question": "What major shift in perspective regarding autism is represented in the neurodiversity movement?"
  },
  {
    "text": "Their first-hand accounts provide coverage from the radical beginnings of autistic culture to the present cross-disability socio-political impacts. These have shifted the landscape toward viewing autism in social terms of human rights and identity to accept, rather than as a medical collection of deficits and symptoms to cure. The exception to personal accounts and part of the impetus for the book, Jim Sinclair, has become inactive since leading the autism rights movement's development\n\nS. K. Kapp (B)\n\nCollege of Social Sciences and International Studies, University of Exeter, Exeter, UK\n\ne-mail: steven.kapp@port.ac.uk\n\nDepartment of Psychology, University of Portsmouth, Portsmouth, UK\n\nof culture and identity after co-founding its first organization Autism Network International (ANI) in 1992. Yet this book respects the disability rights motto of \"Nothing About Us Without Us\" by commissioning an autistic historian and chairperson of an organization inspired by ANI's historic autistic community retreat to analyze the context and impact of Sinclair's legendary work. Similarly, I am an autistic neurodiversity activist (a role that precedes my career as an autism researcher), but I endeavor to apply robust scholarly standards to editing this collection (see the Preface).\n\n#### **Introduction to the Neurodiversity Movement**\n\nMany descriptions arguably misunderstand the concept of *neurodiversity* and the framework and actions of the *neurodiversity movement*, so this chapter seeks to explain them before introducing the core chapters.",
    "is_useful": true,
    "question": "What impact has the neurodiversity movement had on the perception of autism within social and human rights contexts?"
  },
  {
    "text": "Yet this book respects the disability rights motto of \"Nothing About Us Without Us\" by commissioning an autistic historian and chairperson of an organization inspired by ANI's historic autistic community retreat to analyze the context and impact of Sinclair's legendary work. Similarly, I am an autistic neurodiversity activist (a role that precedes my career as an autism researcher), but I endeavor to apply robust scholarly standards to editing this collection (see the Preface).\n\n#### **Introduction to the Neurodiversity Movement**\n\nMany descriptions arguably misunderstand the concept of *neurodiversity* and the framework and actions of the *neurodiversity movement*, so this chapter seeks to explain them before introducing the core chapters.\n\nThe term *neurodiversity* originates from the autism rights movement in 1998 from Judy Singer on Martijn Dekker's mailing list InLv, but as the movement has matured into a more active part of a cross-disability rights coalition, the term has evolved to become more politicized and radical (a change noted by a few contributors, especially Dekker in Chapter 3). *Neurodiversity* has come to mean \"variation in neurocognitive functioning\" (p. 3) [1], a broad concept that includes everyone: both *neurodivergent* people (those with a condition that renders their neurocognitive functioning significantly different from a \"normal\" range) and *neurotypical* people (those within that socially acceptable range).",
    "is_useful": true,
    "question": "What is the origin and meaning of the term neurodiversity within the context of disability rights?"
  },
  {
    "text": "The term *neurodiversity* originates from the autism rights movement in 1998 from Judy Singer on Martijn Dekker's mailing list InLv, but as the movement has matured into a more active part of a cross-disability rights coalition, the term has evolved to become more politicized and radical (a change noted by a few contributors, especially Dekker in Chapter 3). *Neurodiversity* has come to mean \"variation in neurocognitive functioning\" (p. 3) [1], a broad concept that includes everyone: both *neurodivergent* people (those with a condition that renders their neurocognitive functioning significantly different from a \"normal\" range) and *neurotypical* people (those within that socially acceptable range). The *neurodiversity movement* advocates for the rights of neurodivergent people, applying a framework or approach that values the full spectra of differences and rights such as inclusion and autonomy. The movement arguably adopts a spectrum or dimensional concept to neurodiversity, in which people's neurocognitive differences largely have no natural boundaries. While the extension from this concept to group-based identity politics that distinguish between the neurodivergent and neurotypical may at first seem contradictory, the neurodiversity framework draws from reactions to *existing* stigma- and mistreatment-inducing medical categories *imposed* on people that they *reclaim* by negotiating their meaning into an affirmative construct.",
    "is_useful": true,
    "question": "What movement advocates for the rights and inclusion of individuals with variations in neurocognitive functioning?"
  },
  {
    "text": "The *neurodiversity movement* advocates for the rights of neurodivergent people, applying a framework or approach that values the full spectra of differences and rights such as inclusion and autonomy. The movement arguably adopts a spectrum or dimensional concept to neurodiversity, in which people's neurocognitive differences largely have no natural boundaries. While the extension from this concept to group-based identity politics that distinguish between the neurodivergent and neurotypical may at first seem contradictory, the neurodiversity framework draws from reactions to *existing* stigma- and mistreatment-inducing medical categories *imposed* on people that they *reclaim* by negotiating their meaning into an affirmative construct. People who are not discriminated against on the basis of their perceived or actual neurodivergences arguably benefit from neurotypical privilege [2], so they do not need corresponding legal protections and access to services. I have observed little serious aggrandizement of neurodivergent people or denigration of neurotypical people, but satire has been misinterpreted (Tisoncik, Chapter 5) or rhetoric misunderstood due to disability-related communication or class differences.\n\n#### **The Diversity in** *Neurodiversity*\n\nAlthough the people for whom the neurodiversity movement advocates far exceed autistic people, they also fall outside the main scope of the book. Some contributors' topics do include campaigns directly affecting people with other disabilities, such as that to close the tortuous Judge Rotenberg Center in the U.S.",
    "is_useful": true,
    "question": "What is the main focus of the neurodiversity movement and how does it relate to the rights and recognition of neurodivergent individuals?"
  },
  {
    "text": "People who are not discriminated against on the basis of their perceived or actual neurodivergences arguably benefit from neurotypical privilege [2], so they do not need corresponding legal protections and access to services. I have observed little serious aggrandizement of neurodivergent people or denigration of neurotypical people, but satire has been misinterpreted (Tisoncik, Chapter 5) or rhetoric misunderstood due to disability-related communication or class differences.\n\n#### **The Diversity in** *Neurodiversity*\n\nAlthough the people for whom the neurodiversity movement advocates far exceed autistic people, they also fall outside the main scope of the book. Some contributors' topics do include campaigns directly affecting people with other disabilities, such as that to close the tortuous Judge Rotenberg Center in the U.S. (Neumeier and Brown, Chapter 14) and to pass the Autism/Neurodiversity Manifesto in the U.K. (Craine, Chapter 19), yet the movement remains led by autistic people. Mainly though, the scope of the movement remains unclear; at a disability studies conference I asked participants how they felt about minimum criteria for eligibility within it, but they felt uncomfortable posing limits [3]. A woman suggested her multiple sclerosis should qualify; indeed, coverage of people with not only chronic illnesses but also primary sensory disabilities like blindness and psychiatric conditions like schizophrenia remain unclear [4].",
    "is_useful": true,
    "question": "What are the key challenges faced by the neurodiversity movement regarding its scope and inclusion of various disabilities?"
  },
  {
    "text": "#### **The Diversity in** *Neurodiversity*\n\nAlthough the people for whom the neurodiversity movement advocates far exceed autistic people, they also fall outside the main scope of the book. Some contributors' topics do include campaigns directly affecting people with other disabilities, such as that to close the tortuous Judge Rotenberg Center in the U.S. (Neumeier and Brown, Chapter 14) and to pass the Autism/Neurodiversity Manifesto in the U.K. (Craine, Chapter 19), yet the movement remains led by autistic people. Mainly though, the scope of the movement remains unclear; at a disability studies conference I asked participants how they felt about minimum criteria for eligibility within it, but they felt uncomfortable posing limits [3]. A woman suggested her multiple sclerosis should qualify; indeed, coverage of people with not only chronic illnesses but also primary sensory disabilities like blindness and psychiatric conditions like schizophrenia remain unclear [4]. One issue may be the importance of the cure issue to the movement; for example, an autistic neurodiversity activist advocates for acceptance for autism but a cure for epilepsy (which she sees as separate from her sense of self and understands as potentially fatal). Such neurological conditions fall within the broader disability rights movement and deserve basic rights accommodated, such as, arguably, policy to ban flash photography in public places that could trigger seizures in people with photosensitive epilepsy [5].",
    "is_useful": true,
    "question": "What are the complexities surrounding the eligibility criteria for the neurodiversity movement and its relationship with broader disability rights?"
  },
  {
    "text": "(Craine, Chapter 19), yet the movement remains led by autistic people. Mainly though, the scope of the movement remains unclear; at a disability studies conference I asked participants how they felt about minimum criteria for eligibility within it, but they felt uncomfortable posing limits [3]. A woman suggested her multiple sclerosis should qualify; indeed, coverage of people with not only chronic illnesses but also primary sensory disabilities like blindness and psychiatric conditions like schizophrenia remain unclear [4]. One issue may be the importance of the cure issue to the movement; for example, an autistic neurodiversity activist advocates for acceptance for autism but a cure for epilepsy (which she sees as separate from her sense of self and understands as potentially fatal). Such neurological conditions fall within the broader disability rights movement and deserve basic rights accommodated, such as, arguably, policy to ban flash photography in public places that could trigger seizures in people with photosensitive epilepsy [5]. The primacy of biology to the movement seems clear due to the *neuro*- in *neurodiversity*, and debates as to whether relevant neurodivergences must be neurodevelopmental or can be acquired environmentally or in adulthood have taken place in the U.K. [6]. Conditions such as schizophrenia fall within another identity-based socio-political movement (the mad pride movement, and while the neurodiversity movement may help provide a bridge to the disability rights movement, many adherents do not view themselves as disabled [7].",
    "is_useful": true,
    "question": "What is a central issue regarding the scope and inclusivity of the neurodiversity movement in relation to different types of disabilities?"
  },
  {
    "text": "Such neurological conditions fall within the broader disability rights movement and deserve basic rights accommodated, such as, arguably, policy to ban flash photography in public places that could trigger seizures in people with photosensitive epilepsy [5]. The primacy of biology to the movement seems clear due to the *neuro*- in *neurodiversity*, and debates as to whether relevant neurodivergences must be neurodevelopmental or can be acquired environmentally or in adulthood have taken place in the U.K. [6]. Conditions such as schizophrenia fall within another identity-based socio-political movement (the mad pride movement, and while the neurodiversity movement may help provide a bridge to the disability rights movement, many adherents do not view themselves as disabled [7]. More importantly and practically, campaigns to attribute these conditions to the brain have backfired, likely because the public often associates them with violence and thinks brain-based conditions are more difficult to treat [8, 9]. Ultimately, book contributors did not exclude any particular conditions from the domain of the movement, and the right to self-determination offers the opportunity for other people to identify and organize within the movement.\n\nWhile some activists say *neurodiversity* refers simply to a biological fact of this variance as opposed to the movement [10, 11], contributors to this volume\u2014as aware autistics do generally: see Preface\u2014suggest the term implicitly refers to a tenet of inclusion based on universal rights principles, with an emphasis on those with neurological disabilities.",
    "is_useful": true,
    "question": "What is the significance of neurodiversity in relation to the disability rights movement?"
  },
  {
    "text": "More importantly and practically, campaigns to attribute these conditions to the brain have backfired, likely because the public often associates them with violence and thinks brain-based conditions are more difficult to treat [8, 9]. Ultimately, book contributors did not exclude any particular conditions from the domain of the movement, and the right to self-determination offers the opportunity for other people to identify and organize within the movement.\n\nWhile some activists say *neurodiversity* refers simply to a biological fact of this variance as opposed to the movement [10, 11], contributors to this volume\u2014as aware autistics do generally: see Preface\u2014suggest the term implicitly refers to a tenet of inclusion based on universal rights principles, with an emphasis on those with neurological disabilities. This includes aspirations of full inclusion in education, employment, and housing; freedom from abuse (e.g. abolition of seclusion and both chemical\u2014that is, overmedication to control behavior\u2014and physical restraint); and the right to make one's own decisions with support as needed. Contributors evoke \"the compassionate, inclusive flavor of the word\" (Seidel, Chapter 7) and \"human rights concept\" (Greenberg, Chapter 12): \"the specific premise of neurodiversity is full and equal inclusion\u2026Neurodiversity is for everyone\" (daVanport, Chapter 11).",
    "is_useful": true,
    "question": "What principles does the concept of neurodiversity emphasize regarding inclusion and rights for individuals with neurological disabilities?"
  },
  {
    "text": "While some activists say *neurodiversity* refers simply to a biological fact of this variance as opposed to the movement [10, 11], contributors to this volume\u2014as aware autistics do generally: see Preface\u2014suggest the term implicitly refers to a tenet of inclusion based on universal rights principles, with an emphasis on those with neurological disabilities. This includes aspirations of full inclusion in education, employment, and housing; freedom from abuse (e.g. abolition of seclusion and both chemical\u2014that is, overmedication to control behavior\u2014and physical restraint); and the right to make one's own decisions with support as needed. Contributors evoke \"the compassionate, inclusive flavor of the word\" (Seidel, Chapter 7) and \"human rights concept\" (Greenberg, Chapter 12): \"the specific premise of neurodiversity is full and equal inclusion\u2026Neurodiversity is for everyone\" (daVanport, Chapter 11). Buckle (Chapter 8) clarifies that this inclusion involves interaction between diverse groups even in settings prioritized around the needs of a particular group: neurodiversity \"means having NTs [neurotypicals] in autistic space as much as it does autistics in NT space\".",
    "is_useful": true,
    "question": "What principles does the concept of neurodiversity emphasize regarding inclusion and rights for individuals with neurological disabilities?"
  },
  {
    "text": "This includes aspirations of full inclusion in education, employment, and housing; freedom from abuse (e.g. abolition of seclusion and both chemical\u2014that is, overmedication to control behavior\u2014and physical restraint); and the right to make one's own decisions with support as needed. Contributors evoke \"the compassionate, inclusive flavor of the word\" (Seidel, Chapter 7) and \"human rights concept\" (Greenberg, Chapter 12): \"the specific premise of neurodiversity is full and equal inclusion\u2026Neurodiversity is for everyone\" (daVanport, Chapter 11). Buckle (Chapter 8) clarifies that this inclusion involves interaction between diverse groups even in settings prioritized around the needs of a particular group: neurodiversity \"means having NTs [neurotypicals] in autistic space as much as it does autistics in NT space\". Raymaker (Chapter 10) explains both parts of the compound word: \"Neurodiversity, to me, means both a fabulous celebration of all kinds of individual minds, and a serious, holistic acknowledgement of the necessity of diversity in order for society to survive, thrive, and innovate\", which as Garcia (Chapter 17) states requires that society \"welcome neurodivergent people and give them the tools necessary to live a life of dignity\".",
    "is_useful": true,
    "question": "What are the key components of the neurodiversity movement regarding inclusion and the rights of individuals?"
  },
  {
    "text": "Buckle (Chapter 8) clarifies that this inclusion involves interaction between diverse groups even in settings prioritized around the needs of a particular group: neurodiversity \"means having NTs [neurotypicals] in autistic space as much as it does autistics in NT space\". Raymaker (Chapter 10) explains both parts of the compound word: \"Neurodiversity, to me, means both a fabulous celebration of all kinds of individual minds, and a serious, holistic acknowledgement of the necessity of diversity in order for society to survive, thrive, and innovate\", which as Garcia (Chapter 17) states requires that society \"welcome neurodivergent people and give them the tools necessary to live a life of dignity\". Inspired by the principles of other social justice movements, the neuro*diversity* movement recognizes intersectionality (how neurodivergent people's disadvantages are compounded by other types of social oppression) beyond cross-disability solidarity, such as race (see Giwa Onaiwu, Chapter 18), gender including gender identity (see daVanport, Chapter 11), and class (such as the call by Woods [2017] for universal basic income).\n\nLike the far-reaching concept of *diversity*, the neurodiversity movement as applied to autism functions inclusively, in that activists include nonautistic people as allies, and it accepts and fights for the full developmental spectrum of autistic people (including those with intellectual disability and no or minimal language).",
    "is_useful": true,
    "question": "What does the neurodiversity movement emphasize in terms of societal inclusion and the recognition of diverse minds?"
  },
  {
    "text": "Like the far-reaching concept of *diversity*, the neurodiversity movement as applied to autism functions inclusively, in that activists include nonautistic people as allies, and it accepts and fights for the full developmental spectrum of autistic people (including those with intellectual disability and no or minimal language). Marginalization of non-autistic people by nonautistic relative-led autism organizations catalyzed the movement (Pripas-Kapit, Chapter 2; [12]). Thus it seeks to help families with advocacy for acceptance, understanding, and support that can positively impact people across the autism spectrum and their parents [13]. Celebratory acts for parents toward autistic children such as learning to speak their child's language and even accepting autism as part of their child's identity, and ameliorative acts like parents teaching their child adaptive skills to cope in wider society, both show nearly universal support among the autism community\u2014including \"pro-cure\" parents and \"pro-acceptance\" autistic people [14], yet many of the more powerful parental organizations have behaved in dehumanizing and polarizing ways toward autistic people, such as using fear and pity as fundraising strategies and seeking an end to all autistic people regardless of their preferences (daVanport, Chapter 11). They have appropriated self-advocacy by using language such as \"families with autism\" (whereas if anyone \"has\" autism, autistic people do).",
    "is_useful": true,
    "question": "What role does inclusion play in the neurodiversity movement related to autism?"
  },
  {
    "text": "They have appropriated self-advocacy by using language such as \"families with autism\" (whereas if anyone \"has\" autism, autistic people do). They have also claimed autistic people cannot advocate for public policy affecting their children (even though some autistic activists themselves have intellectual disability, language impairment or no speech, epilepsy, gastrointestinal disorders, self-care needs such as toileting or daily living, meltdowns, etc., or their children do: [15, 16].\n\n#### The *Neuro*- in *Neurodiversity*\n\nWhile the neurodiversity movement generally views autism as natural and essentially innate, despite the inability of clinicians to identify itfrom birth, this viewpoint transcends politics despite its utility in activism. Autistic people tend to view autism as arising entirely from biological causes, with no evident influence from the movement [14]. This may occur both\n\nbecause autistic people likely cannot remember their life before autism becomes diagnosable, and because autistic people more often conceive of and describe autism from the inside, referring to internal processes such as thoughts, emotions, and sensations rather than behavior [17]. This conception of autism privileges lived experience, and complements autistic activists' arguments that underlying differences and difficulties persist despite coping mechanisms that may behaviorally \"mask\" autism, which have support from neuroscientific and other research [18]. Such a phenomenon helps autistic people counter the attack \"You're not like my child\" from parents; see the group blog We Are Like Your Child (http://wearelikeyourchild.blogspot.com/).",
    "is_useful": true,
    "question": "How does the neurodiversity movement view the origins of autism?"
  },
  {
    "text": "Autistic people tend to view autism as arising entirely from biological causes, with no evident influence from the movement [14]. This may occur both\n\nbecause autistic people likely cannot remember their life before autism becomes diagnosable, and because autistic people more often conceive of and describe autism from the inside, referring to internal processes such as thoughts, emotions, and sensations rather than behavior [17]. This conception of autism privileges lived experience, and complements autistic activists' arguments that underlying differences and difficulties persist despite coping mechanisms that may behaviorally \"mask\" autism, which have support from neuroscientific and other research [18]. Such a phenomenon helps autistic people counter the attack \"You're not like my child\" from parents; see the group blog We Are Like Your Child (http://wearelikeyourchild.blogspot.com/). It also facilitates a neurological kinship of sorts with fellow autistic people, helping us to emphasize within-group commonalities to develop a sense of community despite variability in how our behaviors present, and to argue for our rights based on what Silverman [19] calls \"biological citizenship\". An inside-out viewpoint of autism also helps advocates of neurodiversity explain adaptive reasons why autistic people engage in atypical behaviors, such as \"stimming\" (e.g. body rocking and hand flapping: Kapp et al. [20]; Schaber [21].\n\nImportantly, brain-based explanations facilitate the movement's compatibility with alliances with non-autistic parents.",
    "is_useful": true,
    "question": "How do brain-based explanations contribute to the understanding and advocacy of autism within the neurodiversity movement?"
  },
  {
    "text": "body rocking and hand flapping: Kapp et al. [20]; Schaber [21].\n\nImportantly, brain-based explanations facilitate the movement's compatibility with alliances with non-autistic parents. They reject a role in caregiving for causing autism, absolving parents of the responsibility scientists and clinicians assign(ed) to them when Freudian psychogenic theories have dominated (as they still do in France and to a lesser extent in countries such as Brazil). This may reduce parents' aversion toward listening to neurodiversity advocates describe helpful parenting practices. Many of the more successful \"therapeutic\" approaches involve educating others to respectfully understand autistic people's differences, such as teaching *responsive* caregiving tactics to parents that require them to \"learn to speak their child's language\" and communicate on their terms [13]. Researchers developed these techniques based on successful positive parenting practices in general [22]. A model that allows more for environmental contributions to autism's causation might look like parent-blaming, sparking resistance, and stifling progress. Moreover, biological explanations argue against environmental toxins as a risk factor for autism, helping to direct parents away from cottage industries based on rejected and unproven theories that offer dangerous \"treatments\" like heavy metal-injecting chelation therapy, chemical castration (Lupron therapy) bleach enemas, and vaccine avoidance (amid other expensive or at least ill-conceived \"interventions\").",
    "is_useful": true,
    "question": "How do brain-based explanations of autism influence parental involvement and attitudes toward neurodiversity advocacy?"
  },
  {
    "text": "Instead, biological explanations led by the neurodiversity movement help to raise ethical concerns about the basic scientific research that dominate autism research (such as the possibility of eugenics; see Evans, Chapter 9).\n\n#### **Interaction with the Medical Model**\n\nAlthough many claim that the neurodiversity movement simply supports the social model of disability and opposes the medical model, neurodiversity activists instead acknowledge the transaction between inherent weaknesses and the social environment [23, 24]. The social model of disability distinguishes between the core*impairments* inherent to medicalized conditions and *disability* caused by societal barriers (e.g. lack of assistive technology and physical infrastructure to enable someone with a mobility disability to move where they want to go), which for autism especially include social norms that result in misunderstandings and mistreatment [25]. One of the social model originators Mike Oliver [26] explained that he never advocated it as all-encompassing or intended it to *replace* the individual (medical) model, but to serve as an academic-political tool to help empower disabled people by emphasizing attention to the social obstacles that unite us; that it has certainly done. Yet the impairment that the model separates from disability may certainly add to any individuals' struggles. In practice this means that the neurodiversity movement begins with its goal of quality of life, which includes but surpasses adaptive functioning (e.g. self-determination and rights, well-being, social relationships and inclusion, and personal development: Robertson [27]; see also Tsatsanis et al.",
    "is_useful": true,
    "question": "What ethical concerns are raised by the neurodiversity movement regarding scientific research in autism?"
  },
  {
    "text": "lack of assistive technology and physical infrastructure to enable someone with a mobility disability to move where they want to go), which for autism especially include social norms that result in misunderstandings and mistreatment [25]. One of the social model originators Mike Oliver [26] explained that he never advocated it as all-encompassing or intended it to *replace* the individual (medical) model, but to serve as an academic-political tool to help empower disabled people by emphasizing attention to the social obstacles that unite us; that it has certainly done. Yet the impairment that the model separates from disability may certainly add to any individuals' struggles. In practice this means that the neurodiversity movement begins with its goal of quality of life, which includes but surpasses adaptive functioning (e.g. self-determination and rights, well-being, social relationships and inclusion, and personal development: Robertson [27]; see also Tsatsanis et al. [28]), and works backward from there to address the individual and social factors that *interact* to produce disability. In contrast, a \"pure\" medical model approach would assume an individual's \"symptoms\" (behaviors or traits) directly and specifically cause dysfunction or disability, and work to disrupt this linear relationship by preventing or curing the condition. Yet the disability rights movement has already helped enshrine access (e.g. reasonable accommodations) and non-discrimination into law, and medical practices have gradually changed to allow more patient and client autonomy [29]. Indeed, social and medical models have moved toward one another over time [24].",
    "is_useful": true,
    "question": "How do the social and medical models of disability differ in their approach to understanding and addressing disability?"
  },
  {
    "text": "In practice this means that the neurodiversity movement begins with its goal of quality of life, which includes but surpasses adaptive functioning (e.g. self-determination and rights, well-being, social relationships and inclusion, and personal development: Robertson [27]; see also Tsatsanis et al. [28]), and works backward from there to address the individual and social factors that *interact* to produce disability. In contrast, a \"pure\" medical model approach would assume an individual's \"symptoms\" (behaviors or traits) directly and specifically cause dysfunction or disability, and work to disrupt this linear relationship by preventing or curing the condition. Yet the disability rights movement has already helped enshrine access (e.g. reasonable accommodations) and non-discrimination into law, and medical practices have gradually changed to allow more patient and client autonomy [29]. Indeed, social and medical models have moved toward one another over time [24].\n\nThe neurodiversity movement's opposition to \"curing\" autism has produced misunderstandings, such as mistaken assumptions that it attributes all challenges to social injustices and rejects interventions to mitigate them. While the movement disagrees with certain principles, means, and goals of interventions, with those caveats, it does support therapies to help build useful skills such as language and flexibility. It opposes framing these matters in unnecessarily medical or clinical ways; arguably all interventions that have a scientific evidence base for truly helping autistic people's core functioning involve active learning (by the autistic person or others), and therefore one might describe them as \"educational\".",
    "is_useful": true,
    "question": "What does the neurodiversity movement emphasize regarding quality of life and the approach to disability compared to traditional medical models?"
  },
  {
    "text": "Yet the disability rights movement has already helped enshrine access (e.g. reasonable accommodations) and non-discrimination into law, and medical practices have gradually changed to allow more patient and client autonomy [29]. Indeed, social and medical models have moved toward one another over time [24].\n\nThe neurodiversity movement's opposition to \"curing\" autism has produced misunderstandings, such as mistaken assumptions that it attributes all challenges to social injustices and rejects interventions to mitigate them. While the movement disagrees with certain principles, means, and goals of interventions, with those caveats, it does support therapies to help build useful skills such as language and flexibility. It opposes framing these matters in unnecessarily medical or clinical ways; arguably all interventions that have a scientific evidence base for truly helping autistic people's core functioning involve active learning (by the autistic person or others), and therefore one might describe them as \"educational\". It recognizes that some behaviors associated with neurodivergences like autism can serve as strengths (such as interests), as coping mechanisms for underlying differences that can prove challenging at times (such as forms of stimming like hand flapping and body rocking, which help to self-regulate and communicate overpowering emotions, among other functions: Kapp et al. [20]; Bascom [30]), or as inherently neutral differences (such as an apparently monotone voice or a preference for solitude: Winter [31].\n\nWhile all social movements have more radical left wings, arguably the organized, politically mobilized autism rights branch of the neurodiversity movement largely practices critical yet reformist pragmatism rather than revolution.",
    "is_useful": true,
    "question": "How have social and medical models related to autism evolved in response to movements advocating for disability rights and neurodiversity?"
  },
  {
    "text": "It opposes framing these matters in unnecessarily medical or clinical ways; arguably all interventions that have a scientific evidence base for truly helping autistic people's core functioning involve active learning (by the autistic person or others), and therefore one might describe them as \"educational\". It recognizes that some behaviors associated with neurodivergences like autism can serve as strengths (such as interests), as coping mechanisms for underlying differences that can prove challenging at times (such as forms of stimming like hand flapping and body rocking, which help to self-regulate and communicate overpowering emotions, among other functions: Kapp et al. [20]; Bascom [30]), or as inherently neutral differences (such as an apparently monotone voice or a preference for solitude: Winter [31].\n\nWhile all social movements have more radical left wings, arguably the organized, politically mobilized autism rights branch of the neurodiversity movement largely practices critical yet reformist pragmatism rather than revolution. The movement in some ways supports a Western biomedical model more than autism's medical establishment and certainly more than autism's organized cure movement. For example, the neurodiversity movement's framework conceptualizes autism itself as purely biological, as opposed to resulting from dynamic genetic-environmental interplay (as the mainstream autism field believes and as most research suggests) or at least in part from toxins in the physical environment (as many \"procure\" parents and their advocacy organizations have believed).",
    "is_useful": true,
    "question": "What perspective does the neurodiversity movement promote regarding the understanding of autism and its behaviors?"
  },
  {
    "text": "The movement in some ways supports a Western biomedical model more than autism's medical establishment and certainly more than autism's organized cure movement. For example, the neurodiversity movement's framework conceptualizes autism itself as purely biological, as opposed to resulting from dynamic genetic-environmental interplay (as the mainstream autism field believes and as most research suggests) or at least in part from toxins in the physical environment (as many \"procure\" parents and their advocacy organizations have believed). Neurodiversity activists support traditional medicine for preventing and treating ill health, such as vaccines to prevent infectious diseases and (with the individual's consent) psychotropic medication to treat anxiety and depression (see Murray, Chapter 4), whereas beliefs in the likes of false and discredited vaccine-autism links have energized radical pro-cure activists, pseudoscience, and fringe medicine.\n\nNeurodiversity supporters cling essentially to autism's diagnostic criteria when challenging even mainstream critics, as we support acceptance of official autism domains of atypical communication, intense and \"special\" interests, a need for familiarity or predictability, and atypical sensory processing, yet distinguish between those core traits and co-occurring conditions we would be happy to cure such as anxiety, gastrointestinal disorders, sleep disorders, and epilepsy. We, as do all of the authors for this book and the latest revisions of autism's official diagnoses ([32]; https://icd. who.int/), generally support a unified conception of the autism spectrum.",
    "is_useful": true,
    "question": "What is the relationship between the neurodiversity movement and traditional medical practices in the context of autism?"
  },
  {
    "text": "Understanding and production of structural language now fall outside of autism's criteria (as a separate communication diagnosis), and neurodiversity activists have likewise supported efforts to expand access to language and communication but do not regard this as making someone \"less autistic\", unlike arguably most autism advocates. Autistic neurodiversity activists have defined critical autism studies not in terms of being critical of autism's existence (unlike many non-autistic thinkers outside the movement), but of the power dynamics that marginalize autistic scholars, pathologize autism, and overlook social factors that contribute to disability in autistic people [33].While we support moving to an alternative identification system that recognizes autism's nuances ([34]; Kapp and Ne'eman, Chapter 13), such as strengths that can aide or add difficulties to autistic people's lives depending on myriad factors [35], the often fractious autism community united around the need to protect autistic people's access to diagnosis because of the practical services and supports medical classification can provide. While the psychiatric and clinical establishment sharply criticized the American Psychiatric Association's Diagnostic and Statistical Manual of Mental Disorders (DSM) for *adding* and *expanding* most diagnoses (increasing medicalization of everyday problems) in its latest revision (DSM-5) or for lacking validity [36], the neurodiversity movement's leading organization the Autistic Self Advocacy Network (ASAN) worked more closely with the DSM-5 than any other in the autism community to protect access to diagnosis (Kapp and Ne'eman, Chapter 13).",
    "is_useful": true,
    "question": "What are the main goals of the neurodiversity movement in relation to autism diagnosis and access to communication? "
  },
  {
    "text": "While the psychiatric and clinical establishment sharply criticized the American Psychiatric Association's Diagnostic and Statistical Manual of Mental Disorders (DSM) for *adding* and *expanding* most diagnoses (increasing medicalization of everyday problems) in its latest revision (DSM-5) or for lacking validity [36], the neurodiversity movement's leading organization the Autistic Self Advocacy Network (ASAN) worked more closely with the DSM-5 than any other in the autism community to protect access to diagnosis (Kapp and Ne'eman, Chapter 13).\n\n#### **Self-Advocacy**\n\nThe neurodiversity movement's approach holds autistic and neurodivergent people responsible not for the origin of our problems (social barriers exacerbating biological challenges), but for leading the effort to solve them. This position\u2014responsibility for the \"offset\" but not \"onset\" of problems\u2014aligns with the compensatory model of helping and coping according to an analysis [37] of a classic theoretical paper [38]. Other identity-based social justice movements such as the civil rights movement share this approach, which Brickman and colleagues viewed as arguably superior because it encourages people to seek help (because it does not blame people for problems), yet actively exert control over their lives. Yet while they say on page 372 that the model \"allows help recipients to command the maximum possible respect from the social environment\" and enables mobilization, people oriented this way put enormous pressure on themselves to solve problems they did not create, risking distressing strain.",
    "is_useful": true,
    "question": "How does the neurodiversity movement's approach to responsibility differ from traditional views on the problems faced by autistic and neurodivergent individuals?"
  },
  {
    "text": "### **History and Introduction to Contributors**\n\nI commissioned contributors who have made significant achievements to the development or maturation of the autistic community or the neurodiversity movement. I posed the same questions to all contributors for them to consider: why and how they got involved, how they carried out their contribution, whether it has accomplished what they intended, etc. Contributors took different approaches to addressing these questions, and while I suggested a topic (originally limited to a particular action) and length, they negotiated their preferences and needs with me. I chose to prioritize content rather than style in my editing, giving substantive feedback on drafts but deemphasizing grammar and structure, especially considering contributors' wide-ranging educational and cultural backgrounds as well as communication abilities, to preserve the voices of the activists (see also Giwa Onaiwu, Chapter 18).\n\nThe chapters follow a chronological order that reveals patterns in the growth of the neurodiversity movement over time, a historical orientation that emphasizes where the movement and autism field have been most active: the U.S. and U.K. (the home of all contributors except Dekker, who lives in the Netherlands but also spends significant time in the U.K.). These countries have had exceptional roles in pioneering mother-blaming psychoanalytic child psychology that have unjustly blamed parents and sometimes removed autistic children from them, giving rise to the first autism advocacy organizations [40, 41]. Those parent-led organizations empowered both world-leading scientific research and pseudoscience to establish autism as a treatable developmental disability [42].",
    "is_useful": true,
    "question": "What role do contributors play in advancing the neurodiversity movement and autism advocacy?"
  },
  {
    "text": "The chapters follow a chronological order that reveals patterns in the growth of the neurodiversity movement over time, a historical orientation that emphasizes where the movement and autism field have been most active: the U.S. and U.K. (the home of all contributors except Dekker, who lives in the Netherlands but also spends significant time in the U.K.). These countries have had exceptional roles in pioneering mother-blaming psychoanalytic child psychology that have unjustly blamed parents and sometimes removed autistic children from them, giving rise to the first autism advocacy organizations [40, 41]. Those parent-led organizations empowered both world-leading scientific research and pseudoscience to establish autism as a treatable developmental disability [42]. Yet these nations also arguably hosted the birth of the disability rights movement (in the U.S.), the social model of disability (the U.K.), and disability studies (arguably both countries; see Waltz, 2013). Hence autistic adults had more to resist and resources at their service in these contexts, with similarities in various other anglophone countries and nations with high English fluency. Furthermore, most activities of the neurodiversity movement have taken place online, where people can participate internationally. This organizational approach to the book not only reflects not wishing to oversimplify other national and cultural contexts (e.g. Germany or Israel) with single chapters, but also the limitations of where I have lived and my social networks.",
    "is_useful": true,
    "question": "What historical factors have influenced the growth of the neurodiversity movement in the U.S. and U.K.?"
  },
  {
    "text": "Those parent-led organizations empowered both world-leading scientific research and pseudoscience to establish autism as a treatable developmental disability [42]. Yet these nations also arguably hosted the birth of the disability rights movement (in the U.S.), the social model of disability (the U.K.), and disability studies (arguably both countries; see Waltz, 2013). Hence autistic adults had more to resist and resources at their service in these contexts, with similarities in various other anglophone countries and nations with high English fluency. Furthermore, most activities of the neurodiversity movement have taken place online, where people can participate internationally. This organizational approach to the book not only reflects not wishing to oversimplify other national and cultural contexts (e.g. Germany or Israel) with single chapters, but also the limitations of where I have lived and my social networks.\n\n#### **Part I: Gaining Community**\n\nAt a time when non-autistic parents dominated autism advocacy in the early 1990s, Sinclair (Chapter 2) led the launch of the movement and delivered its pro-acceptance manifesto mainly intended for parents, \"Don't Mourn for Us\", helping autistic people gain an identity and communicate in cyberspace (ANI-L) and in person (Autreat). In 1996, Martijn Dekker's e-mail list InLv provided an inclusive, autistic-hosted space that helped spawn new ideas such as the term *neurodiversity* (Chapter 3).",
    "is_useful": true,
    "question": "What role did parent-led organizations play in the development of autism advocacy and the neurodiversity movement?"
  },
  {
    "text": "Furthermore, most activities of the neurodiversity movement have taken place online, where people can participate internationally. This organizational approach to the book not only reflects not wishing to oversimplify other national and cultural contexts (e.g. Germany or Israel) with single chapters, but also the limitations of where I have lived and my social networks.\n\n#### **Part I: Gaining Community**\n\nAt a time when non-autistic parents dominated autism advocacy in the early 1990s, Sinclair (Chapter 2) led the launch of the movement and delivered its pro-acceptance manifesto mainly intended for parents, \"Don't Mourn for Us\", helping autistic people gain an identity and communicate in cyberspace (ANI-L) and in person (Autreat). In 1996, Martijn Dekker's e-mail list InLv provided an inclusive, autistic-hosted space that helped spawn new ideas such as the term *neurodiversity* (Chapter 3). By 1998 Autistic activists demonstrated their ability to partner and ally with parents and non-autistic professionals on early campaigns they led, such as Dinah Murray's \"Autistic People Against Neuroleptic Abuse\" (Chapter 4). Laura Tisoncik's autistics.org website launched that year and gave voice to injustices such as through satire like the Institute for the Study of the Neurologically Typical (Chapter 5), yet now \"neurotypical\" has become a common descriptor for people without neurological disabilities in medical studies.",
    "is_useful": true,
    "question": "What strategies have been used by the neurodiversity movement to foster community and advocate for autistic individuals?"
  },
  {
    "text": "In 1996, Martijn Dekker's e-mail list InLv provided an inclusive, autistic-hosted space that helped spawn new ideas such as the term *neurodiversity* (Chapter 3). By 1998 Autistic activists demonstrated their ability to partner and ally with parents and non-autistic professionals on early campaigns they led, such as Dinah Murray's \"Autistic People Against Neuroleptic Abuse\" (Chapter 4). Laura Tisoncik's autistics.org website launched that year and gave voice to injustices such as through satire like the Institute for the Study of the Neurologically Typical (Chapter 5), yet now \"neurotypical\" has become a common descriptor for people without neurological disabilities in medical studies. Protest campaigns in response to specific events and initiatives have mounted, such as Mel Bagg's Getting the Truth Out website created in 2005 in response to the Autism Society of America's fear-mongering Getting the Word Out (Chapter 6), along with ongoing efforts like Autistics Speaking Day in response to Communication Shutdown and Autism Acceptance Day and Month in response to their Autism Awareness counterparts. The movement has grown to create annual events by autistic activists not in specific response to those by non-autistic people, including Autistic Pride Day launched by Amy and Gareth Nelson in 2005 and the Disability Community Day of Mourning, begun by Zoe Gross in 2012 to remember those people with disabilities murdered by family members and try to prevent future filicide.",
    "is_useful": true,
    "question": "How have autistic activists contributed to the development of neurodiversity and advocacy efforts within the disability community?"
  },
  {
    "text": "Protest campaigns in response to specific events and initiatives have mounted, such as Mel Bagg's Getting the Truth Out website created in 2005 in response to the Autism Society of America's fear-mongering Getting the Word Out (Chapter 6), along with ongoing efforts like Autistics Speaking Day in response to Communication Shutdown and Autism Acceptance Day and Month in response to their Autism Awareness counterparts. The movement has grown to create annual events by autistic activists not in specific response to those by non-autistic people, including Autistic Pride Day launched by Amy and Gareth Nelson in 2005 and the Disability Community Day of Mourning, begun by Zoe Gross in 2012 to remember those people with disabilities murdered by family members and try to prevent future filicide.\n\n#### **Part II: Getting Heard**\n\nThese activities have helped raise consciousness that the neurodiversity movement, while arising to counter the exclusion and pathologization autistic adults felt by organizations and conferences run mainly by nonautistic parents, serves to create a world where autistic and other disabled people are free to be themselves in a respectful and inclusive society. Indeed, Kathleen Seidel (Chapter 7) has hosted neurodiversity.com as a non-autistic parent, without significant protests that an autistic does not own the domain name (Chapter 7). The historic archives, posts by autistic and non-autistic guests on debates or issues, and Seidel's counters to disinformation like false, dangerous treatments for and beliefs of causes of autism have demonstrated the movement's alliance with like-minded parents and impactful commitment to science.",
    "is_useful": true,
    "question": "What impact have protest campaigns and initiatives had on the neurodiversity movement and the awareness of issues facing autistic and disabled individuals?"
  },
  {
    "text": "#### **Part II: Getting Heard**\n\nThese activities have helped raise consciousness that the neurodiversity movement, while arising to counter the exclusion and pathologization autistic adults felt by organizations and conferences run mainly by nonautistic parents, serves to create a world where autistic and other disabled people are free to be themselves in a respectful and inclusive society. Indeed, Kathleen Seidel (Chapter 7) has hosted neurodiversity.com as a non-autistic parent, without significant protests that an autistic does not own the domain name (Chapter 7). The historic archives, posts by autistic and non-autistic guests on debates or issues, and Seidel's counters to disinformation like false, dangerous treatments for and beliefs of causes of autism have demonstrated the movement's alliance with like-minded parents and impactful commitment to science.\n\nInspired by Sinclair's Autreat, Autscape (Buckle, Chapter 8) provides the longest-running ongoing example of physical \"autistic space\": an annual conference mostly by and for autistic people, which has demonstrated the possibilities and limits of inclusion. Beginning at a similar time, the Autistic Genocide Clock webpage publicized autistic people's fears of eugenics to prevent autism through the development of a genetic test for selective abortion, and its creator Meg Evans (Chapter 9) took it down early mainly because of the progress of the neurodiversity movement in changing attitudes toward acceptance.",
    "is_useful": true,
    "question": "What is the primary goal of the neurodiversity movement within society?"
  },
  {
    "text": "The historic archives, posts by autistic and non-autistic guests on debates or issues, and Seidel's counters to disinformation like false, dangerous treatments for and beliefs of causes of autism have demonstrated the movement's alliance with like-minded parents and impactful commitment to science.\n\nInspired by Sinclair's Autreat, Autscape (Buckle, Chapter 8) provides the longest-running ongoing example of physical \"autistic space\": an annual conference mostly by and for autistic people, which has demonstrated the possibilities and limits of inclusion. Beginning at a similar time, the Autistic Genocide Clock webpage publicized autistic people's fears of eugenics to prevent autism through the development of a genetic test for selective abortion, and its creator Meg Evans (Chapter 9) took it down early mainly because of the progress of the neurodiversity movement in changing attitudes toward acceptance. During the time span between the autism genocide clock being created (2005) and taken down (2011), ASAN led the movement's maturation from a sociocultural to a sociopolitical movement actively part of the disability rights coalition, organizing a protest against a cross-disability campaign that united autistic people with parents of autistic individuals and disability rights activists alike [43].\n\nThe Academic Autism Spectrum Partnership in Research and Education (AASPIRE) project has demonstrated the expertise of even lay autistic people as the leading provider of participatory autism research (Raymaker, Chapter 10), illustrating the growing reach of the neurodiversity movement, as have other developments.",
    "is_useful": true,
    "question": "What are some examples of initiatives or movements that have contributed to the evolution of autism awareness and neurodiversity?"
  },
  {
    "text": "During the time span between the autism genocide clock being created (2005) and taken down (2011), ASAN led the movement's maturation from a sociocultural to a sociopolitical movement actively part of the disability rights coalition, organizing a protest against a cross-disability campaign that united autistic people with parents of autistic individuals and disability rights activists alike [43].\n\nThe Academic Autism Spectrum Partnership in Research and Education (AASPIRE) project has demonstrated the expertise of even lay autistic people as the leading provider of participatory autism research (Raymaker, Chapter 10), illustrating the growing reach of the neurodiversity movement, as have other developments. The Autistic Women and Non-Binary Network (AWN) has provided powerful advocacy for intersectional feminism, as exemplified by its recent selection by the U.S. Library of Congress for preservation of its website, giving access to archives for current and future generations of advocates (daVanport, Chapter 11).\n\nThe Thinking Person's Guide to Autism provides a network of proneurodiversity and pro-science information hosted by autistic and nonautistic parents, providing the neurodiversity movement with an influential alliance that helps to reach the critical demographic of non-autistic parents (Greenburg and Rosa, Chapter 12). ASAN consulted on the revision of autism's diagnosis in the DSM-5, marking a historic collaboration that substantially affecting the core criteria and accompanying text to help maintain access to autism diagnoses and therefore needed supports (Kapp and Ne'eman, Chapter 13).",
    "is_useful": true,
    "question": "What role did the neurodiversity movement play in advocating for inclusive and participatory approaches to autism research and activism?"
  },
  {
    "text": "The Autistic Women and Non-Binary Network (AWN) has provided powerful advocacy for intersectional feminism, as exemplified by its recent selection by the U.S. Library of Congress for preservation of its website, giving access to archives for current and future generations of advocates (daVanport, Chapter 11).\n\nThe Thinking Person's Guide to Autism provides a network of proneurodiversity and pro-science information hosted by autistic and nonautistic parents, providing the neurodiversity movement with an influential alliance that helps to reach the critical demographic of non-autistic parents (Greenburg and Rosa, Chapter 12). ASAN consulted on the revision of autism's diagnosis in the DSM-5, marking a historic collaboration that substantially affecting the core criteria and accompanying text to help maintain access to autism diagnoses and therefore needed supports (Kapp and Ne'eman, Chapter 13).\n\nShain Neumeier and Lydia Brown (Chapter 14) have taken leading roles in activism to stop the electric use of shocks as \"treatment\", raising the profile of the issue and providing strong legal and ethical arguments that have assisted progress toward banning the tortuous practice.\n\nLarry Arnold (Chapter 15) edits *Autonomy, the Critical Journal of Interdisciplinary Autism Studies*, a journal that not only advances the cause of autistic people as editors and authors of new academic studies, but also preserves key texts of the neurodiversity movement.",
    "is_useful": true,
    "question": "What role does advocacy play in the preservation and advancement of open science, particularly in relation to neurodiversity and autism?"
  },
  {
    "text": "ASAN consulted on the revision of autism's diagnosis in the DSM-5, marking a historic collaboration that substantially affecting the core criteria and accompanying text to help maintain access to autism diagnoses and therefore needed supports (Kapp and Ne'eman, Chapter 13).\n\nShain Neumeier and Lydia Brown (Chapter 14) have taken leading roles in activism to stop the electric use of shocks as \"treatment\", raising the profile of the issue and providing strong legal and ethical arguments that have assisted progress toward banning the tortuous practice.\n\nLarry Arnold (Chapter 15) edits *Autonomy, the Critical Journal of Interdisciplinary Autism Studies*, a journal that not only advances the cause of autistic people as editors and authors of new academic studies, but also preserves key texts of the neurodiversity movement.\n\nJohn Elder Robison (Chapter 16) served as the only autistic advisor to Autism Speaks, the world's most powerful autism organization and the main enemy of the movement, and his resignation from his attempts to serve as a moderating influence contributed to reforms that have begun to soften its most contentious practices [44].\n\nA journalist based in Washington, DC has found that a story in which he \"outed\" and explained himself as a member of the autistic community has led to opportunities to explain autism and disability politics in neurodiversity-affirming ways, a warm reception that demonstrates the growing public interest in autism rights and acceptance (Garcia, Chapter 17).",
    "is_useful": true,
    "question": "What are some significant contributions of activists and editors to the discourse and rights of autistic individuals in the context of open science and neurodiversity?"
  },
  {
    "text": "Larry Arnold (Chapter 15) edits *Autonomy, the Critical Journal of Interdisciplinary Autism Studies*, a journal that not only advances the cause of autistic people as editors and authors of new academic studies, but also preserves key texts of the neurodiversity movement.\n\nJohn Elder Robison (Chapter 16) served as the only autistic advisor to Autism Speaks, the world's most powerful autism organization and the main enemy of the movement, and his resignation from his attempts to serve as a moderating influence contributed to reforms that have begun to soften its most contentious practices [44].\n\nA journalist based in Washington, DC has found that a story in which he \"outed\" and explained himself as a member of the autistic community has led to opportunities to explain autism and disability politics in neurodiversity-affirming ways, a warm reception that demonstrates the growing public interest in autism rights and acceptance (Garcia, Chapter 17).\n\nMor\u00e9nike Giwa Onaiwu (Chapter 18) describes the principles of and her experience in editing the first anthology of autistic people of color, which in part through its publication by AWN further demonstrates the neurodiversity movement's intersectional autism advocacy [45], amid the broader autism community and media that often implicitly associate autism with whiteness [46].\n\n#### **Part III: Entering the Establishment?**\n\nAt the present time in which autism acceptance continues to reach new heights, the neurodiversity movement has edged closer to the autism establishment, although the current status looks uncertain. A couple of current examples from the U.K. illustrate this point.",
    "is_useful": true,
    "question": "How has the neurodiversity movement progressed in its relationship with the autism establishment?"
  },
  {
    "text": "Mor\u00e9nike Giwa Onaiwu (Chapter 18) describes the principles of and her experience in editing the first anthology of autistic people of color, which in part through its publication by AWN further demonstrates the neurodiversity movement's intersectional autism advocacy [45], amid the broader autism community and media that often implicitly associate autism with whiteness [46].\n\n#### **Part III: Entering the Establishment?**\n\nAt the present time in which autism acceptance continues to reach new heights, the neurodiversity movement has edged closer to the autism establishment, although the current status looks uncertain. A couple of current examples from the U.K. illustrate this point. In Chapter 19, Craine tells the story of how, following endorsement of the Autism/Neurodiversity Manifesto by the Labour Party's finance minister, Neurodiversity Labour was launched in February 2019. This organization, led by people with neurodivergences such as autism, ADHD, dyslexia, and dyspraxia, fights discrimination against neurodivergent people within the Labour Party and society. In addition, the National Autistic Taskforce (Murray, Chapter 20) seeks to help implement the U.K.'s principled but hardly enforced legislation such as the Autism Act 2009, which has provisions for the needs of autistic adults. This autistic-led taskforce prioritizes minimally verbal autistic people with high support needs. It grew out of the National Autism Project, which provides access to government consultations and contacts that could help achieve its aims.",
    "is_useful": true,
    "question": "What is the significance of the neurodiversity movement in relation to autism advocacy and societal discrimination?"
  },
  {
    "text": "illustrate this point. In Chapter 19, Craine tells the story of how, following endorsement of the Autism/Neurodiversity Manifesto by the Labour Party's finance minister, Neurodiversity Labour was launched in February 2019. This organization, led by people with neurodivergences such as autism, ADHD, dyslexia, and dyspraxia, fights discrimination against neurodivergent people within the Labour Party and society. In addition, the National Autistic Taskforce (Murray, Chapter 20) seeks to help implement the U.K.'s principled but hardly enforced legislation such as the Autism Act 2009, which has provisions for the needs of autistic adults. This autistic-led taskforce prioritizes minimally verbal autistic people with high support needs. It grew out of the National Autism Project, which provides access to government consultations and contacts that could help achieve its aims. If the broader autism community, public, and levers of power attain a critical mass of understanding and support for the neurodiversity framework and movement, autistic people will lead advocacy for control of our own affairs.\n\n### **References**\n\n- 1. Hughes, J. M. F. (2016). *Nothing about us without us: Increasing neurodiversity in disability and social justice advocacy groups*. Retrieved from https:// autisticadvocacy.org/wp-content/uploads/2016/06/whitepaper-Increasing-Neurodiversity-in-Disability-and-Social-Justice-Advocacy-Groups.pdf.\n- 2.",
    "is_useful": true,
    "question": "What movement advocates for the rights and representation of neurodivergent individuals within society?"
  },
  {
    "text": "This autistic-led taskforce prioritizes minimally verbal autistic people with high support needs. It grew out of the National Autism Project, which provides access to government consultations and contacts that could help achieve its aims. If the broader autism community, public, and levers of power attain a critical mass of understanding and support for the neurodiversity framework and movement, autistic people will lead advocacy for control of our own affairs.\n\n### **References**\n\n- 1. Hughes, J. M. F. (2016). *Nothing about us without us: Increasing neurodiversity in disability and social justice advocacy groups*. Retrieved from https:// autisticadvocacy.org/wp-content/uploads/2016/06/whitepaper-Increasing-Neurodiversity-in-Disability-and-Social-Justice-Advocacy-Groups.pdf.\n- 2. Harp, B. [bev]. (2009, August 7). *Check list of neurotypical privilege: New draft* (Blog post). Retrieved from http://aspergersquare8.blogspot.com/2009/08/ checklist-of-neurotypical-privilege-new.html.\n- 3. Kapp, S. (2011). *Neurodiversity and progress for intercultural equity*. Paper presented at the meeting of the Society for Disability Studies, San Jose, CA.\n- 4. Liebowitz, C. (2016, March 4).",
    "is_useful": true,
    "question": "What is the role of autistic-led taskforces in advocating for the priorities of minimally verbal autistic individuals?"
  },
  {
    "text": "(2017). Charting relations between intersectionality theory and the neurodiversity paradigm. *Disability Studies Quarterly*, 37 (2). Retrieved from http://dsq-sds.org/article/view/5374/4647.\n- 46. Heilker, P. (2012). Autism, rhetoric, and whiteness. *Disability Studies Quarterly*, 32(4). Retrieved from http://dsq-sds.org/article/view/1756/3181.\n\n**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/ 4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and\n\nindicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright\n\n![](_page_35_Picture_6.jpeg)\n\nholder.\n\n# **Part I**\n\n**Gaining Community**\n\n!",
    "is_useful": true,
    "question": "What are the key principles of open access licensing as mentioned in relation to Creative Commons licenses?"
  },
  {
    "text": "**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/ 4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and\n\nindicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright\n\n![](_page_35_Picture_6.jpeg)\n\nholder.\n\n# **Part I**\n\n**Gaining Community**\n\n![](_page_37_Figure_0.jpeg)\n\n# 2\n\n## **Historicizing Jim Sinclair's \"Don't Mourn for Us\": A Cultural and Intellectual History of Neurodiversity's First Manifesto**\n\n**Sarah Pripas-Kapit**\n\nIn reviewing the intellectual history of neurodiversity, Jim Sinclair's 1993 essay \"Don't Mourn for Us\" stands out as almost singularly influential [1]. The essay was first published in the third-ever issue of *Our Voice*, the newsletter of Autism Network International (ANI). Sinclair, an ANI co-founder, based the essay on a presentation xe delivered at the 1993 International Conference on Autism in Toronto.",
    "is_useful": true,
    "question": "What are the key permissions granted by the Creative Commons Attribution 4.0 International License regarding the use and sharing of content?"
  },
  {
    "text": "Sinclair, an ANI co-founder, based the essay on a presentation xe delivered at the 1993 International Conference on Autism in Toronto. The essay implored parents not to mourn for their autistic child's disability, but rather to embrace their child's differences and work to meet their needs.\n\nEven nearly thirty years after its original publication, \"Don't Mourn for Us\" remains a touchstone for the neurodiversity movement, cited in both casual conversations on social media as well as more academic pieces offering cultural commentary and criticism.\n\nThe essay has served as a springboard for conversations about parental expectations in the context of an autism diagnosis. Many autistic people and parents cite the piece as leading them toward a path of self-acceptance\n\nS. Pripas-Kapit (B)\n\nBellevue, WA, USA\n\nS. K. Kapp (ed.), *Autistic Community and the Neurodiversity Movement*, https://doi.org/10.1007/978-981-13-8437-0_2\n\n[2, 3]. A few activists have critiqued Sinclair for not going far enough [4]. Conversely, some parents have criticized Sinclair for an alleged failure to understand their perspective [5, 6].\n\nAs a historian, I am less interested in arguing about the correctness of Sinclair's views here\u2014although as an autistic person and advocate for neurodiversity, I agree with them. Rather, I'd like to illuminate the historical context of \"Don't Mourn for Us.\"",
    "is_useful": true,
    "question": "What impact did the essay \"Don't Mourn for Us\" have on the neurodiversity movement and perceptions of autism?"
  },
  {
    "text": "Many autistic people and parents cite the piece as leading them toward a path of self-acceptance\n\nS. Pripas-Kapit (B)\n\nBellevue, WA, USA\n\nS. K. Kapp (ed.), *Autistic Community and the Neurodiversity Movement*, https://doi.org/10.1007/978-981-13-8437-0_2\n\n[2, 3]. A few activists have critiqued Sinclair for not going far enough [4]. Conversely, some parents have criticized Sinclair for an alleged failure to understand their perspective [5, 6].\n\nAs a historian, I am less interested in arguing about the correctness of Sinclair's views here\u2014although as an autistic person and advocate for neurodiversity, I agree with them. Rather, I'd like to illuminate the historical context of \"Don't Mourn for Us.\" This piece will explore how Sinclair's work fits into the broader history of autistic people's advocacy and public speech.\n\nIn the interest of full disclosure, I don't just come at this from the perspective of a historian. I attended the event Sinclair founded, Autreat (ANI) in 2008 and 2010. While in attendance, I briefly met Sinclair. Currently, I am the chairperson of the Association for Autistic Community (AAC). We sponsor an autistic community retreat, Autspace, that continues many of the same traditions of Autreat (which met for the last time in 2013).",
    "is_useful": true,
    "question": "How has the work referenced contributed to the advocacy and self-acceptance of autistic individuals and their communities?"
  },
  {
    "text": "As a historian, I am less interested in arguing about the correctness of Sinclair's views here\u2014although as an autistic person and advocate for neurodiversity, I agree with them. Rather, I'd like to illuminate the historical context of \"Don't Mourn for Us.\" This piece will explore how Sinclair's work fits into the broader history of autistic people's advocacy and public speech.\n\nIn the interest of full disclosure, I don't just come at this from the perspective of a historian. I attended the event Sinclair founded, Autreat (ANI) in 2008 and 2010. While in attendance, I briefly met Sinclair. Currently, I am the chairperson of the Association for Autistic Community (AAC). We sponsor an autistic community retreat, Autspace, that continues many of the same traditions of Autreat (which met for the last time in 2013). These experiences have undoubtedly shaped my perspective on \"Don't Mourn for Us\" and Sinclair's place in the neurodiversity movement, though this piece is primarily intended as a historicization of Sinclair's body of work.\n\nTo historicize \"Don't Mourn for Us,\" I will begin by looking at Sinclair's contemporaries. The mid-1980s and early 1990s saw some of the first published writings by autistic people in the English-speaking world, including the works of Temple Grandin and Donna Williams. By looking at Grandin's and Williams' writings, we can better understand the radicalism of \"Don't Mourn for Us.\"",
    "is_useful": true,
    "question": "What is the significance of historical context in understanding autistic advocacy movements?"
  },
  {
    "text": "While in attendance, I briefly met Sinclair. Currently, I am the chairperson of the Association for Autistic Community (AAC). We sponsor an autistic community retreat, Autspace, that continues many of the same traditions of Autreat (which met for the last time in 2013). These experiences have undoubtedly shaped my perspective on \"Don't Mourn for Us\" and Sinclair's place in the neurodiversity movement, though this piece is primarily intended as a historicization of Sinclair's body of work.\n\nTo historicize \"Don't Mourn for Us,\" I will begin by looking at Sinclair's contemporaries. The mid-1980s and early 1990s saw some of the first published writings by autistic people in the English-speaking world, including the works of Temple Grandin and Donna Williams. By looking at Grandin's and Williams' writings, we can better understand the radicalism of \"Don't Mourn for Us.\"\n\nWithin this context, I will then analyze Sinclair's intellectual evolution as seen through xyr public writings. Finally, I will suggest how Sinclair and \"Don't Mourn for Us\" have shaped the neurodiversity movement since 1993\u2014and how the movement has developed since.\n\n## **Autistic Writings and the Neurotypical Audience**\n\nThe first autistic people to write for a wide English-speaking audience were Temple Grandin and Donna Williams.",
    "is_useful": true,
    "question": "What impact did early autistic writers have on the development of the neurodiversity movement?"
  },
  {
    "text": "## **Autistic Writings and the Neurotypical Audience**\n\nThe first autistic people to write for a wide English-speaking audience were Temple Grandin and Donna Williams. Grandin's autobiography, *Emergence: Labeled Autistic* was first published in 1986 by Arena Press, and Williams' *Nobody, Nowhere*, was first published in Great Britain in 1991 and in the U.S. in 1992. *Nobody, Nowhere* became an international bestseller.\n\nBoth memoirs were radical in the sense that they introduced neurotypical audiences to the idea that autistic people could narrate their own experiences and had rich internal lives. Yet they were written for a neurotypical audience, and that shaped numerous aspects of the books' publication and content.\n\nGrandin, who was born in the U.S. in 1947 and raised in an affluent white family, enjoyed several privileges that many autistic people of her generation lacked. She was not, however, diagnosed as autistic until she was a teenager. Her initial diagnosis was \"brain damage,\" which likely saved her from being institutionalized as a child (Silberman [7]). However, Grandin was somewhat elusive on this point in the book's narrative. As the title suggests, *Emergence* presents the narrative that Grandin was able to \"emerge\" (or recover) from autism.\n\nAlthough this is not made explicit in the book's text, Margaret M. Scariano was listed as a co-author of *Emergence: Labeled Autistic*.",
    "is_useful": true,
    "question": "What impact did early autistic autobiographies have on the understanding of autistic experiences by neurotypical audiences?"
  },
  {
    "text": "Rimland, who later became an advocate of dubious biomedical \"treatments\" for autism, gushed about Grandin's ongoing recovery in the introduction. The memoir's framing hence implicitly became something of a how-to guide for autism recovery.\n\nIn addition to Rimland's introduction, the book also includes a preface from William Carlock, who taught Grandin at a private school as an adolescent. These two introductory pieces served the function of \"proving\" Grandin's autistic status to a skeptical audience, while simultaneously suggesting that recovery from autism was both possible and desirable.\n\nGrandin herself suggested this narrative in the text. *Emergence* is rife with descriptions of her autistic differences, including sensory sensitivities, communication differences, and other autistic traits. Grandin, a successful animal behavior scientist, used scientific terminology to explain autistic differences. But one cannot help but be left with the impression that autism is a tragedy. She described her alleged regression into autism at the age of six months:\n\nMother, who was only nineteen when I was born, said she remembers me as a normal, healthy newborn with big blue eyes, a mass of downy brown hair, and a dimple in my chine. A quiet, 'good' baby girl named Temple.\n\nIf I could remember those first days and weeks of life, would I have known I was on a fast slide slipping into an abyss of aloneness?",
    "is_useful": true,
    "question": "How do narratives surrounding autism recovery impact public perception and the understanding of autistic traits?"
  },
  {
    "text": "Grandin herself suggested this narrative in the text. *Emergence* is rife with descriptions of her autistic differences, including sensory sensitivities, communication differences, and other autistic traits. Grandin, a successful animal behavior scientist, used scientific terminology to explain autistic differences. But one cannot help but be left with the impression that autism is a tragedy. She described her alleged regression into autism at the age of six months:\n\nMother, who was only nineteen when I was born, said she remembers me as a normal, healthy newborn with big blue eyes, a mass of downy brown hair, and a dimple in my chine. A quiet, 'good' baby girl named Temple.\n\nIf I could remember those first days and weeks of life, would I have known I was on a fast slide slipping into an abyss of aloneness? Cut off by overreactions or inconsistent reactions from my five senses?Would I have sensed the alienation I would experience because of brain damage suffered as an unborn child\u2014the brain damage that would become apparent in life when that part of the damaged brain matured? (Grandin 2005)\n\nSignificantly, Grandin did not actually remember any of the events recounted here, since they purportedly occurred when she was a mere six months old. Rather, she created this narrative using her mother's memories and a paradigm of autism in which autism entraps autistic people into a world of isolation and misery.",
    "is_useful": true,
    "question": "How does the text portray the experiences of individuals with autism, particularly regarding their sensory sensitivities and communication differences?"
  },
  {
    "text": "Williams, who was Australian, was not diagnosed with autism as a child. Like many other autistic people who grew up in the 1960s and 1970s, she received alternative diagnoses, including psychosis. Williams discovered that she was autistic as an adult, after going through many years of familial abuse, homelessness, and domestic abuse in relationships with men. Her lyrical prose gives the book a very different reading experience than *Emergence*.\n\nYet despite these important differences, there are several similarities between the works. *Nobody, Nowhere* also began with a forward from Bernard Rimland, which echoed many of the same themes as the *Emergence* introduction. Rimland praised the book for providing inside insights into autism, which he valued as a researcher and as a parent. He explained, \"Much of what Donna Williams has written about the experience of autism was already familiar to me\u2014at an intellectual level. But *Nobody, Nowhere* provides a heretofore unavailable\u2014and alarming\u2014highly subjective appreciation of what it's like to be autistic\" [10]. According to such non-autistic \"experts,\" autistic people's internal experiences were inherently alarming.\n\nA second introduction, written by Australian psychologist Lawrence Bartak, also appeared in *Nobody, Nowhere*. Bartak discussed autism from a clinical perspective at length. As with *Emergence*, the multiple introductions essentially suggested that autistic people can't be fully trusted to narrate autism. Their words must first be contextualized by non-autistic \"experts\" who can attest to the narrative's authenticity.",
    "is_useful": true,
    "question": "What challenges do autistic individuals face in having their personal narratives recognized and validated in the field of autism research?"
  },
  {
    "text": "Rimland praised the book for providing inside insights into autism, which he valued as a researcher and as a parent. He explained, \"Much of what Donna Williams has written about the experience of autism was already familiar to me\u2014at an intellectual level. But *Nobody, Nowhere* provides a heretofore unavailable\u2014and alarming\u2014highly subjective appreciation of what it's like to be autistic\" [10]. According to such non-autistic \"experts,\" autistic people's internal experiences were inherently alarming.\n\nA second introduction, written by Australian psychologist Lawrence Bartak, also appeared in *Nobody, Nowhere*. Bartak discussed autism from a clinical perspective at length. As with *Emergence*, the multiple introductions essentially suggested that autistic people can't be fully trusted to narrate autism. Their words must first be contextualized by non-autistic \"experts\" who can attest to the narrative's authenticity.\n\nThis is not to say, however, that autism is presented identically in the two narratives. While *Emergence* suggested that autism trapped Grandin in an unpleasant world of isolation, Williams admitted that she enjoyed being \"in her own world\" at times.\n\nAlthough many autistic people have since come to interrogate the notion of autistic people as being trapped in their own worlds, at the time of *Nobody, Nowhere*'s publication it remained a dominant paradigm.",
    "is_useful": true,
    "question": "How does the perception of autistic individuals' experiences differ from that of non-autistic 'experts' according to the narratives presented in works like *Nobody, Nowhere*?"
  },
  {
    "text": "Although many autistic people have since come to interrogate the notion of autistic people as being trapped in their own worlds, at the time of *Nobody, Nowhere*'s publication it remained a dominant paradigm. Williams utilizes the paradigm in many interesting ways, writing:\n\nEverything I did, from holding two fingers together to scrunching up my toes, had a meaning, usually to do with reassuring myself that I was in control and no one could reach me, wherever the hell I was. Sometimes it had to do with telling people how I felt, but it was so subtle it was often unnoticed or simply taken to be some new quirk that 'mad Donna' had thought up. [10]\n\nHence,Williams showed that her autistic chances\u2014even ones that were thought of as \"mad Donna\"\u2014served a meaningful purpose for her. In this way she anticipated many of the ideas of the neurodiversity movement, including the popular notion that \"behavior is communication.\"\n\nHowever, *Nobody, Nowhere* hardly rejected the autism-as-tragedy paradigm in its entirety. Williams explained how she found the world as so hostile as a child that she created two personas to help her, Carole and Willie. She explained:\n\nI had created an ego detached from the self, which was still trapped by crippled emotions. It became more than an act. It became my life, and as I had to reject all acknowledgment of an emotional self, I had to reject all acknowledgment of Donna. I eventually lost Donna and became trapped in a new way.",
    "is_useful": true,
    "question": "How does the notion of behavior as communication relate to the experiences of individuals within the neurodiversity movement?"
  },
  {
    "text": "Sinclair, Williams, and other autistic people who attended non-autistic-run conferences found them to be inaccessible, prohibitively expensive, and sometimes downright dehumanizing.\n\nThere were a few exceptions. In 1992, Sinclair wrote very positively of a TEACCH conference xe attended in 1989 [12]. It speaks to the paucity of such events at the time that Sinclair drove 1200 miles to attend the conference. The essay, entitled, \"Bridging the Gaps: An Inside-Out View of Autism (Or, Do You Know What I Don't Know?),\" was published in a TEACCH anthology that also included pieces from non-autistic experts Lorna Wing and Catherine Lord [7, 12].\n\nThis 1992 piece\u2014cited much less frequently than \"Don't Mourn for Us\"\u2014provides interesting glimpses into Sinclair's intellectual evolution. More so than any of Sinclair's subsequent writings, this essay included discussion of Sinclair's personal experiences and autism-related impairments. As the parenthetical part of the title suggests, Sinclair presented the autistic experience largely as an experience of not knowing. This not-knowing experience encompassed both not knowing the norms of the neurotypical world and not knowing about one's own autistic differences.\n\nSinclair reflected on xyr own experiences as an autistic child who grew up in the 1960s and 1970s. They explained:\n\nI've been living with autism for 27 years. But I'm just beginning to learn about what that means.",
    "is_useful": true,
    "question": "What challenges do autistic individuals face when attending conferences typically organized by non-autistic people?"
  },
  {
    "text": "),\" was published in a TEACCH anthology that also included pieces from non-autistic experts Lorna Wing and Catherine Lord [7, 12].\n\nThis 1992 piece\u2014cited much less frequently than \"Don't Mourn for Us\"\u2014provides interesting glimpses into Sinclair's intellectual evolution. More so than any of Sinclair's subsequent writings, this essay included discussion of Sinclair's personal experiences and autism-related impairments. As the parenthetical part of the title suggests, Sinclair presented the autistic experience largely as an experience of not knowing. This not-knowing experience encompassed both not knowing the norms of the neurotypical world and not knowing about one's own autistic differences.\n\nSinclair reflected on xyr own experiences as an autistic child who grew up in the 1960s and 1970s. They explained:\n\nI've been living with autism for 27 years. But I'm just beginning to learn about what that means. I grew up hearing the word but never knowing what was behind it. My parents did not attend programs to learn about autism, did not collect literature to educate schools about autism, did not explain, to me or to anyone else, why my world was not the same one that normal people live in. [12]\n\nFor Sinclair, thisfeeling of isolation and not-knowing started to dissipate upon attending autism conferences. These conferences included a small number of other autistic adults. Yet at the same time, the autism conference world introduced Sinclair to a new type of isolation: being seen as an Other by non-autistic parents and professionals.",
    "is_useful": true,
    "question": "How do autism conferences contribute to the understanding and experiences of autistic individuals?"
  },
  {
    "text": "Sinclair reflected on xyr own experiences as an autistic child who grew up in the 1960s and 1970s. They explained:\n\nI've been living with autism for 27 years. But I'm just beginning to learn about what that means. I grew up hearing the word but never knowing what was behind it. My parents did not attend programs to learn about autism, did not collect literature to educate schools about autism, did not explain, to me or to anyone else, why my world was not the same one that normal people live in. [12]\n\nFor Sinclair, thisfeeling of isolation and not-knowing started to dissipate upon attending autism conferences. These conferences included a small number of other autistic adults. Yet at the same time, the autism conference world introduced Sinclair to a new type of isolation: being seen as an Other by non-autistic parents and professionals. To them, autistic people's experiences were something to be studied under a microscope, like an unusual virus. (Indeed, one sees evidence of this attitude in Rimland's introduction to Grandin's and Williams' narratives.) Sinclair did not care for such attitudes.\n\nIn the article, Sinclair took care to dispel myths about autistic people. The article's first subheading is \"Being Autistic Does Not Mean Being Mentally Retarded,\" in a point that reads to many contemporary autistic activists as problematic in its failure to extend solidarity toward people with intellectual disabilities. Other subheadings are \"Being Autistic Does Not Mean Being Uncaring\" and \"Being Autistic Will Always Mean Being Different.\"",
    "is_useful": true,
    "question": "What challenges do autistic individuals face in relation to societal perceptions and understanding of their experiences?"
  },
  {
    "text": "These conferences included a small number of other autistic adults. Yet at the same time, the autism conference world introduced Sinclair to a new type of isolation: being seen as an Other by non-autistic parents and professionals. To them, autistic people's experiences were something to be studied under a microscope, like an unusual virus. (Indeed, one sees evidence of this attitude in Rimland's introduction to Grandin's and Williams' narratives.) Sinclair did not care for such attitudes.\n\nIn the article, Sinclair took care to dispel myths about autistic people. The article's first subheading is \"Being Autistic Does Not Mean Being Mentally Retarded,\" in a point that reads to many contemporary autistic activists as problematic in its failure to extend solidarity toward people with intellectual disabilities. Other subheadings are \"Being Autistic Does Not Mean Being Uncaring\" and \"Being Autistic Will Always Mean Being Different.\"\n\nBut being different was not bad to Sinclair. \"Bridging the Gaps\" included several hints at the neurodiversity ideology that Sinclair would later articulate more fully. Xe started to articulate the idea that autistic people's impairments largely stemmed from societal factors, not inherent deficits.\n\nSinclair pointed to non-autistic people's assumptions as a key factor that limited autistic people. Xe criticized the special education field for being particularly unwilling to extend understanding toward autistics. Xe wrote:\n\nNot all the gaps are caused by my failure to share other people's unthinking assumptions. Other people's failure to question their assumptions creates at least as many barriers to understanding.",
    "is_useful": true,
    "question": "What are the common misconceptions about autistic individuals that are highlighted in discussions of neurodiversity?"
  },
  {
    "text": "Xe wrote:\n\nNot all the gaps are caused by my failure to share other people's unthinking assumptions. Other people's failure to question their assumptions creates at least as many barriers to understanding. The most damaging assumptions, the causes of the most painful misunderstandings, are the same now as they were when I was a child who couldn't talk, a teenager who couldn't drive, and a college student who couldn't get a job: assumptions that I understand what is expected of me, that I know how to do it, and that I fail to perform as expected out of deliberate spite or unconscious hostility.\n\nOther people's assumptions are usually much more resistant to learning than my ignorance. As a graduate student I encountered these assumptions in employers who had extensive backgrounds in special education. [12]\n\nSinclair did not specifically reference the social model of disability (which was much less well-known in 1992 than it is today, even in disability circles). However, xe did suggest a view of autism congruent with the social model.\n\nThese ideas would take a more fully realized view in \"Don't Mourn for Us\" one year later. In this piece, which has been referred to as a manifesto for the neurodiversity movement, Sinclair did not blunt xyr criticisms of parents. Xe focused on the parental tendency to \"mourn\" a child's autistic status. Sinclair stated,\n\nParents often report that learning their child is autistic was the most traumatic thing that ever happened to them.",
    "is_useful": true,
    "question": "What are the challenges posed by assumptions in understanding and integrating individuals with disabilities in educational and employment contexts?"
  },
  {
    "text": "Other people's assumptions are usually much more resistant to learning than my ignorance. As a graduate student I encountered these assumptions in employers who had extensive backgrounds in special education. [12]\n\nSinclair did not specifically reference the social model of disability (which was much less well-known in 1992 than it is today, even in disability circles). However, xe did suggest a view of autism congruent with the social model.\n\nThese ideas would take a more fully realized view in \"Don't Mourn for Us\" one year later. In this piece, which has been referred to as a manifesto for the neurodiversity movement, Sinclair did not blunt xyr criticisms of parents. Xe focused on the parental tendency to \"mourn\" a child's autistic status. Sinclair stated,\n\nParents often report that learning their child is autistic was the most traumatic thing that ever happened to them. Non-autistic people see autism as a great tragedy, and parents experience continuing disappointment and grief at all stages of the child's and family's life cycle.\n\nBut this grief does not stem from the child's autism in itself. It is grief over the loss of the normal child the parents had hoped and expected to have. [1]\n\nWith this declaration, Sinclair identified the source of parental grief over having an autistic child. The fundamental cause was not the inherent tragedy of disability, but rather the pernicious cultural assumption that parents ought to have a \"normal\" child.\n\nSuch an assumption, Sinclair wrote, was damaging to both the parent and child.",
    "is_useful": true,
    "question": "What cultural assumptions can negatively impact the perception of disability and the experiences of families with autistic children?"
  },
  {
    "text": "Xe focused on the parental tendency to \"mourn\" a child's autistic status. Sinclair stated,\n\nParents often report that learning their child is autistic was the most traumatic thing that ever happened to them. Non-autistic people see autism as a great tragedy, and parents experience continuing disappointment and grief at all stages of the child's and family's life cycle.\n\nBut this grief does not stem from the child's autism in itself. It is grief over the loss of the normal child the parents had hoped and expected to have. [1]\n\nWith this declaration, Sinclair identified the source of parental grief over having an autistic child. The fundamental cause was not the inherent tragedy of disability, but rather the pernicious cultural assumption that parents ought to have a \"normal\" child.\n\nSuch an assumption, Sinclair wrote, was damaging to both the parent and child. Although xe acknowledged that \"Some amount of grief is natural as parents adjust to the fact that an event and a relationship they've been looking forward to isn't going to materialize,\" Sinclair urged parents to move beyond those feelings. Xe stated simply, \"Continuing focus on the child's autism as a source of grief is damaging for both the parents and the child, and precludes the development of an accepting and authentic relationship between them\" [1].\n\nSuch sentiments are radical even today. But when we consider the relevant historical context, they become even more so. Prior to Sinclair's declaration, the default mode for autistic people discussing autism was to focus almost exclusively on their personal experiences.",
    "is_useful": true,
    "question": "What societal assumptions contribute to parental grief when learning that their child is autistic?"
  },
  {
    "text": "The fundamental cause was not the inherent tragedy of disability, but rather the pernicious cultural assumption that parents ought to have a \"normal\" child.\n\nSuch an assumption, Sinclair wrote, was damaging to both the parent and child. Although xe acknowledged that \"Some amount of grief is natural as parents adjust to the fact that an event and a relationship they've been looking forward to isn't going to materialize,\" Sinclair urged parents to move beyond those feelings. Xe stated simply, \"Continuing focus on the child's autism as a source of grief is damaging for both the parents and the child, and precludes the development of an accepting and authentic relationship between them\" [1].\n\nSuch sentiments are radical even today. But when we consider the relevant historical context, they become even more so. Prior to Sinclair's declaration, the default mode for autistic people discussing autism was to focus almost exclusively on their personal experiences. Even if they admitted to enjoying parts of the autistic experience (as did Donna Williams), previous autistic writers always made sure to acknowledge the pain and danger that autistic people inflicted upon family members. Sinclair told parents that their feelings of grief, while very real, weren't the result of autism per se.\n\n\"Don't Mourn for Us\" also dispelled the myth of the autistic person as being in their own world, another trope that appeared prominently in Grandin and Williams' work. Sinclair explained:\n\nYou try to relate to your autistic child, and the child doesn't respond. He doesn't see you; you can't reach her; there's no getting through.",
    "is_useful": true,
    "question": "What cultural assumption regarding children with disabilities does the text suggest can be damaging to the relationship between parents and their children?"
  },
  {
    "text": "Such sentiments are radical even today. But when we consider the relevant historical context, they become even more so. Prior to Sinclair's declaration, the default mode for autistic people discussing autism was to focus almost exclusively on their personal experiences. Even if they admitted to enjoying parts of the autistic experience (as did Donna Williams), previous autistic writers always made sure to acknowledge the pain and danger that autistic people inflicted upon family members. Sinclair told parents that their feelings of grief, while very real, weren't the result of autism per se.\n\n\"Don't Mourn for Us\" also dispelled the myth of the autistic person as being in their own world, another trope that appeared prominently in Grandin and Williams' work. Sinclair explained:\n\nYou try to relate to your autistic child, and the child doesn't respond. He doesn't see you; you can't reach her; there's no getting through. That's the hardest thing to deal with, isn't it? The only thing is, it isn't true.\n\nLook at it again: You try to relate as parent to child, using your own understanding of normal children, your own feelings about parenthood, your own experiences and intuitions about relationships. And the child doesn't respond in any way you can recognize as being part of that system.\n\nThat does not mean the child is incapable of relating at all. It only means you're assuming a shared system, a shared understanding of signals and meanings, that the child in fact does not share.",
    "is_useful": true,
    "question": "How has the perception of autism and autistic individuals' communication styles evolved in relation to historical discussions about personal experiences and parental grief?"
  },
  {
    "text": "It only means you're assuming a shared system, a shared understanding of signals and meanings, that the child in fact does not share. [1]\n\nIn rejecting the \"own world\" paradigm, Sinclair also offered practical advice to struggling parents. But xe asked parents to understand their child's perspective rather than impose their own preferences and perspective on the child.\n\nAlthough \"Don't Mourn for Us\" has oftentimes been interpreted as being dismissive of parental perspectives, Sinclair explicitly acknowledged the reality of parental grief, and the array of impairments that autistic people can experience. However, xe strenuously argued that parental grief should not be directed at the child. Xe wrote, \"You didn't lose a child to autism. You lost a child because the child you waited for never came into existence. [\u2026] Grieve if you must, for your own lost dreams. But don't mourn for us. We are alive. We are real. And we're here waiting for you\" [1].\n\nThe essay also included commentary about how parent-run autism organizations could reorient themselves to better reflect autistic people's needs and priorities. Sinclair went on to suggest, \"this is what I think autism societies should be about: not mourning for what never was, but exploration of what is. We need you. We need your help and your understanding. Your world is not very open to us, and we won't make it without your strong support\" [1].",
    "is_useful": true,
    "question": "What should organizations focused on autism prioritize to better align with the needs and perspectives of autistic individuals?"
  },
  {
    "text": "Sinclair went on to suggest, \"this is what I think autism societies should be about: not mourning for what never was, but exploration of what is. We need you. We need your help and your understanding. Your world is not very open to us, and we won't make it without your strong support\" [1]. Xe hence invited parents to join autistic adults in creating a better world for autistic people\u2014but not by demanding that autistic people \"recover.\" Rather, Sinclair's vision of neurodiversity prioritized the reshaping of social expectations and norms. Non-autistic parents had a place in this movement, but it was primarily as allies to autistic adults (in the parlance of today's social justice vocabulary).\n\nBy choosing to take this radical stance, Sinclair sacrificed much. A friend of mine who conversed with xem on the subject said that Sinclair was on track to become a professional autistic speaker akin to Grandin and Stephen Shore. After taking more radical stances on autism and neurodiversity, those opportunities were no longer open. For a time, Sinclair was homeless. Xe never found a full-time job in xyr chosen profession as a rehabilitation counselor despite obvious knowledge and qualifications.\n\nYet Sinclair's sacrifices have borne considerable fruit. Although much has changed since the essay's original publication, the core idea articulated in \"Don'tMournfor Us\" has continued to animate neurodiversity activism. Sinclair changed the paradigm with which autistic adults would approach public speech.",
    "is_useful": true,
    "question": "How has the concept of neurodiversity influenced the advocacy for the rights and recognition of autistic individuals?"
  },
  {
    "text": "Non-autistic parents had a place in this movement, but it was primarily as allies to autistic adults (in the parlance of today's social justice vocabulary).\n\nBy choosing to take this radical stance, Sinclair sacrificed much. A friend of mine who conversed with xem on the subject said that Sinclair was on track to become a professional autistic speaker akin to Grandin and Stephen Shore. After taking more radical stances on autism and neurodiversity, those opportunities were no longer open. For a time, Sinclair was homeless. Xe never found a full-time job in xyr chosen profession as a rehabilitation counselor despite obvious knowledge and qualifications.\n\nYet Sinclair's sacrifices have borne considerable fruit. Although much has changed since the essay's original publication, the core idea articulated in \"Don'tMournfor Us\" has continued to animate neurodiversity activism. Sinclair changed the paradigm with which autistic adults would approach public speech. No longer were autistic people limited to personal narratives that relied heavily on tropes of autism as tragedy or entrapment. Autistic people could\u2014and would\u2014articulate their own views independent of parent and professional validation. That is the legacy of \"Don't Mourn for Us.\"\n\n#### **Sinclair, Autspace, and the Development of Autistic Culture**\n\nAutistic culture as it exists today would be very different if not for the considerable contributions of Jim Sinclair. However, autistic culture and the philosophy of neurodiversity have undergone substantial shifts since 1993.",
    "is_useful": true,
    "question": "How has the philosophy of neurodiversity impacted the way autistic individuals express their views and participate in public discourse?"
  },
  {
    "text": "Yet Sinclair's sacrifices have borne considerable fruit. Although much has changed since the essay's original publication, the core idea articulated in \"Don'tMournfor Us\" has continued to animate neurodiversity activism. Sinclair changed the paradigm with which autistic adults would approach public speech. No longer were autistic people limited to personal narratives that relied heavily on tropes of autism as tragedy or entrapment. Autistic people could\u2014and would\u2014articulate their own views independent of parent and professional validation. That is the legacy of \"Don't Mourn for Us.\"\n\n#### **Sinclair, Autspace, and the Development of Autistic Culture**\n\nAutistic culture as it exists today would be very different if not for the considerable contributions of Jim Sinclair. However, autistic culture and the philosophy of neurodiversity have undergone substantial shifts since 1993.\n\nOne of the most notable features of Sinclair's early work is the extent to which it began as a response to autism parent and professional culture. In some ways, this is seen even in Autreat, which Sinclair designed to be an autistic space that prioritized autistic needs [11].\n\nTake, for example, Autreat's famous \"Ask a Neurotypical\" panel. According to Sinclair's description at one of the Autreats I attended, the idea for the panel originated as a parody of sorts. Sinclair disdained the \"ask an autistic\" panels frequently found at conferences for parents and participants. Xe had participated in many such panels, in which autistic panelists were asked entirely inappropriate questions such as \"do you have sex?\"",
    "is_useful": true,
    "question": "What impact did Jim Sinclair's work have on the representation and self-advocacy of autistic individuals within the context of open science and neurodiversity?"
  },
  {
    "text": "However, autistic culture and the philosophy of neurodiversity have undergone substantial shifts since 1993.\n\nOne of the most notable features of Sinclair's early work is the extent to which it began as a response to autism parent and professional culture. In some ways, this is seen even in Autreat, which Sinclair designed to be an autistic space that prioritized autistic needs [11].\n\nTake, for example, Autreat's famous \"Ask a Neurotypical\" panel. According to Sinclair's description at one of the Autreats I attended, the idea for the panel originated as a parody of sorts. Sinclair disdained the \"ask an autistic\" panels frequently found at conferences for parents and participants. Xe had participated in many such panels, in which autistic panelists were asked entirely inappropriate questions such as \"do you have sex?\"\n\nXyr idea was to turn the tables. At the \"Ask an NT\" panel, autistic audience members would ask non-autistic panelists the same sorts of questions. This subversive idea, focused as it was on flipping the script, was fairly characteristic of the approach Sinclair took in \"Don't Mourn for Us\" and throughout xyr other works.\n\nHowever, xe ran into a problem when trying to implement this plan. The autistic attendees at Autreat felt that the idea was unethical, premised as it was on asking people invasive questions in public without advance warning.",
    "is_useful": true,
    "question": "What cultural shift regarding communication and understanding between autistic individuals and non-autistic individuals is evident in recent developments within autistic community events?"
  },
  {
    "text": "According to Sinclair's description at one of the Autreats I attended, the idea for the panel originated as a parody of sorts. Sinclair disdained the \"ask an autistic\" panels frequently found at conferences for parents and participants. Xe had participated in many such panels, in which autistic panelists were asked entirely inappropriate questions such as \"do you have sex?\"\n\nXyr idea was to turn the tables. At the \"Ask an NT\" panel, autistic audience members would ask non-autistic panelists the same sorts of questions. This subversive idea, focused as it was on flipping the script, was fairly characteristic of the approach Sinclair took in \"Don't Mourn for Us\" and throughout xyr other works.\n\nHowever, xe ran into a problem when trying to implement this plan. The autistic attendees at Autreat felt that the idea was unethical, premised as it was on asking people invasive questions in public without advance warning. So Sinclair scrapped the idea and the \"Ask an NT\" panel turned into something very different\u2014an opportunity for autistic adults to learn more about non-autistic perspectives in a non-judgmental environment. (In one example of this dialogue, Sinclair asked panelists why neurotypicals enjoy eating at restaurants. Aside from the obvious pleasures of someone else cooking food for you, Sinclair asked, why bother with it?)\n\nThe evolution of the \"Ask an NT\" panel is in some ways emblematic of autistic culture's historical trajectory. Although it originated as a response to parents and professionals, it has since grown and mutated to develop its own traditions and community norms.",
    "is_useful": true,
    "question": "How has the approach to discussions about autism evolved within autistic culture, particularly in the context of audience engagement with non-autistic perspectives?"
  },
  {
    "text": "The evolution of the \"Ask an NT\" panel is in some ways emblematic of autistic culture's historical trajectory. Although it originated as a response to parents and professionals, it has since grown and mutated to develop its own traditions and community norms. Certainly Sinclair played a major role in the development of autistic culture, but always in dialogue with other autistic people.\n\nSinclair and ANI, the organization xe co-created, did not directly engage in policy advocacy. Yet the philosophy xe established would form the foundations of today's autistic-led policy advocacy work. Ari Ne'eman, cofounder of the Autistic Self-Advocacy Network (ASAN), explained it to me this way: \"I never would've founded ASAN if not for Jim. ASAN might have popularized neurodiversity, but Jim Sinclair created it\" (personal communication, February 20, 2019).\n\nIt is this fundamental idea that is the greatest legacy of \"Don't Mourn for Us.\" Popular autism narratives of the 1980s and early 1990s suggested that autistic people were primarily useful for our ability to provide \"inside insights\" into the autistic experience. Sinclair transformed the paradigm by suggesting that autistic people could articulate a larger vision for social change. And we could do so without capitulating to the notion that autism was inherently tragic.\n\nSinclair's intellectual legacy extends well beyond \"Don't Mourn for Us.\" Xe was likely the first autistic person to reject person-first language, in a 1999 essay \"Why I Dislike Person First Language\" [13].",
    "is_useful": true,
    "question": "What is the significance of the paradigm shift introduced by Sinclair in the context of autistic culture and advocacy?"
  },
  {
    "text": "ASAN might have popularized neurodiversity, but Jim Sinclair created it\" (personal communication, February 20, 2019).\n\nIt is this fundamental idea that is the greatest legacy of \"Don't Mourn for Us.\" Popular autism narratives of the 1980s and early 1990s suggested that autistic people were primarily useful for our ability to provide \"inside insights\" into the autistic experience. Sinclair transformed the paradigm by suggesting that autistic people could articulate a larger vision for social change. And we could do so without capitulating to the notion that autism was inherently tragic.\n\nSinclair's intellectual legacy extends well beyond \"Don't Mourn for Us.\" Xe was likely the first autistic person to reject person-first language, in a 1999 essay \"Why I Dislike Person First Language\" [13]. Xe also coined the term \"self-narrating zoo exhibit,\" which described the tendency of non-autistic parents and professionals to solicit personal narratives\u2014like Grandin's and Williams'\u2014that treated autistic people as peculiar curiosities. In all of xyr work, Sinclair was uncompromising in xyr willingness to question dominant narratives of autism as created by both non-autistic experts and less radical autistic representatives\u2014the ones who were more likely to get conference invitations and book contracts.\n\nGiven Sinclair's emphasis on questioning all received wisdom, there is a certain irony to the now canonical status of \"Don't Mourn for Us.\"",
    "is_useful": true,
    "question": "What fundamental idea transformed the narrative surrounding autism, suggesting that autistic individuals could envision social change rather than being seen merely as sources of insight?"
  },
  {
    "text": "In all of xyr work, Sinclair was uncompromising in xyr willingness to question dominant narratives of autism as created by both non-autistic experts and less radical autistic representatives\u2014the ones who were more likely to get conference invitations and book contracts.\n\nGiven Sinclair's emphasis on questioning all received wisdom, there is a certain irony to the now canonical status of \"Don't Mourn for Us.\" The piece is certainly deserving of such status, but I believe Sinclair would be the first to admit that it was by no means intended as the final word on neurodiversity as a philosophy. It's particularly important to note that Sinclair's early work was shaped heavily by parent- and professional-dominated autism culture\u2014a necessary move at the time xe first wrote the essay. Fortunately, we have now reached a point where it is possible to start creating more of our own cultural and intellectual traditions\u2014a process which Sinclair began. Moving forward, I'd propose that future generations of autistics embrace the spirit of Sinclair's work by continuing to question, to challenge, and to move forward with new and innovative ideas.\n\n#### **References**\n\n- 1. Sinclair, J. (1993). Don't mourn for us. *Our Voice, 1*(3). Retrieved from http://www.autreat.com/dont_mourn.html.\n- 2. Darroch, G. (2008, July 30). *Grief* (Web log post). Retrieved February 21, 2019 from http://autisticdad.blogspot.com/2008/07.",
    "is_useful": true,
    "question": "How can future generations contribute to the evolution of neurodiversity as a philosophy?"
  },
  {
    "text": "Sinclair, J. (2010). Being autistic together. *Disability Studies Quarterly*, 30(1). Retrieved from http://www.dsq-sds.org/article/view/1075/1248.\n- 12. Sinclair, J. (1992). Bridging the gaps: An inside-out view of autism (or, do you know what I don't know?). In E. Schopler & G. B. Mesibov (Eds.), *Highfunctioning individuals with autism.* New York: Plenum Press. Retrieved from http://jisincla.mysite.syr.edu/bridgingnc.htm.\n- 13. Sinclair, J. (1999). *Why I dislike person first language*. Retrieved from http://www.larry-arnold.net/Autonomy/index.php/autonomy/article/ view/OP1/html_1.\n\n**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/ 4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.\n\nThe images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material.",
    "is_useful": true,
    "question": "What is the significance of the Creative Commons Attribution 4.0 International License in relation to open access and the use of published works?"
  },
  {
    "text": "Retrieved from http://jisincla.mysite.syr.edu/bridgingnc.htm.\n- 13. Sinclair, J. (1999). *Why I dislike person first language*. Retrieved from http://www.larry-arnold.net/Autonomy/index.php/autonomy/article/ view/OP1/html_1.\n\n**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/ 4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.\n\nThe images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\n![](_page_53_Picture_3.jpeg)\n\n!",
    "is_useful": true,
    "question": "What is the primary licensing requirement for materials published under the Creative Commons Attribution 4.0 International License?"
  },
  {
    "text": "Retrieved from http://www.larry-arnold.net/Autonomy/index.php/autonomy/article/ view/OP1/html_1.\n\n**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/ 4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.\n\nThe images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\n![](_page_53_Picture_3.jpeg)\n\n![](_page_54_Figure_0.jpeg)\n\n# 3\n\n## **From Exclusion to Acceptance: Independent Living on the Autistic Spectrum**\n\n**Martijn Dekker**\n\nIn 1985, as an oblivious and undiagnosed autistic 11-year-old with no idea who I really was or what my life was for, I was introduced to the amazing and captivating world of programmable home computers, who always mean what they say and say what they mean, and expect nothing less of you. Thus I acquired my most central and enduring identity, that of a computer programmer. It was then that my real social life began.",
    "is_useful": true,
    "question": "What licensing terms facilitate the use, sharing, adaptation, and distribution of academic works in open science?"
  },
  {
    "text": "They were special in not only technically enabling, but actively encouraging their users to run server software\u2014to become a full-fledged host on the 'Net alongside giants like Syracuse University. Freeware mailing list server software called Macjordomo had also become available. Power to the people! The puzzle pieces fell into place.\n\nI offered X the position of co-moderator, but at this point he had lost interest in taking an active role. So he took a back seat as I started it as my group. As was customary, the BBS had been divided into various topicbased forums; I decided to keep that aspect by creating several mailing lists dedicated to similar topics. I began to operate these mailing lists together as a set, so members could choose their topics of interest in which to participate.\n\nThus, the first entirely self-run and self-hosted autistic community on the Internet was born in July 1996, called Independent Living (InLv), with the Internet hostname of inlv.demon.nl (later changed to inlv.org). From then on, until cable internet became available in 2000, I had a routine of actively distributing group mail over the Internet through my dial-up line in batches, a few times a day. The communication was slower than ANI-L, but still much faster than Fidonet.\n\nThe group grew quickly.While some members were non-autistic friends and sympathizers, often with other neurological conditions, most of us were autistic\u2014some recently diagnosed as adults, others seeking and receiving diagnoses as a result of their membership, still others content with having self-identified.",
    "is_useful": true,
    "question": "What factors contributed to the establishment of self-run communities on the Internet?"
  },
  {
    "text": "I began to operate these mailing lists together as a set, so members could choose their topics of interest in which to participate.\n\nThus, the first entirely self-run and self-hosted autistic community on the Internet was born in July 1996, called Independent Living (InLv), with the Internet hostname of inlv.demon.nl (later changed to inlv.org). From then on, until cable internet became available in 2000, I had a routine of actively distributing group mail over the Internet through my dial-up line in batches, a few times a day. The communication was slower than ANI-L, but still much faster than Fidonet.\n\nThe group grew quickly.While some members were non-autistic friends and sympathizers, often with other neurological conditions, most of us were autistic\u2014some recently diagnosed as adults, others seeking and receiving diagnoses as a result of their membership, still others content with having self-identified. In finding each other, we found ourselves. The collective process of self-rediscovery as autistic people that we went through as members was so intense, I stopped engaging in computer programming altogether for a number of years. I made friends, and more friends through friends, in various countries. Thus, after that of a computer programmer, I acquired my second-most central and enduring identity: that of an autistic person. It was then that I felt truly accepted in a community for the first time.\n\nThe text-only email nature of our community, far from being limiting or disabling, was found to be an advantage.",
    "is_useful": true,
    "question": "What impact can self-hosted online communities have on individuals' identities and sense of belonging?"
  },
  {
    "text": "The communication was slower than ANI-L, but still much faster than Fidonet.\n\nThe group grew quickly.While some members were non-autistic friends and sympathizers, often with other neurological conditions, most of us were autistic\u2014some recently diagnosed as adults, others seeking and receiving diagnoses as a result of their membership, still others content with having self-identified. In finding each other, we found ourselves. The collective process of self-rediscovery as autistic people that we went through as members was so intense, I stopped engaging in computer programming altogether for a number of years. I made friends, and more friends through friends, in various countries. Thus, after that of a computer programmer, I acquired my second-most central and enduring identity: that of an autistic person. It was then that I felt truly accepted in a community for the first time.\n\nThe text-only email nature of our community, far from being limiting or disabling, was found to be an advantage. We were able to skip all the social rituals and awkwardness and cut right to the chase, undistracted by body language, timing, sensory or eye contact issues, or any of the other autism-related difficulties with socializing. In the words of one member, the Internet was where \"people can see the real me, not just how I interact superficially with other people\" [4]. This helped us support each other more effectively. Email seemed like a natural communication medium for us autistics, like sign language is for deaf people [5].",
    "is_useful": true,
    "question": "How does the communication style within a community of autistic individuals enhance their ability to connect and support each other?"
  },
  {
    "text": "Based in large part on InLv discussions, he wrote the first book about the condition, which he published online [7]. The book gained traction and spread knowledge on the condition, and the term is now widely used.\n\nMeanwhile, as people in and (mostly) out of the Netherlands began to take note of my activities, I began to be invited to autism conferences as a speaker. I would warn the InLv members of the impending silence when I was about to travel, then upon arrival I would find places to plug my laptop into a phone line and distribute the backlogged mail.\n\nThe feeling, at once grave and uplifting, of having an entire worldwide human community inside your laptop computer, depending on your own continued action to survive, is hard to describe. Wherever I went, they went with me. As I boarded airplanes, the announcement \"in the event of an emergency, you must leave all hand luggage behind\" acquired an existential level of fearsomeness.\n\nIn July 1997, when InLv was a year old, I met X in person in Minneapolis, Minnesota, USA as the two of us were invited to speak on the new phenomenon of online autistic community and activism at a conference organized by the Society for Disability Studies. In person, he seemed much more timid than intimidating. The conference speech was a success for both of us.\n\nBut on InLv, though X continued to prefer his back seat, he was growing increasingly frustrated with the members' self-discovery and mutual support, which he had begun to see as spineless psychologizing. He wanted action.",
    "is_useful": true,
    "question": "What impact did the publication of the first book about a specific condition have on the awareness and terminology used in the open science community?"
  },
  {
    "text": "Nevertheless, his behavior deteriorated to the point where it became both detrimental to the group and personal to me. Mere weeks after our conference speech in Minneapolis, I had to remove him. He and I both know the exact reasons, and that is enough. I have avoided contact since, but I wish him well. The group had succeeded in including him for a full year. In the process I had learned some hard lessons about both the possibilities and the limits of inclusion, which proved invaluable in later years.\n\nInLv continued without him just as the discussions had begun to gravitate from the purely personal to the more political. A new idea came up in the group, based on the evidence and lived experience that autistic brains are wired differently from the mainstream on a fundamental level. Biological diversity of all kinds is essential to the survival of an ecosystem\u2014so why should neurological diversity, which is one aspect of biological diversity, be any different? The objective fact that neurological diversity exists emerged as a strong argument for the acceptance of autistics and other neurological minorities as distinct classes of people among many, who have something valuable of their own to contribute, and who are as inherently worthy of equal rights as anyone.\n\nIn 1998, Judy Singer from Australia, who identified as having \"AS [Asperger's Syndrome] traits\", turned these InLv discussions into an influential sociological thesis [8] and book chapter [9], citing plenty of group members with their permission, and adding the requisite academic language to lend it legitimacy.",
    "is_useful": true,
    "question": "What lessons can be learned about the importance of neurological diversity in the context of open science and inclusion?"
  },
  {
    "text": "A new idea came up in the group, based on the evidence and lived experience that autistic brains are wired differently from the mainstream on a fundamental level. Biological diversity of all kinds is essential to the survival of an ecosystem\u2014so why should neurological diversity, which is one aspect of biological diversity, be any different? The objective fact that neurological diversity exists emerged as a strong argument for the acceptance of autistics and other neurological minorities as distinct classes of people among many, who have something valuable of their own to contribute, and who are as inherently worthy of equal rights as anyone.\n\nIn 1998, Judy Singer from Australia, who identified as having \"AS [Asperger's Syndrome] traits\", turned these InLv discussions into an influential sociological thesis [8] and book chapter [9], citing plenty of group members with their permission, and adding the requisite academic language to lend it legitimacy. Thus, she is correctly credited with coining the term 'neurodiversity' [10]. However, it may be argued that the American journalist Harvey Blume, who was also an InLv member and whom Singer cites as a frequent discussion partner, first popularized the term [11].What is certain to me is that InLv, due to the ethos of acceptance, inclusivity, and rejection of social and political conformism that I imparted on it, was able to provide the environment in which the idea could emerge.\n\nIt is important to note that InLv's notion of neurodiversity was different from the \"neurodiversity paradigm\" that many contemporary activists subscribe to.",
    "is_useful": true,
    "question": "What is the importance of recognizing neurological diversity in the context of biological diversity and social acceptance?"
  },
  {
    "text": "Thus, she is correctly credited with coining the term 'neurodiversity' [10]. However, it may be argued that the American journalist Harvey Blume, who was also an InLv member and whom Singer cites as a frequent discussion partner, first popularized the term [11].What is certain to me is that InLv, due to the ethos of acceptance, inclusivity, and rejection of social and political conformism that I imparted on it, was able to provide the environment in which the idea could emerge.\n\nIt is important to note that InLv's notion of neurodiversity was different from the \"neurodiversity paradigm\" that many contemporary activists subscribe to. These days it is often held that there is no such thing as a brain that is \"less\" or \"broken\" because \"all neurologies are valid\" [12]. By contrast, neurodiversity as an aspect of biodiversity includes and accepts people with suboptimal neurological configurations.While autistic people who would have preferred to be \"cured\" if possible were a minority in the InLv community, we never excluded or denounced them.\n\nMeanwhile, the InLv community was joined by the #asperger IRC (Internet Relay Chat) channel for which I took over management in 1998. It had been started in 1997 by a German man nicknamed Nox, who had a diagnosis of schizoid personality disorder. He created the channel to be only for people on the autistic spectrum and related conditions.",
    "is_useful": true,
    "question": "What is the significance of the InLv community in relation to the concept of neurodiversity?"
  },
  {
    "text": "These days it is often held that there is no such thing as a brain that is \"less\" or \"broken\" because \"all neurologies are valid\" [12]. By contrast, neurodiversity as an aspect of biodiversity includes and accepts people with suboptimal neurological configurations.While autistic people who would have preferred to be \"cured\" if possible were a minority in the InLv community, we never excluded or denounced them.\n\nMeanwhile, the InLv community was joined by the #asperger IRC (Internet Relay Chat) channel for which I took over management in 1998. It had been started in 1997 by a German man nicknamed Nox, who had a diagnosis of schizoid personality disorder. He created the channel to be only for people on the autistic spectrum and related conditions. I disagreed with the exclusion of neurotypical guests and still do, but I did not feel like I could change this after the channel had become established as what it was. In any case it provided a way for autistic people, including many InLv members, to have text-based conversations that are much more direct than email.\n\nSoon, the combination of these two communities started carrying over into the physical world. Many \"real-world\" relationships resulted, and I would estimate that at least a dozen children were born because of them, including my own three. Amazingly, the #asperger channel survives to this day, though people who join need to be patient as activity is intermittent.",
    "is_useful": true,
    "question": "What movement emphasizes the acceptance of diverse neurological configurations as part of the broader concept of biodiversity?"
  },
  {
    "text": "In spite of all this, InLv continued until early 2013. Sixteen years is a good run for any online community.\n\nProbably the most significant real-life outgrowth of InLv and related communities is the yearly Autscape residential conference (see Buckle, Chapter 8), founded after a 2004 InLv discussion on the idea of creating a European equivalent of Autreat. As one of the Autscape organization's directors, it makes me happy to see InLv's spirit of inclusion and acceptance continue there.\n\n## **References**\n\n- 1. Wing, L., & Shah, A. (2000). Catatonia in autistic spectrum disorders. The *British Journal of Psychiatry, 176* (4), 357\u2013362.\n- 2. American Psychiatric Association. (1994). *Diagnostic and statistical manual of mental disorders* (4th ed.). Washington, DC: American Psychiatric Association.\n- 3. Sinclair, J. (2005, January). *Autism network international: The development of a community and its culture*. Retrieved from http://www.autreat.com/ History_of_ANI.html.\n- 4. Blume, H. (1997, June 30). Autistics, freed from face-to-face encounters, are communicating in cyberspace. *The New York Times*. Retrieved from https://www.nytimes.com.\n- 5. Dekker, M. (1999).",
    "is_useful": true,
    "question": "What significant event was inspired by discussions within an online community related to autism?"
  },
  {
    "text": "*The Atlantic*. Retrieved from https://www. theatlantic.com.\n- 12. Grace, A. (2015, February 24). *Ten things you reject by embracing neurodiversity* (Web log post). Retrieved from http://respectfullyconnected.com/ 2015/02/ten-things-you-reject-by-embracing.\n\n**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/ 4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.\n\nThe images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\n![](_page_62_Picture_8.jpeg)\n\n!",
    "is_useful": true,
    "question": "What are the key principles of Open Access as defined by Creative Commons licenses?"
  },
  {
    "text": "Retrieved from http://respectfullyconnected.com/ 2015/02/ten-things-you-reject-by-embracing.\n\n**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/ 4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.\n\nThe images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\n![](_page_62_Picture_8.jpeg)\n\n![](_page_63_Picture_0.jpeg)\n\n# 4\n\n## **Autistic People Against Neuroleptic Abuse**\n\n**Dinah Murray**\n\n## **Origins**\n\nIn the mid-90s I had a job as a support worker for people with severe and multiple learning (intellectual) disabilities including autism, who had been discharged from National Health Service (NHS)-run long-stay hospitals into \"the community.\" Our training for this job was thorough and humane in many ways, but little was said about the stacks of ring-bound blister packs of pills kept under lock and key.",
    "is_useful": true,
    "question": "What are the terms of the Creative Commons license that allow for the use and sharing of content related to open science?"
  },
  {
    "text": "This also gave me more time for research and to develop a campaigning website. Thanks to pharmacologist Paul Shattock who told me about it\u2014he was publicly concerned about the use of neuroleptics for behavior control long\n\n![](_page_65_Figure_3.jpeg)\n\n**Fig. 4.1** The chart clearly showed that the vast majority of claimed \"benefits\" of medication in this sphere are about reducing behavior rather than enhancing personal well-being or capacity\n\nbefore his public support for Andrew Wakefield's dubious research into the MMR vaccine\u2014I went to an Autism Europe Conference in Brussels and heard about their Code of Good Practice on Prevention of Violence against Persons with Autism [1]. Paul also introduced me there to a concerned mother from the UK; meeting her at that conference was the first necessary step toward getting some activists working together in the tiny and unstructured group we called Autistic People Against Neuroleptic Abuse, or APANA for short.\n\nThe point of APANA was to be an effective vehicle to raise awareness of the harms being done to vulnerable people in the name of care, and to penetrate some entrenched positions in huge and deep-rooted power structures. For people with access (not the subjects of my case studies!), the Internet proved a rich source of information about both government and NGO thinking on these and related issues: there were consultations, and guidelines, and that was one way to get one's voice heard.",
    "is_useful": true,
    "question": "What is the purpose of the group Autistic People Against Neuroleptic Abuse (APANA)?"
  },
  {
    "text": "Paul also introduced me there to a concerned mother from the UK; meeting her at that conference was the first necessary step toward getting some activists working together in the tiny and unstructured group we called Autistic People Against Neuroleptic Abuse, or APANA for short.\n\nThe point of APANA was to be an effective vehicle to raise awareness of the harms being done to vulnerable people in the name of care, and to penetrate some entrenched positions in huge and deep-rooted power structures. For people with access (not the subjects of my case studies!), the Internet proved a rich source of information about both government and NGO thinking on these and related issues: there were consultations, and guidelines, and that was one way to get one's voice heard.\n\n#### **Getting to Work and Forming Alliances**\n\nAPANA recruited two autistic people: as Chairperson, David Andrews, who was in the process of acquiring several psychology-related qualifications; as Patron, Wendy Lawson (now known as Wenn Lawson), who was well along a similar path. The rest of the team were all parents of adult offspring they had seen damaged by psychiatrictreatment. I was spending more and more time reading research (for example, [2\u20136]). I discovered that an unexpected result of the hospital closures was that prescriptions of psychotropics had gone up as neighbors of the new noisy people in the community complained. When I looked into the old prescription records, I found that this was true for almost everyone in the houses where I most often worked. The institutions had eventually had a policy of reducing medication.",
    "is_useful": true,
    "question": "What role does the Internet play in raising awareness and advocating for vulnerable individuals in the context of open science and collaborative activism?"
  },
  {
    "text": "#### **Getting to Work and Forming Alliances**\n\nAPANA recruited two autistic people: as Chairperson, David Andrews, who was in the process of acquiring several psychology-related qualifications; as Patron, Wendy Lawson (now known as Wenn Lawson), who was well along a similar path. The rest of the team were all parents of adult offspring they had seen damaged by psychiatrictreatment. I was spending more and more time reading research (for example, [2\u20136]). I discovered that an unexpected result of the hospital closures was that prescriptions of psychotropics had gone up as neighbors of the new noisy people in the community complained. When I looked into the old prescription records, I found that this was true for almost everyone in the houses where I most often worked. The institutions had eventually had a policy of reducing medication.\n\nThere was one discovery in the files that tipped my concern from a commitment to an out-and-out mission. A service user I'll call Patrick, who was on the highest neuroleptic dose I found in this group, had become borderline catatonic. From the files it was clear that he had once been fairly lively. Then I found a letter, addressed to senior management from the \"community\" learning disability psychiatrist. She complained in her letter that support workers had questioned her judgment that Patrick's medication should be increased by 30%. Since the rules that govern care providers explicitly require them to follow all medical advice or put their business at risk, this reproach was significant and probably got some caring and conscientious workers into a bit of trouble.",
    "is_useful": true,
    "question": "What factors can influence the prescription of psychotropic medications in a community following the closure of mental health institutions?"
  },
  {
    "text": "It is ironic that \"health and safety\" risk aversion in social care settings led to routine, authorized, high-risk behavior by both staff and management toward the people receiving their \"care.\"\n\nI discovered that distinguished psychiatrist Lorna Wing was interested in autism and catatonia, and had expressed concerns that neuroleptic prescribing was sometimes implicated in its onset. So I rang Lorna Wing's Centre, then known as the Centre for Social and Communication Disorders, in the hopes of speaking to her. She was not there that day but Judith Gould came to the phone to deal with this unknown support worker's anxious query. She listened to the problem, immediately said I should ring Lorna Wing herself and gave me her home phone number. So next I picked up the phone and dialed that. The great Wing answered the phone herself and was so interested and open that I trusted her at once and said very soon after our conversation began, that all I had found among her colleagues was \"Arrogance, ignorance, and hypocrisy\"\u2014to which, taking my breath away, she replied with vigor, \"I couldn't agree more!\" One couldn't hope for a better ally in this particular battle [7].\n\n#### **Communicating Our Message Across Many Platforms**\n\nI decided to write up case studies of what I'd found in the files in a way that would make the research as effective as possible. I looked at quality of life issues and the impact on those of the ramified adverse effects of the prescribed drugs; I compiled detailed timelines for four service users, three of them autistic, and their life events [8].",
    "is_useful": true,
    "question": "What concerns arise from risk aversion in health and social care, and how can effective communication strengthen the understanding of quality of life issues related to prescribed treatments?"
  },
  {
    "text": "I looked at quality of life issues and the impact on those of the ramified adverse effects of the prescribed drugs; I compiled detailed timelines for four service users, three of them autistic, and their life events [8].\n\nUnfortunately, how to carry out quality of life assessments objectively remains a vexed question to this day [9], so instead I took and followed advice on assessing relevant costs, such as travel and staff time, in British pounds and went back through the records adding actual costs or rulegoverned estimates of costs to the timelines.\n\nOur website, run by my friend Sue Craig, had much factual information, including all the illustrations to this chapter, and useful links and ancient advertisements for old drugs and new. Canadian artist Ralph Smith designed an elegant logo for us (Fig. 4.2). This served us well and got the message out that Autistic People Against Neuroleptic Abuse was an active force. Autistic activists in the USA such as A. M. Baggs (now\n\n![](_page_68_Figure_3.jpeg)\n\n**Fig. 4.2** Summarized comparisons of restrictive or potentially fatal effects of psychotropic drugs prescribed for autistic adults with learning disabilities and challenging behavior (ca 2001). From left to right, starting with the antipsychotics: phenothiazines (e.g. Chlorpromazine), thioxanthines (e.g.",
    "is_useful": true,
    "question": "What challenges are associated with objectively assessing the quality of life for individuals affected by prescribed drugs?"
  },
  {
    "text": "Our website, run by my friend Sue Craig, had much factual information, including all the illustrations to this chapter, and useful links and ancient advertisements for old drugs and new. Canadian artist Ralph Smith designed an elegant logo for us (Fig. 4.2). This served us well and got the message out that Autistic People Against Neuroleptic Abuse was an active force. Autistic activists in the USA such as A. M. Baggs (now\n\n![](_page_68_Figure_3.jpeg)\n\n**Fig. 4.2** Summarized comparisons of restrictive or potentially fatal effects of psychotropic drugs prescribed for autistic adults with learning disabilities and challenging behavior (ca 2001). From left to right, starting with the antipsychotics: phenothiazines (e.g. Chlorpromazine), thioxanthines (e.g. Fluenthixol), Haloperidol, pimozide, risperidone, olanzapine; and then various non-neuroleptic experimental drugs for autism-related perceived problems, viz paroxetine, lithium, carbamazepine, buspirone, naltrexone, and the beta-blocker propranolol\n\nknown as Mel Baggs) and Kassiane Sibley (now Kassiane Asasumasu) were supportive and deepened my understanding of what it is like to be on the receiving end of interventions designed solely for the purpose of suppressing behavior which other people condemn.",
    "is_useful": true,
    "question": "What role do community-based resources play in shaping awareness and understanding of the effects of psychotropic drugs on autistic individuals?"
  },
  {
    "text": "I also got a poster presentation at the Autism Europe Conference in Glasgow that year and Wen Lawson and/or I were there in person throughout ready to discuss it.\n\n#### **Pressing Parliament and Leaning on the Law**\n\nI undertook a careful analysis of a range of medications proposed at the time for \"ameliorating autistic behavior,\" and I scored adverse effects according to their recognized frequency (using the free Medline database). Risperidone, newly popular with prescribers, was only slightly less harmful than chlorpromazine at the recommended (likefor-like) dose ranges for psychosis (see Fig. 4.3). Wenn Lawson and myself addressed a sub-committee of the All-Party Parliamentary Group\n\n![](_page_70_Picture_1.jpeg)\n\n**Fig. 4.3** The APANA logo, designed by Ralph Smith\n\non Autism about prescribing, illustrated with Fig. 4.1. (One of the people who heard us was Virgina Bovell, a mother of an autistic person with learning disabilities, in whom we found a new ally despite her involvement with behaviorism\u2014I later came to understand that parents may be presented with behavioral approaches as the only alternative to a medicalized attitude and recourse to drugs.)\n\nI got views from as many as possible autistic people with relevant experience and discovered that some, but not all, were saying that at a very low dose, they found risperidone positively helpful; that it improved their mood and could make social encounters less stressful.",
    "is_useful": true,
    "question": "What factors should be considered when evaluating medications proposed for addressing autistic behavior?"
  },
  {
    "text": "These are to be found following the main text of my \"Potions and Pills\" piece [8] in the longer version online.\n\nA related line I decided to follow was to get a proper legal view pro *bono* (for no fee because for the public good) if possible. Somehow I found a deeply committed solicitor, Karen Ashton, and barrister Paul Bowen of the Doughty Street Chambers in London, who were willing to look to assess the legal situation vis-\u00e0-vis medical treatment that flouts the Hippocratic Oath, Do No Harm\u2014especially to people deemed to be \"mentally incapacitated.\" They needed someone to gather the evidence together and I gave up my job entirely for three months, during which I created a compendium of abstracts substantiating a great variety of adverse effects and at the same time demonstrating that reduced behavior was the key index of \"efficacy\" in their use (see Fig. 4.1). This resulted in lawyers Bowen and Ashton producing a discussion paper (available in the online edition of this chapter), which suggested that medical treatment can amount to an assault unless great care is taken regarding consent or \"best interests.\"They proposed that the Human Rights Court \"may well be willing to exercise its power in relation to the prescribing of psychotropic drugs, particularly where serious side-effects are well-established.\" This was circulated widely.\n\nIn 2001, Ashcroft and colleagues [10] called for better research into antipsychotic prescribing for \"challenging behaviours.\"",
    "is_useful": true,
    "question": "What legal considerations are associated with medical treatments that may violate the Hippocratic Oath, particularly in relation to consent and the rights of mentally incapacitated individuals?"
  },
  {
    "text": "They needed someone to gather the evidence together and I gave up my job entirely for three months, during which I created a compendium of abstracts substantiating a great variety of adverse effects and at the same time demonstrating that reduced behavior was the key index of \"efficacy\" in their use (see Fig. 4.1). This resulted in lawyers Bowen and Ashton producing a discussion paper (available in the online edition of this chapter), which suggested that medical treatment can amount to an assault unless great care is taken regarding consent or \"best interests.\"They proposed that the Human Rights Court \"may well be willing to exercise its power in relation to the prescribing of psychotropic drugs, particularly where serious side-effects are well-established.\" This was circulated widely.\n\nIn 2001, Ashcroft and colleagues [10] called for better research into antipsychotic prescribing for \"challenging behaviours.\" They cited Brylewski and Duggan's [11] *Cochrane Review*, as showing \"over 500 citations assessing the impact of antipsychotic drugs on challenging behaviour. Of these only three were methodologically sound randomised controlled trials, but even these were unable to show whether antipsychotic drugs were beneficial or not in controlling challenging behaviour\" [10].\n\n### **Impact**\n\nAshcroft and colleagues frame the issues thus: **\"**People with learning disability sometimes display challenging behaviour. This can be managed by use of antipsychotic medication or behavioural therapy or both. There is no solid evidence, however, that these therapies are safe and effective.\"",
    "is_useful": true,
    "question": "What concerns are raised regarding the safety and effectiveness of antipsychotic medication in managing challenging behavior for individuals with learning disabilities?"
  },
  {
    "text": "This was circulated widely.\n\nIn 2001, Ashcroft and colleagues [10] called for better research into antipsychotic prescribing for \"challenging behaviours.\" They cited Brylewski and Duggan's [11] *Cochrane Review*, as showing \"over 500 citations assessing the impact of antipsychotic drugs on challenging behaviour. Of these only three were methodologically sound randomised controlled trials, but even these were unable to show whether antipsychotic drugs were beneficial or not in controlling challenging behaviour\" [10].\n\n### **Impact**\n\nAshcroft and colleagues frame the issues thus: **\"**People with learning disability sometimes display challenging behaviour. This can be managed by use of antipsychotic medication or behavioural therapy or both. There is no solid evidence, however, that these therapies are safe and effective.\" Unfortunately the possibility that behavioral therapy may not be safe was not pursued, nor was the possibility that a focus on behavior control cannot preserve mutuality, create trust, or be authentically \"person-centered.\" This fixation with behavior, along with skillful marketing of \"Positive Behaviour Support,\" has underpinned and undermined a medical campaign against the drugging launched in 2018 (see below).\n\nConsent issues and the best interest concept were soon to be leading themes in the Mental Capacity Act (2007), (a development to which Paul Bowen contributed).",
    "is_useful": true,
    "question": "What are the concerns regarding the safety and effectiveness of therapies for managing challenging behavior in individuals with learning disabilities?"
  },
  {
    "text": "### **Impact**\n\nAshcroft and colleagues frame the issues thus: **\"**People with learning disability sometimes display challenging behaviour. This can be managed by use of antipsychotic medication or behavioural therapy or both. There is no solid evidence, however, that these therapies are safe and effective.\" Unfortunately the possibility that behavioral therapy may not be safe was not pursued, nor was the possibility that a focus on behavior control cannot preserve mutuality, create trust, or be authentically \"person-centered.\" This fixation with behavior, along with skillful marketing of \"Positive Behaviour Support,\" has underpinned and undermined a medical campaign against the drugging launched in 2018 (see below).\n\nConsent issues and the best interest concept were soon to be leading themes in the Mental Capacity Act (2007), (a development to which Paul Bowen contributed). That is a very strong piece of rights legislation in principle, though it has thrown up some paradoxes in practice (see, e.g., DoLS discussion at House of Lords 2015, or this Parliamentary video from 2018, https://www.parliamentlive.tv/Event/Index/d47bf41e-72b1- 48d8-afc5-b5727a40f05b). The MCA guidelines draw attention to the possibility that the psychotropic effects of some medications may hinder judgment, and there are widespread guidelines on administering medication to people whose best interests must in law be factored in.",
    "is_useful": true,
    "question": "What are some concerns related to the safety and effectiveness of therapies for individuals with learning disabilities?"
  },
  {
    "text": "*Psychotropic drug prescribing for people with intellectual disability, mental health problems and/or behaviours that challenge: Practice guidelines.* (Faculty Report No. FR/ID/09). Retrieved from The Royal College of Psychiatry website: https://www. rcpsych.ac.uk.\n- 14. DeFilippis, M., & Wagner, K. D. (2016). Treatment of autism spectrum disorder in children and adolescents. *Psychopharmacology Bulletin, 46* (2), 18\u201341.\n\n**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/ 4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and\n\nindicate if changes were made.\n\nThe images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\n![](_page_75_Picture_6.jpeg)\n\n!",
    "is_useful": true,
    "question": "What are the licensing terms for the chapter that discusses psychotropic drug prescribing for individuals with intellectual disabilities and mental health issues?"
  },
  {
    "text": "The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\n![](_page_75_Picture_6.jpeg)\n\n![](_page_76_Picture_0.jpeg)\n\n# 5\n\n## **Autistics.Org and Finding Our Voices as an Activist Movement**\n\n**Laura A. Tisoncik**\n\n## **Deep Origins: Martin Luther King Jr. and the Fair Housing Campaign**\n\nMany have said that the first explicitly political act that emerged from the autistic community was the website autistics.org. A few people have said that I, as the founder of autistics.org, am the founding mother of autistic activism.\n\nThat's not true. While the community, at the time of the founding of autistics.org in 1998, was oriented toward support groups, political activism was in the air. Some persons active at that time, notably Cal Montgomery, had roots in the broader disability rights, psychiatric survivors, and developmental disability movements, and were posting cogent political positions on Listservs, Usenet, and other now almost forgotten corners of the internet before autistics.org was even imagined. Nothing arises from a vacuum, and neither did autistics.org.",
    "is_useful": true,
    "question": "What are the licensing implications for using third-party materials included in a chapter under a Creative Commons license?"
  },
  {
    "text": "According to the diagnoses of the day. Whatever. We are all Autism Spectrum now.\n\nand whose every \"treatment\" and \"care\" is an act of violence against who we are. If we do find work, we are target number one for workplace bullying, and for being fired for autistic traits, regardless of our performance. We are first to be targeted by criminals and among the first to be targeted by police, at least in the US, where every year unarmed autistic people are among those shot by police, and where autistic people are so often stopped while going about our business we have taken to calling this the crime of \"walking while autistic.\" We are more likely to be the victims of violence, yet we are portrayed in the media and by charities \"raising awareness\" as dangerous perpetrators of violence. Above all we are isolated from society at every stage as the odd, the weird, the other.\n\nBut no matter the exact life path we find ourselves on, oppression comes down to others holding or aspiring to hold undue power over us. Oppression is always the same story, and the same struggle, of the powerless against the powerful. \"Injustice anywhere is a threat to justice everywhere\" wrote the Great Agitator, King [1], \"We are caught in an inescapable network of mutuality, tied in a single garment of destiny.\"\n\n#### **What Led up to the Creation of Autistics.Org**\n\nI joined the mailing lists and IRC channels where the early online autistic community congregated, including the main mixed parent and autistic IRC channel, #autism on Starlink IRC.",
    "is_useful": true,
    "question": "What challenges do individuals on the autism spectrum face in society?"
  },
  {
    "text": "When I circulated a petition requesting that one parent be removed as an operator for his open hostility toward autistic people on #autism, I was banned from the channel. So I started my own channel for autistics and parents, #autfriends.\n\n#Autfriends quickly becomes a popular channel. In response the owner of #autism approached the owners of a channel for autistic persons and persuaded him to order me to close down #autfriends and tell everyone to rejoin #autism. When I refused, pointing out that he had no authority to give me or anyone else any orders outside of his own channel, and stating again that management at #autism tolerated open abuse of autistic people, the autistic channel owner banned me from his channel and mailing list, and sent the log of that private conversation to the IRC operators at Starlink IRC, possibly through the owner of #autism.\n\nNever fear an opponent who goes too far. It is at times like this, when one's opponents overreach, that they grant you power, much as when an attacker lunges at a judoka, the force of that attack becomes the power the judo player can use to defeat the attacker. Almost every time a political action I was involved in had succeeded, it had been in part because of gross overreach by the other side.\n\nThis attack on #autfriends gave us the moral power and the outrage to create autistics.org. And of course, #autfriends reopened within minutes of the ban on Dalnet, because trying to ban an idea from the Internet is one of the more futile acts imaginable.",
    "is_useful": true,
    "question": "What can be the consequences of overreach by opponents in the context of advocacy and open forums?"
  },
  {
    "text": "But I was being treated as the defective child that so many of the regulars in #autism imagined autistic adults to be, and so in their minds there were no boundaries.\n\n*Letter from a Birmingham Jail* was an open letter, not to enemies of the civil rights movement, but to supporters who objected to Rev. King's tactics. \"History is the long and tragic story of the fact that privileged groups seldom give up their privileges voluntarily,\" King wrote to his critics who had argued for negotiations instead of protests. \"Individuals may see the moral light and voluntarily give up their unjust posture; but as Reinhold Niebuhr has reminded us, groups are more immoral than individuals.\" Politely asking the operators of the Judge Rotenberg Center to stop torturing inmates will make little headway, as all of their interests, and that of the many other centers of power that oppress autistic people, lie in the direction of continuing to do so. But picketing the home of the head of the US Food and Drug Administration to demand that shock devices be outlawed (as the disability rights organization ADAPT, in an action organized by Cal Montgomery, did recently) can begin to dislodge that evil. And before you can get to political action, you need to give voice to truths not spoken.\n\n#### **Autistics.Org**\n\nAutistics.org came directly out of this tumult around #Autfriends. It was envisioned as a compilation of support and resources for autistic adults, in contrast to providing resources only for parents of autistic children as #autism did.",
    "is_useful": true,
    "question": "What is the significance of open letters in advocating for marginalized groups and their rights?"
  },
  {
    "text": "Politely asking the operators of the Judge Rotenberg Center to stop torturing inmates will make little headway, as all of their interests, and that of the many other centers of power that oppress autistic people, lie in the direction of continuing to do so. But picketing the home of the head of the US Food and Drug Administration to demand that shock devices be outlawed (as the disability rights organization ADAPT, in an action organized by Cal Montgomery, did recently) can begin to dislodge that evil. And before you can get to political action, you need to give voice to truths not spoken.\n\n#### **Autistics.Org**\n\nAutistics.org came directly out of this tumult around #Autfriends. It was envisioned as a compilation of support and resources for autistic adults, in contrast to providing resources only for parents of autistic children as #autism did. But, even as such a compilation proved too difficult for us at that time, inspired by the outrageousness of the attacks against us, speaking truths not spoken became our mission.\n\nOne of our more popular truth-telling sections was ostensibly a medical institute operated by autistic researchers, the Institute for the Study of the Neurologically Typical (ISNT). ISNT turned the tables on the dehumanization done to autistic people by autism researchers.",
    "is_useful": true,
    "question": "What is the importance of giving voice to unspoken truths in the context of advocating for the rights of autistic individuals?"
  },
  {
    "text": "And before you can get to political action, you need to give voice to truths not spoken.\n\n#### **Autistics.Org**\n\nAutistics.org came directly out of this tumult around #Autfriends. It was envisioned as a compilation of support and resources for autistic adults, in contrast to providing resources only for parents of autistic children as #autism did. But, even as such a compilation proved too difficult for us at that time, inspired by the outrageousness of the attacks against us, speaking truths not spoken became our mission.\n\nOne of our more popular truth-telling sections was ostensibly a medical institute operated by autistic researchers, the Institute for the Study of the Neurologically Typical (ISNT). ISNT turned the tables on the dehumanization done to autistic people by autism researchers. Several contributors to that site picked apart characteristics of neurotypical individuals in the same patronizing, pathologizing, voice in which traits commonly held by autistic people are described, with feigned obliviousness to how such traits might also be useful, and no concession that neurotypicals might not be carbon-copy identical. The point of ISNT (if I can presume to speak for multiple authors here, though I do think this was a universal goal among us at that time) was to shine a light on how we are treated. Some people to this day take it literally as an assertion of autistic superiority, which leaves me wondering how deeply ingrained are their assumptions about our supposed inferiority, that they cannot recognize satire.",
    "is_useful": true,
    "question": "What is the role of truth-telling in advocating for marginalized communities in the context of open science?"
  },
  {
    "text": "One of our more popular truth-telling sections was ostensibly a medical institute operated by autistic researchers, the Institute for the Study of the Neurologically Typical (ISNT). ISNT turned the tables on the dehumanization done to autistic people by autism researchers. Several contributors to that site picked apart characteristics of neurotypical individuals in the same patronizing, pathologizing, voice in which traits commonly held by autistic people are described, with feigned obliviousness to how such traits might also be useful, and no concession that neurotypicals might not be carbon-copy identical. The point of ISNT (if I can presume to speak for multiple authors here, though I do think this was a universal goal among us at that time) was to shine a light on how we are treated. Some people to this day take it literally as an assertion of autistic superiority, which leaves me wondering how deeply ingrained are their assumptions about our supposed inferiority, that they cannot recognize satire.\n\nUnderstand, though, that I am not upset about claims of autistic superiority, even though I don't believe in anyone's intrinsic superiority. Every oppressed group struggles to reclaim a sense of intactness and worth, and phases where some members of the group claim superiority and even attempt to separate from the mainstream, \"inferior,\" society are a normal stage in the reclamation of human dignity.",
    "is_useful": true,
    "question": "What role does the reclamation of human dignity play in the context of oppressed groups asserting a sense of superiority?"
  },
  {
    "text": "Another popular feature was the graphics we produced. One of my life's proudest accomplishments was designing the original \"I am not a puzzle/I am a person\" graphic with a human-shaped puzzle piece inside a red circle crossed with a slash\u2014the \"not\" symbol used on road signs. The original still hangs on my living room wall. Every time I see this design reused, probably by people who know only that it is a classic image of the autistic civil rights movement, I am reminded that I do have children who will carry on the values I imbued them with long after I am gone: that graphic is one of my progeny.\n\nWe encouraged autistic people to write for our library, where we posted essays that did anything but reaffirm that we were defective children whose job it was to obey. It was in the library where we most clearly spoke truths that had not been often said: that almost all autism treatment is based on false models of autism, that the institutions that have grown up around autism engage in violence, sometime subtle violence, and often overt violence, against autistic people in order to obtain compliance, that parents are often part of the problem\u2014that bearing an autistic child makes no one, *ipso facto*, a saint, and that actual parents of autistic children are, if anything, more inclined to engage in abusive behavior toward autistic persons than the average parent, as illustrated by the many cases where parents of autistic children murder their own children. Of course, there are also many allies of ours among the parents of autistic children.",
    "is_useful": true,
    "question": "What are some key themes related to the experiences and treatment of autistic individuals discussed in the context of open science?"
  },
  {
    "text": "We encouraged autistic people to write for our library, where we posted essays that did anything but reaffirm that we were defective children whose job it was to obey. It was in the library where we most clearly spoke truths that had not been often said: that almost all autism treatment is based on false models of autism, that the institutions that have grown up around autism engage in violence, sometime subtle violence, and often overt violence, against autistic people in order to obtain compliance, that parents are often part of the problem\u2014that bearing an autistic child makes no one, *ipso facto*, a saint, and that actual parents of autistic children are, if anything, more inclined to engage in abusive behavior toward autistic persons than the average parent, as illustrated by the many cases where parents of autistic children murder their own children. Of course, there are also many allies of ours among the parents of autistic children. The point is that no one automagically becomes an ally by virtue of merely existing\u2014if you want to be an ally of any oppressed group, and you have privilege, you have to choose to be an ally, educate yourself, and work at it. Just thinking that you deserve to be counted among the good guys is *always* insufficient.\n\n#### **The End of Autistics.Org?**\n\nI began this adventure in middle age at a time when I was beginning to have significant health problems, and aging is no friend of chronic health issues. Eventually I could no longer maintain the website. I put it first into an archived state, trying to preserve what was already there.",
    "is_useful": true,
    "question": "What are some essential elements of allyship for supporting oppressed groups?"
  },
  {
    "text": "I emphasize how you can make anyone look terrible if you describe them selectively.\n\nI ask others to contribute their photos, selectively negative descriptions, and actual opinions [1]. But I'm the only one. Instead of a website starting with multiple pictures you can click, it ends up just me.\n\n\"Getting the Truth Out\" becomes better-known than I expect. I remain unaware of the overall effect this website has had.\n\nWhat I remember is people:\n\n- Not grasping it was a parody, especially after the ASA took \"Getting the Word Out\" down.\n- Thinking I meant it as straight autobiography.\n- Not noticing I was highlighting the dangers of using a selective pathological description of anyone.\n\n- Thinking the selective descriptions and stereotypes must be true, or I'm claiming they're true.\nBut the message was what I got from Cal: Those of us making the online arguments, and those of us described pathologically, can be the same people.\n\nI'm not a different person than the scared child who found she still existed despite being in an institution. Who slammed her head against the wall of that room until brutally restrained. Who attracted pathological descriptions that stripped her of her humanity and future. I didn't become a different person once I started saying, \"Institutions are bad.\"\n\nEvery person who could be described as an autistic child banging their head on the wall of an institution, is a hell of a lot more than that. It conjures a stereotype, a story. But it's not a type of person. It's more how others see us than who we are.",
    "is_useful": true,
    "question": "What warning does the text convey about the implications of selective descriptions in the context of personal identity?"
  },
  {
    "text": "But I didn't write this to tell my story. Each thing I mention can apply to many people and situations that aren't identical to the one I describe. That's how all my writing works: I write the specific but aim for broad applicability.\n\nOur actions matter. We may never recognize the full impact we have. Everything we do can have a profound effect on others. Remember that when you think you've lost the argument, that you've failed, that nobody understood. Most of the effects of our actions are unseen but important. Without Cal's single email, there'd have been no \"Past, Present, and Future,\" \"Getting the Truth Out,\" or \"In My Language\" [2].\n\nI owe debts to different disability movements: Developmental disability (DD) self-advocacy, Deaf culture, psych survivors, the independent living movement, others. They have shaped everything I've ever done in the autistic community, far more than the autistic community itself has.\n\nLike everyone I know who's been called a leader within the autistic community, I took in many perspectives from outside the community, and functioned within a broader sphere than one community. We're part of the disability rights movement. Other disability communities influenced everything I'm known for within the autistic community. Some things people say as \"about autism,\" were not only never \"about autism,\" but came out of things like the DD self-advocacy movement. \"In My Language\" was an act of DD solidarity with a girl with cerebral palsy whose parents mutilated her, not a statement about autism.",
    "is_useful": true,
    "question": "How can individual actions contribute to broader movements such as open science or disability rights?"
  },
  {
    "text": "I owe debts to different disability movements: Developmental disability (DD) self-advocacy, Deaf culture, psych survivors, the independent living movement, others. They have shaped everything I've ever done in the autistic community, far more than the autistic community itself has.\n\nLike everyone I know who's been called a leader within the autistic community, I took in many perspectives from outside the community, and functioned within a broader sphere than one community. We're part of the disability rights movement. Other disability communities influenced everything I'm known for within the autistic community. Some things people say as \"about autism,\" were not only never \"about autism,\" but came out of things like the DD self-advocacy movement. \"In My Language\" was an act of DD solidarity with a girl with cerebral palsy whose parents mutilated her, not a statement about autism. I told CNN this. They edited it out, replaced with something I never said [3].\n\nWhatever perspective we come from, we need to be prepared to think we've failed, to never know our full impact. Cal may've lost an argument in 1995, but he showed me a way of seeing myself and other disabled people that proved central in nearly everything I ever did that had an impact. I'm sure even my least pleasant contributions have had important effects I'll never know about.\n\nSometimes we think we've lost, but it's only the beginning of things we can't imagine. We have to do the right thing even when it looks like we're failing.",
    "is_useful": true,
    "question": "How do different disability movements influence perspectives within the autistic community?"
  },
  {
    "text": "A psychologist encouraged us to obtain an evaluation, and after several months of testing, in mid-2000 our child was diagnosed with Asperger syndrome.\n\n<sup>1</sup>Since mid-2006, I have consistently used masculine pronouns to refer to the person identified in many of my earlier advocacy letters as my youngest daughter.\n\nK. Seidel (B) Peterborough, NH, USA\n\nI set about learning everything I could about autism in order to best understand his needs and articulate those needs to his teachers. I read works by Lorna Wing [2], Uta Frith [3], Tony Attwood [4], and Simon Baron-Cohen [5]. I attended AANE (Asperger/Autism Network) seminars. I participated in discussions on the St. John's Autism List [6]. I trawled the internet for information and created a database to store my bookmarks and notes. I am a librarian by training, and had already developed several websites; an autism portal seemed like a potentially useful and rewarding creative project.\n\nThat took a few years to happen. After losing my job in the Dot-Com Bust of 2000, I started a used bookselling business, and underwent a profound shift in my understanding of autism.\n\n#### **Engaging Neurodiversity**\n\nI first encountered \"neurodiversity\" in an article in*The Atlantic* describing online spaces created by autistic adults [7]. I loved the compassionate, inclusive flavor of the word, and its broad call for respect for people like my child.",
    "is_useful": true,
    "question": "What is the concept that emphasizes respect and inclusion for individuals with autism, as referenced in discussions among autistic adults?"
  },
  {
    "text": "I attended AANE (Asperger/Autism Network) seminars. I participated in discussions on the St. John's Autism List [6]. I trawled the internet for information and created a database to store my bookmarks and notes. I am a librarian by training, and had already developed several websites; an autism portal seemed like a potentially useful and rewarding creative project.\n\nThat took a few years to happen. After losing my job in the Dot-Com Bust of 2000, I started a used bookselling business, and underwent a profound shift in my understanding of autism.\n\n#### **Engaging Neurodiversity**\n\nI first encountered \"neurodiversity\" in an article in*The Atlantic* describing online spaces created by autistic adults [7]. I loved the compassionate, inclusive flavor of the word, and its broad call for respect for people like my child. I tucked it into my memory, whence it emerged one evening in January 2001, as Dave and I were brainstorming ideas for domain names. As it turned out, \"neurodiversity\" was available, so we registered it on the spot.\n\nAlthough bookselling temporarily derailed my autism website plans, I continued to read and squirrel away URLs, increasingly gravitating to work by autistic authors. I delved into essays by Jim Sinclair [8], Frank Klein [9], Larry Arnold [10], and Joelle (then Joel) Smith [11].",
    "is_useful": true,
    "question": "What are some ways individuals contribute to the promotion of open science in the field of neurodiversity?"
  },
  {
    "text": "#### **Engaging Neurodiversity**\n\nI first encountered \"neurodiversity\" in an article in*The Atlantic* describing online spaces created by autistic adults [7]. I loved the compassionate, inclusive flavor of the word, and its broad call for respect for people like my child. I tucked it into my memory, whence it emerged one evening in January 2001, as Dave and I were brainstorming ideas for domain names. As it turned out, \"neurodiversity\" was available, so we registered it on the spot.\n\nAlthough bookselling temporarily derailed my autism website plans, I continued to read and squirrel away URLs, increasingly gravitating to work by autistic authors. I delved into essays by Jim Sinclair [8], Frank Klein [9], Larry Arnold [10], and Joelle (then Joel) Smith [11]. I discovered the writings of Michelle Dawson, who opposed efforts in Canada to mandate one form of behavioral training as \"medically necessary\" for autistic children [12]. Laura Tisoncik's and Mel (then Amanda) Baggs's \"Institute for the Study of the Neurologically Typical\" [13] (see Tisoncik, Chapter 5) made a strong impression, and both eventually became friends. Janet Norman-Bain's \"Oops \u2026Wrong Planet! Syndrome\" website led to hours of exploration [14]. I gave Gunilla Gerland's *Finding Out about Asperger Syndrome* [15] to my child to help him understand his diagnosis. Every reading sparked new shocks of recognition.",
    "is_useful": true,
    "question": "What concept emphasizes the respect and inclusion of individuals with diverse neurological conditions?"
  },
  {
    "text": "I delved into essays by Jim Sinclair [8], Frank Klein [9], Larry Arnold [10], and Joelle (then Joel) Smith [11]. I discovered the writings of Michelle Dawson, who opposed efforts in Canada to mandate one form of behavioral training as \"medically necessary\" for autistic children [12]. Laura Tisoncik's and Mel (then Amanda) Baggs's \"Institute for the Study of the Neurologically Typical\" [13] (see Tisoncik, Chapter 5) made a strong impression, and both eventually became friends. Janet Norman-Bain's \"Oops \u2026Wrong Planet! Syndrome\" website led to hours of exploration [14]. I gave Gunilla Gerland's *Finding Out about Asperger Syndrome* [15] to my child to help him understand his diagnosis. Every reading sparked new shocks of recognition.\n\nI began to notice autistic traits in every interaction I had with my father, and my mother\u2014a music and special education teacher\u2013increasingly shared that recognition. A chemical engineer born in 1933, Dad contracted polio at the age of ten, and used crutches, braces, and a wheelchair thereafter. In our household, \"disability\" and mobility accommodations were ordinary, as was a certain interpersonal *je ne sais quoi*. Dad was an analytical thinker with minimal tolerance for his own frustration or others' emotionality; he was practical, brusque, and disinclined to social niceties.",
    "is_useful": true,
    "question": "What themes related to autism and the understanding of disability are explored in the essays and writings mentioned?"
  },
  {
    "text": "In our household, \"disability\" and mobility accommodations were ordinary, as was a certain interpersonal *je ne sais quoi*. Dad was an analytical thinker with minimal tolerance for his own frustration or others' emotionality; he was practical, brusque, and disinclined to social niceties. (One especially memorable phone call began in mid-sentence, as if I were mid-conversation with him a continent away.) I gradually recognized that I, too, lay somewhere along the banks of the \"broader autistic phenotype\": intense focus; bluntness; anxiety and occasional sensory overload; fondness for collecting, organizing, and diving deeply into subjects that interest me. Why had nearly a decade passed before Dave and I figured out that our youngest child was on the spectrum? Perhaps because he was not so different from his grandfather, or from me.\n\n#### **Neurodiversity.Com**\n\nThe appearance of \"neurodiversity\" in the *New York Times* [16] signaled that the time had come to transform my database into a website. In May 2004, after a week of nonstop HTML writing, Neurodiversity.com was born. Over 100 different pages included \"Positive Perspectives on Autism,\" \"Girls & Women on the Spectrum,\" \"The Question of Cure,\" and \"Neurotypical Issues\"; the site also featured a game, \"Unmasking the Face\" [17], based on Paul Ekman's book on nonverbal communication [18].",
    "is_useful": true,
    "question": "What is the significance of neurodiversity in the context of open science?"
  },
  {
    "text": "Why had nearly a decade passed before Dave and I figured out that our youngest child was on the spectrum? Perhaps because he was not so different from his grandfather, or from me.\n\n#### **Neurodiversity.Com**\n\nThe appearance of \"neurodiversity\" in the *New York Times* [16] signaled that the time had come to transform my database into a website. In May 2004, after a week of nonstop HTML writing, Neurodiversity.com was born. Over 100 different pages included \"Positive Perspectives on Autism,\" \"Girls & Women on the Spectrum,\" \"The Question of Cure,\" and \"Neurotypical Issues\"; the site also featured a game, \"Unmasking the Face\" [17], based on Paul Ekman's book on nonverbal communication [18]. Later, I added a collection of papers on the history of autism research, including early works by Kanner, Asperger, and Lovaas [19].\n\nFrom my original mission statement:\n\nMy goal is to increase goodwill and compassion in the world, and to help reduce suffering. I seek to help reduce the suffering of autistic children and adults, who often face extraordinary challenges in many domains of life, challenges made more difficult by others' unrealistic expectations and demands, negative judgments, harassment and marginalization. I seek to help reduce the suffering of family and community members who are bewildered and distressed by actions of and interactions with autistic people, and who are concerned for their own and others' safety and well-being.",
    "is_useful": true,
    "question": "What is the primary mission of initiatives aimed at promoting neurodiversity?"
  },
  {
    "text": "Later, I added a collection of papers on the history of autism research, including early works by Kanner, Asperger, and Lovaas [19].\n\nFrom my original mission statement:\n\nMy goal is to increase goodwill and compassion in the world, and to help reduce suffering. I seek to help reduce the suffering of autistic children and adults, who often face extraordinary challenges in many domains of life, challenges made more difficult by others' unrealistic expectations and demands, negative judgments, harassment and marginalization. I seek to help reduce the suffering of family and community members who are bewildered and distressed by actions of and interactions with autistic people, and who are concerned for their own and others' safety and well-being. I seek to help increase the capability of educators and service providers to provide effective, respectful support for those on the autistic spectrum. My means of achieving that goal is to share some of the information that has helped us to move from a place of grief and stress to a place of recognition, understanding, and positive regard. [20]\n\n#### **Engaging Advocacy**\n\nShortly after Neurodiversity.com launched, I noticed a visit from the Yahoo group AutAdvo; soon I was avidly participating in discussions with autistic men and women whose writings I had previously encountered.",
    "is_useful": true,
    "question": "What is the goal of initiatives seeking to support autistic individuals and their families in the context of open science?"
  },
  {
    "text": "[20]\n\n#### **Engaging Advocacy**\n\nShortly after Neurodiversity.com launched, I noticed a visit from the Yahoo group AutAdvo; soon I was avidly participating in discussions with autistic men and women whose writings I had previously encountered. I enjoyed exchanges with Camille Clark, co-creator of the Autistic Adults Picture Project [21], who blogged from 2005 to 2007 as \"Autism Diva\" [22]; Jane Meyerding, author of \"Thoughts on Finding Myself Differently Brained\" [23]; Phil Schwarz, father of an autistic son and vice-president of AANE [24]; Kassianne Sibley (now Kassiane Asasumasu); Patricia Clark (d. 2005 [25]); Alyric (d. 2009 [26]); and many other members.\n\nThrough AutAdvo, I befriended Gayle Kirkpatrick, whose autistic son had been banned from their town's only playground, and traveled to Maine in August 2004 to attend hearings in the family's suit against the school district. Shortly before trial, I summarized many themes that would inform my subsequent advocacy work in \"The Autistic Distinction\":\n\n\u2026 Those who value compassion must work to change the content and tenor of public discussion about cognitive difference.",
    "is_useful": true,
    "question": "What is a key theme in advocacy work related to changing public discussions about cognitive differences?"
  },
  {
    "text": "#### **7 Neurodiversity.Com: A Decade of Advocacy** 93\n\n\u2026 Many parents experience the unveiling of autism as a grievous revelation. However, I am convinced that the negative impact on families and on autistic persons is increased and perpetuated by crisis-oriented descriptions of autism that focus on abnormality and deficit, that automatically characterize early education as a heroic \"intervention\" if the children being educated are autistic, that raise the specter of institutionalization simply because that is the way society has tended to address cognitive difference in the past, and that describe autism as something that must be destroyed.\n\nAssertions that autism can and must be \"cured\" create unrealistic expectations, promote the exploitation of parents made desperate by dire predictions, and perpetuate a climate of negative judgment towards children and adults who are not or do not strive to become \"indistinguishable from their peers,\" those who look and behave like the autistic people that they are.\n\n\u2026 I seek a reconceptualization of cognitive difference, to the end that those who bear now-stigmatizing labels of \"deviance,\" \"disorder\" and \"syndrome,\" may live and manifest their individuality, distinctive interests, gifts and capacities with integrity, in a manner that comes naturally to them, free of pressure to become people they are not, free of the automatic assignation of inferior status; and that they may enjoy the respect of their fellow citizens, rather than disdain and exclusion.\n\nRetrospective consideration of the lives of exceptional human beings offers credible evidence that the autistic distinction has persisted throughout history, and has been a valuable element of human culture.",
    "is_useful": true,
    "question": "What are the implications of societal perceptions of autism on the experiences and identities of autistic individuals?"
  },
  {
    "text": "\u2026 I seek a reconceptualization of cognitive difference, to the end that those who bear now-stigmatizing labels of \"deviance,\" \"disorder\" and \"syndrome,\" may live and manifest their individuality, distinctive interests, gifts and capacities with integrity, in a manner that comes naturally to them, free of pressure to become people they are not, free of the automatic assignation of inferior status; and that they may enjoy the respect of their fellow citizens, rather than disdain and exclusion.\n\nRetrospective consideration of the lives of exceptional human beings offers credible evidence that the autistic distinction has persisted throughout history, and has been a valuable element of human culture. Genetic research indicates that at least twenty different genes can signal a predisposition to autistic development; autism is pervasively embedded in the deep structure of humanity. Psychological research indicates that autistic characteristics constitute an identifiable pattern of traits that are present in varying degrees throughout our entire species.\n\nAutism is as much a part of humanity as is the capacity to dream [27].\n\nThrough AutAdvo, I also befriended University of Wisconsin-Madison psychology professor and fellow autism mom Morton Ann Gernsbacher. When news broke that University of Kentucky chemist Boyd Haley had dubbed autism \"mad child disease,\" we created a \"Petition to Defend the Dignity of Autistic Citizens\" that garnered hundreds of signatures after it was published on Neurodiversity.com [28].",
    "is_useful": true,
    "question": "How does the concept of cognitive difference relate to the acceptance and respect for individuals identified as autistic?"
  },
  {
    "text": "Retrospective consideration of the lives of exceptional human beings offers credible evidence that the autistic distinction has persisted throughout history, and has been a valuable element of human culture. Genetic research indicates that at least twenty different genes can signal a predisposition to autistic development; autism is pervasively embedded in the deep structure of humanity. Psychological research indicates that autistic characteristics constitute an identifiable pattern of traits that are present in varying degrees throughout our entire species.\n\nAutism is as much a part of humanity as is the capacity to dream [27].\n\nThrough AutAdvo, I also befriended University of Wisconsin-Madison psychology professor and fellow autism mom Morton Ann Gernsbacher. When news broke that University of Kentucky chemist Boyd Haley had dubbed autism \"mad child disease,\" we created a \"Petition to Defend the Dignity of Autistic Citizens\" that garnered hundreds of signatures after it was published on Neurodiversity.com [28]. I soon learned that Haley was a hero to many convinced that autism was vaccine-induced, and a proposed expert witness in legal proceedings consolidating thousands of claims filed by parents of autistic children. The experience strengthened my concern about the proliferation of anti-vaccinationist sentiment, litigiousness, and chronic outrage among parents of autistic children; the use of sensationalistic language to describe autism and autistic people; and the hostility toward autistic adults expressed by many proponents of an \"autism epidemic.\"\n\nI wrote to the Congressional Autism Caucus [29], and many more letters to editors, and documented all of the correspondence on the site.",
    "is_useful": true,
    "question": "What role does genetic research play in our understanding of autism as part of human culture?"
  },
  {
    "text": "When news broke that University of Kentucky chemist Boyd Haley had dubbed autism \"mad child disease,\" we created a \"Petition to Defend the Dignity of Autistic Citizens\" that garnered hundreds of signatures after it was published on Neurodiversity.com [28]. I soon learned that Haley was a hero to many convinced that autism was vaccine-induced, and a proposed expert witness in legal proceedings consolidating thousands of claims filed by parents of autistic children. The experience strengthened my concern about the proliferation of anti-vaccinationist sentiment, litigiousness, and chronic outrage among parents of autistic children; the use of sensationalistic language to describe autism and autistic people; and the hostility toward autistic adults expressed by many proponents of an \"autism epidemic.\"\n\nI wrote to the Congressional Autism Caucus [29], and many more letters to editors, and documented all of the correspondence on the site. These were noticed; Neurodiversity.com was mentioned in the December 2004 *New York Times* piece, \"How About Not Curing Us, Some Autistics Are Pleading\" [30]. Later, I had encouraging exchanges with the head of the National Institute of Mental Health, who had stated that autism \"robs a family of [a child's] personhood\" [31], and the head of the University of California, Davis MIND Institute, whose fundraisers had broadcast insupportable claims of an \"autism epidemic\" [32].",
    "is_useful": true,
    "question": "What concerns have arisen regarding the representation and treatment of autism in public discourse?"
  },
  {
    "text": "I wrote to the Congressional Autism Caucus [29], and many more letters to editors, and documented all of the correspondence on the site. These were noticed; Neurodiversity.com was mentioned in the December 2004 *New York Times* piece, \"How About Not Curing Us, Some Autistics Are Pleading\" [30]. Later, I had encouraging exchanges with the head of the National Institute of Mental Health, who had stated that autism \"robs a family of [a child's] personhood\" [31], and the head of the University of California, Davis MIND Institute, whose fundraisers had broadcast insupportable claims of an \"autism epidemic\" [32].\n\nAfter *Rolling Stone* published \"Deadly Immunity,\" alleging a cover-up of evidence linking autism with vaccines, I wrote to its author, Robert F. Kennedy Jr., noting that not all parents regarded their children's autism as a consequence of wrongdoing [33]. As the publicity campaign escalated for David Kirby's book *Evidence of Harm* (EoH) [34], I wrote to question Kirby about the purpose of the discussion list created in connection with it, documenting many instances in which autistic children were disparaged and dissenting parents vilified [35]. I also corresponded with Lenny Schafer\u2014proprietor of the EoH list and publisher of the *Schafer Autism Report*, where autistic advocates were denounced as \"imposters who trivialize the catastrophic nature of real autism\" [36, 37].",
    "is_useful": true,
    "question": "What actions were taken to advocate for the understanding of autism and the perspectives of autistic individuals in response to various public narratives and campaigns?"
  },
  {
    "text": "After *Rolling Stone* published \"Deadly Immunity,\" alleging a cover-up of evidence linking autism with vaccines, I wrote to its author, Robert F. Kennedy Jr., noting that not all parents regarded their children's autism as a consequence of wrongdoing [33]. As the publicity campaign escalated for David Kirby's book *Evidence of Harm* (EoH) [34], I wrote to question Kirby about the purpose of the discussion list created in connection with it, documenting many instances in which autistic children were disparaged and dissenting parents vilified [35]. I also corresponded with Lenny Schafer\u2014proprietor of the EoH list and publisher of the *Schafer Autism Report*, where autistic advocates were denounced as \"imposters who trivialize the catastrophic nature of real autism\" [36, 37]. I did not expect that my letters to Kennedy, Kirby, and Schafer would provoke attitudinal change in their recipients, but persevered and published in the hope that visitors to Neurodiversity.com might reconsider their assumptions about autism, recognize the toxic nature of the crusade to equate autism with contamination, and increase their respect for autistic citizens of all ages.\n\n#### **Neurodiversity Weblog**\n\nIn the summer of 2005, I established Neurodiversity Weblog to streamline publication and facilitate discussion of letters and articles on the site. During the blog's first year [38\u201342], I published writing by Darold Treffert, Rita Jordan, Michelle Dawson, Phil Schwarz, and James Laidler.",
    "is_useful": true,
    "question": "What initiatives have been taken to promote understanding and respect for autistic individuals in the face of misinformation about autism and vaccines?"
  },
  {
    "text": "I did not expect that my letters to Kennedy, Kirby, and Schafer would provoke attitudinal change in their recipients, but persevered and published in the hope that visitors to Neurodiversity.com might reconsider their assumptions about autism, recognize the toxic nature of the crusade to equate autism with contamination, and increase their respect for autistic citizens of all ages.\n\n#### **Neurodiversity Weblog**\n\nIn the summer of 2005, I established Neurodiversity Weblog to streamline publication and facilitate discussion of letters and articles on the site. During the blog's first year [38\u201342], I published writing by Darold Treffert, Rita Jordan, Michelle Dawson, Phil Schwarz, and James Laidler. I deconstructed a *Chronicle of Higher Education* article condoning discrimination against autistic candidates for academic employment [43, 44]. I protested the Autism Society of America's pathos-inducing \"Getting the Word Out about Autism\" campaign [45], the use of vaccine-causation evangelists as consultants by Autism Speaks [46], and their doom-laden film *Autism Every Day* [47]. The blog attracted both sympathetic and critical readers, and featured many lively and occasionally contentious exchanges.\n\nA February 2006 article in the *Concord Monitor* [48] spurred me to investigate a new regimen involving administration of hormonal suppressants such as Lupron to autistic children [49].",
    "is_useful": true,
    "question": "What initiative aimed to facilitate discussion and publication regarding perceptions of autism and promote respect for autistic individuals?"
  },
  {
    "text": "During the blog's first year [38\u201342], I published writing by Darold Treffert, Rita Jordan, Michelle Dawson, Phil Schwarz, and James Laidler. I deconstructed a *Chronicle of Higher Education* article condoning discrimination against autistic candidates for academic employment [43, 44]. I protested the Autism Society of America's pathos-inducing \"Getting the Word Out about Autism\" campaign [45], the use of vaccine-causation evangelists as consultants by Autism Speaks [46], and their doom-laden film *Autism Every Day* [47]. The blog attracted both sympathetic and critical readers, and featured many lively and occasionally contentious exchanges.\n\nA February 2006 article in the *Concord Monitor* [48] spurred me to investigate a new regimen involving administration of hormonal suppressants such as Lupron to autistic children [49]. I learned that applications to patent the \"Lupron protocol\" remained undisclosed in the presentations of its developers, Mark and David Geier [50]; that the latter's academic affiliation was misrepresented in a peer-reviewed study [51]; that they headed the IRB overseeing their own research [52]; that they were diagnosing autistic children with precocious puberty who did not meet formal criteria [53]; and that the \"protocol\" called for excessive blood draws and expensive tests [54]. After publishing a series of articles on the subject [55], I sent the editorial board of *Autoimmunity Reviews* a lengthy critique of a report they had published of the Geiers's research [56].",
    "is_useful": true,
    "question": "What issues related to ethical standards in research and publishing were raised in the blog discussions?"
  },
  {
    "text": "After publishing a series of articles on the subject [55], I sent the editorial board of *Autoimmunity Reviews* a lengthy critique of a report they had published of the Geiers's research [56]. This led to the paper's retraction, an incident later discussed in the *British Medical Journal* [57] and *Slate* [58].\n\nDuring this period, I wrote to the Interagency Autism Coordinating Committee, calling out their lack of autistic members and their references to autism as a disease, and contrasting the scant attention they paid to quality of life issues with their frequent appeals for brain tissue, which gave the impression that autistic adults were more highly valued when dead than alive [47]. I also exchanged correspondence with the United Methodist Church protesting their support of *Sykes v. Bayer*, litigation initiated by a Methodist minister against corporations she held at fault for causing her son's autism [59].\n\nIn the fall of 2007, a paralegal studies course inspired me to write a series of articles on cases alleging environmental causation of autism. *Vaccine Court Chronicles* attracted heat [60]. On March 24, 2008, I published a post discussing economic incentives that biased vaccine-injury attorneys' pronouncements on disability causation, and tallied fees paid by the court to Geier associate Clifford Shoemaker [61].",
    "is_useful": true,
    "question": "What actions can researchers take to promote transparency and accountability in scientific publications?"
  },
  {
    "text": "I also exchanged correspondence with the United Methodist Church protesting their support of *Sykes v. Bayer*, litigation initiated by a Methodist minister against corporations she held at fault for causing her son's autism [59].\n\nIn the fall of 2007, a paralegal studies course inspired me to write a series of articles on cases alleging environmental causation of autism. *Vaccine Court Chronicles* attracted heat [60]. On March 24, 2008, I published a post discussing economic incentives that biased vaccine-injury attorneys' pronouncements on disability causation, and tallied fees paid by the court to Geier associate Clifford Shoemaker [61]. Barely four hours later, Shoemaker issued a subpoena for me to be deposed in *Sykes v. Bayer*, demanding my financial records, tax returns, information about my religious beliefs, and all correspondence about any subject discussed on Neurodiversity.com [62].\n\nAfter I blogged my motion to invalidate the subpoena, all hell broke loose. *Slashdot* covered the case three times, nearly crashing the server with each flood of hits [63\u201365]. *Opinionistas* across the blogosphere offered their support. Attorney Paul Levy of Public Citizen submitted a brief recommending that Shoemaker be sanctioned [66]. Harvard's Digital Media Law Project featured the case on their site [67]. I was profiled by the *Concord Monitor* [68], and in Andrew Solomon's *New York* article, \"The Autism Rights Movement\" [69], Dr.",
    "is_useful": true,
    "question": "What are some examples of legal cases that have raised questions about environmental factors influencing health issues, such as autism?"
  },
  {
    "text": "Barely four hours later, Shoemaker issued a subpoena for me to be deposed in *Sykes v. Bayer*, demanding my financial records, tax returns, information about my religious beliefs, and all correspondence about any subject discussed on Neurodiversity.com [62].\n\nAfter I blogged my motion to invalidate the subpoena, all hell broke loose. *Slashdot* covered the case three times, nearly crashing the server with each flood of hits [63\u201365]. *Opinionistas* across the blogosphere offered their support. Attorney Paul Levy of Public Citizen submitted a brief recommending that Shoemaker be sanctioned [66]. Harvard's Digital Media Law Project featured the case on their site [67]. I was profiled by the *Concord Monitor* [68], and in Andrew Solomon's *New York* article, \"The Autism Rights Movement\" [69], Dr. Paul Offit devoted a chapter to my work in *Autism's False Prophets*, and included me in its dedication [70]. After my motion was granted [71], *Sykes v. Bayer* was dismissed with prejudice [72], and the court ultimately sanctioned Mr. Shoemaker [73].\n\nOver the next few years, I continued to report on ongoing autismvaccine litigation, post announcements of research participation opportunities, and investigate dodgy autism treatments and consumer scams.",
    "is_useful": true,
    "question": "What actions can be taken to support open science in response to legal challenges surrounding research and public discourse?"
  },
  {
    "text": "*Opinionistas* across the blogosphere offered their support. Attorney Paul Levy of Public Citizen submitted a brief recommending that Shoemaker be sanctioned [66]. Harvard's Digital Media Law Project featured the case on their site [67]. I was profiled by the *Concord Monitor* [68], and in Andrew Solomon's *New York* article, \"The Autism Rights Movement\" [69], Dr. Paul Offit devoted a chapter to my work in *Autism's False Prophets*, and included me in its dedication [70]. After my motion was granted [71], *Sykes v. Bayer* was dismissed with prejudice [72], and the court ultimately sanctioned Mr. Shoemaker [73].\n\nOver the next few years, I continued to report on ongoing autismvaccine litigation, post announcements of research participation opportunities, and investigate dodgy autism treatments and consumer scams. Subjects included OSR, an industrial chelator developed by Boyd Haley and promoted for consumption by autistic children [74], and electromagnetic radiation shielding devices touted as autism treatments by debt-ridden multilevel marketers and new-age entrepreneurs [75]. A misleading telephone solicitation provoked me to dig into the public filings of the Autism Spectrum Disorder Foundation, which claimed to help autistic people, but showed little evidence of useful activity [76]. My local paper published an op-ed in which I advised readers to be skeptical of unfounded claims about autism [77].",
    "is_useful": true,
    "question": "What role do public advocacy and skepticism play in the context of autism treatment and research?"
  },
  {
    "text": "After my motion was granted [71], *Sykes v. Bayer* was dismissed with prejudice [72], and the court ultimately sanctioned Mr. Shoemaker [73].\n\nOver the next few years, I continued to report on ongoing autismvaccine litigation, post announcements of research participation opportunities, and investigate dodgy autism treatments and consumer scams. Subjects included OSR, an industrial chelator developed by Boyd Haley and promoted for consumption by autistic children [74], and electromagnetic radiation shielding devices touted as autism treatments by debt-ridden multilevel marketers and new-age entrepreneurs [75]. A misleading telephone solicitation provoked me to dig into the public filings of the Autism Spectrum Disorder Foundation, which claimed to help autistic people, but showed little evidence of useful activity [76]. My local paper published an op-ed in which I advised readers to be skeptical of unfounded claims about autism [77].\n\nIn 2010, my investigations of the \"Lupron protocol\" and OSR inspired (and were cited in) the *Chicago Tribune*'s award-winning series on unproven autism treatments [78, 79]. Shortly thereafter, the FDA ordered OSR taken off the market [80].2 In the spring of 2011, following a Maryland citizen's complaint incorporating my articles on the \"Lupron protocol,\" the state's Board of Physicians suspended Mark Geier's license to practice medicine and charged David Geier with practicing medicine unlawfully [82]. By 2013, Dr. Geier's license had been revoked in all twelve states that had granted it [83].",
    "is_useful": true,
    "question": "What types of issues related to autism treatment have been investigated and reported on in recent years?"
  },
  {
    "text": "My local paper published an op-ed in which I advised readers to be skeptical of unfounded claims about autism [77].\n\nIn 2010, my investigations of the \"Lupron protocol\" and OSR inspired (and were cited in) the *Chicago Tribune*'s award-winning series on unproven autism treatments [78, 79]. Shortly thereafter, the FDA ordered OSR taken off the market [80].2 In the spring of 2011, following a Maryland citizen's complaint incorporating my articles on the \"Lupron protocol,\" the state's Board of Physicians suspended Mark Geier's license to practice medicine and charged David Geier with practicing medicine unlawfully [82]. By 2013, Dr. Geier's license had been revoked in all twelve states that had granted it [83]. As the first person to raise the alarm about the Geier's pharmaceutical experimentation on autistic children, and about Haley's efforts to bypass federal drug approval regulations, I take pride in these outcomes.\n\n#### **Engaging Community**\n\nNeurodiversity.com was, for the most part, a one-woman operation; the occasional conference enabled me to connect with others who shared my interests and perspective. I was grateful to AANE for offering support to children like my son and parents like me. I learned much from Aut-Com's workshops on assistive communication and from accounts of its members.",
    "is_useful": true,
    "question": "What lessons can be learned about the importance of skepticism in evaluating medical treatments related to autism?"
  },
  {
    "text": "By 2013, Dr. Geier's license had been revoked in all twelve states that had granted it [83]. As the first person to raise the alarm about the Geier's pharmaceutical experimentation on autistic children, and about Haley's efforts to bypass federal drug approval regulations, I take pride in these outcomes.\n\n#### **Engaging Community**\n\nNeurodiversity.com was, for the most part, a one-woman operation; the occasional conference enabled me to connect with others who shared my interests and perspective. I was grateful to AANE for offering support to children like my son and parents like me. I learned much from Aut-Com's workshops on assistive communication and from accounts of its members. At Autreat 2008, I met Rosalind Picard of the MIT Media Lab [84], which I later visited with Mel Baggs and Est\u00e9e Klar, founder of The Autism Acceptance Project [85]. I attended AutCom 2007 and Autreat 2009 as Mel's support person, traveling with them and assisting at their presentations.\n\nAs the amount of information and misinformation about autism proliferated online, updating Neurodiversity.com's static link pages grew increasingly laborious, and ended in 2008. The flow of blog posts lessened thanks to newfound employment; my son's labor-intensive adolescence and gender reassignment; my parents' passing; and advocacy burnout exacerbated by often-hostile attention attracted by my writing, and by the escalation of conflict between autism advocates.",
    "is_useful": true,
    "question": "What challenges did individuals face in maintaining accurate information about autism in the online community?"
  },
  {
    "text": "Dave and I were both increasingly burdened by these conflicts, as well as by a member's depressive crisis. Stressors beyond the list included efforts to discredit Mel Baggs, whose video \"In My Language\" [88] had attracted overwhelming media attention. In May 2010, after one too many bouts of *agita*, Dave took the site offline, leaving it to others to proceed without his involvement. Many were unhappy about the abrupt shutdown, but sometimes one's own sanity must come first, and we do not regret our decision.\n\nI continued to blog as I have described above, and attended one more conference. I published my last post in March 2012 [89]; one year later, a botched server migration vaporized Neurodiversity Weblog. Fortunately, the posts can still be accessed via the Internet Archive (http://www.archive. org). Although most of Neurodiversity.com's external links are defunct, I continue to host the site as a document of autism advocacy, the debate over autism and vaccines, and the evolving idea of \"neurodiversity.\"\n\nAlthough I own the web domain, I am reluctant to define \"neurodiversity,\" preferring to express in writing the values I associate with it.",
    "is_useful": true,
    "question": "What challenges can arise in the management of online advocacy related to neurodiversity?"
  },
  {
    "text": "Many were unhappy about the abrupt shutdown, but sometimes one's own sanity must come first, and we do not regret our decision.\n\nI continued to blog as I have described above, and attended one more conference. I published my last post in March 2012 [89]; one year later, a botched server migration vaporized Neurodiversity Weblog. Fortunately, the posts can still be accessed via the Internet Archive (http://www.archive. org). Although most of Neurodiversity.com's external links are defunct, I continue to host the site as a document of autism advocacy, the debate over autism and vaccines, and the evolving idea of \"neurodiversity.\"\n\nAlthough I own the web domain, I am reluctant to define \"neurodiversity,\" preferring to express in writing the values I associate with it. The thoughtless deployment of stigmatizing characterizations of autism; the misleading marketing of unproven \"autism treatments\" to parents of autistic children; the litigation and culture of blame into which so many families were drawn\u2014all presented morally and intellectually compelling matters for consideration, inspired by my conviction that cognitively variant persons of all ages should be afforded respect, appropriate assistance, and freedom from abuse, exploitation, and undue pathologization of their traits and challenges.\n\n#### **Conclusion**\n\nMy path to advocacy began with the need to understand my child, and to marshal understanding within the school and community.",
    "is_useful": true,
    "question": "What are some key concerns addressed by advocates in the field of neurodiversity?"
  },
  {
    "text": "Although most of Neurodiversity.com's external links are defunct, I continue to host the site as a document of autism advocacy, the debate over autism and vaccines, and the evolving idea of \"neurodiversity.\"\n\nAlthough I own the web domain, I am reluctant to define \"neurodiversity,\" preferring to express in writing the values I associate with it. The thoughtless deployment of stigmatizing characterizations of autism; the misleading marketing of unproven \"autism treatments\" to parents of autistic children; the litigation and culture of blame into which so many families were drawn\u2014all presented morally and intellectually compelling matters for consideration, inspired by my conviction that cognitively variant persons of all ages should be afforded respect, appropriate assistance, and freedom from abuse, exploitation, and undue pathologization of their traits and challenges.\n\n#### **Conclusion**\n\nMy path to advocacy began with the need to understand my child, and to marshal understanding within the school and community. I found the greatest insight for this work in writings of and interactions with autistic adults and their allies, both in person and online.With Neurodiversity.com and NeurodiversityWeblog, I sought first to share useful information, then to communicate my evolving concerns and encourage consideration of the concerns of autistic people themselves. I did not seek to join a movement, but ended up participating in one.",
    "is_useful": true,
    "question": "What are some key values and concerns associated with the concept of neurodiversity?"
  },
  {
    "text": "#### **Conclusion**\n\nMy path to advocacy began with the need to understand my child, and to marshal understanding within the school and community. I found the greatest insight for this work in writings of and interactions with autistic adults and their allies, both in person and online.With Neurodiversity.com and NeurodiversityWeblog, I sought first to share useful information, then to communicate my evolving concerns and encourage consideration of the concerns of autistic people themselves. I did not seek to join a movement, but ended up participating in one. As I put it in a 2006 letter to the New *York Times Book Review*:\n\nThe neurodiversity movement does not consist of faddish cultists trolling for converts, but of disabled individuals, their family members, and [other] allies constructively responding to prejudice, stigma and pejorative labeling. People don't all think the same way, and appreciation of this reality is not limited to those who possess a diagnosis. I am one parent who wants her \"neurodiverse\" family members to flourish \u2014 happily, healthily, welleducated and respected in a society that embraces the value of cognitive variety. [90]\n\n## **References**\n\n- 1. Sacks, O. (1993, December 27). An anthropologist on Mars.*The New Yorker,* 69(44), 106\u2013125. Retrieved from https://www.newyorker.com/magazine/ 1993/12/27/anthropologist-mars.\n- 2.",
    "is_useful": true,
    "question": "What is the main goal of the neurodiversity movement as discussed in the text?"
  },
  {
    "text": "Retrieved from https://web.archive.org/web/ 20130401040535/http://neurodiversity.com/weblog/article/78.\n- 91. *Autism Hub* (Archived website). Retrieved from https://web.archive.org/ web/20070102230042/http://www.autism-hub.co.uk.\n\n**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/ 4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and\n\nindicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\n![](_page_117_Picture_6.jpeg)\n\n![](_page_118_Figure_0.jpeg)\n\n# 8\n\n## **Autscape**\n\n**Karen Leneh Buckle**\n\n#### **In the Beginning, There Was a List**\n\nI am currently the event manager for Autscape, an annual three-day residential event for autistic people. It is autistic-led, though not exclusive.",
    "is_useful": true,
    "question": "What are the terms under which a chapter licensed under the Creative Commons Attribution 4.0 International License can be used?"
  },
  {
    "text": "The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n\n![](_page_117_Picture_6.jpeg)\n\n![](_page_118_Figure_0.jpeg)\n\n# 8\n\n## **Autscape**\n\n**Karen Leneh Buckle**\n\n#### **In the Beginning, There Was a List**\n\nI am currently the event manager for Autscape, an annual three-day residential event for autistic people. It is autistic-led, though not exclusive. I was there the day Autscape was conceived, and I have had a major role in organizing the event for 12 of its 14 years. Here's how it happened.\n\nSoon after I first discovered the Internet in late 1996, I sought out autism-related groups and immediately went to spending hours every day on autism chat rooms on Internet Relay Chat (IRC) and email-based support groups (\"lists\") such as Independent Living on the Autistic Spectrum (InLv; Martijn Dekker [1], see Chapter 2) and Autism [2]. Through these activities, I heard about Autreat and joined ANI-L, the list run by the organization responsible for Autreat, Autism Network International (ANI, www.autismnetworkinternational.org).",
    "is_useful": true,
    "question": "What is the importance of obtaining permission for the use of third party material in relation to Creative Commons licensing?"
  }
]